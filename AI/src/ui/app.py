"""
AIæ•°æ®åˆ†æç³»ç»Ÿ - ç”¨æˆ·ç•Œé¢æ¨¡å—

ä½¿ç”¨Streamlitåˆ›å»ºäº¤äº’å¼Webç•Œé¢ï¼Œæä¾›ï¼š
1. æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
2. æ•°æ®é¢„è§ˆ
3. æ¨¡å‹é€‰æ‹©
4. åˆ†æç»“æœå±•ç¤º
5. æŠ¥å‘Šå¯¼å‡º
"""

import sys
import streamlit as st
import pandas as pd
import numpy as np
import os
import time
import uuid
import base64
import matplotlib.pyplot as plt
from pathlib import Path
import logging
from typing import Optional, Dict, Any

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.model_selection.model_selector import ModelSelector, ModelRecommendation
from src.data_processing.data_loader import DataLoader, DataValidator
from src.data_processing.data_processor import DataProcessor
from src.visualization.visualizer import create_visualization_manager
from src.report_generation.report_generator import create_advanced_report_generator
from src.ai_agent.ai_assistant import create_ai_assistant

# å¯¼å…¥AIå¢å¼ºæ¨¡å—
try:
    from src.ai_agent.ai_report_enhancer import create_ai_enhancer, DEFAULT_CONFIGS, AIModelConfig, AIReportEnhancer
    AI_ENHANCEMENT_AVAILABLE = True
except ImportError:
    AI_ENHANCEMENT_AVAILABLE = False
    st.warning("âš ï¸ AIæŠ¥å‘Šå¢å¼ºåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç›¸å…³ä¾èµ–")

logger = logging.getLogger(__name__)


class AppState:
    """åº”ç”¨çŠ¶æ€ç®¡ç†ç±»"""
    
    @staticmethod
    def initialize_session_state():
        """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
        # åœ¨ä¼šè¯åˆå§‹åŒ–æ—¶æ¸…ç†è¿‡æœŸçš„ä¸´æ—¶å›¾åƒæ–‡ä»¶
        try:
            clean_temp_figures()
        except Exception:
            pass
        if 'uploaded_file' not in st.session_state:
            st.session_state.uploaded_file = None
        if 'data' not in st.session_state:
            st.session_state.data = None
        if 'processed_data' not in st.session_state:
            st.session_state.processed_data = None
        if 'data_info' not in st.session_state:
            st.session_state.data_info = None
        if 'analysis_results' not in st.session_state:
            st.session_state.analysis_results = {}
        if 'descriptive_stats' not in st.session_state:
            st.session_state.descriptive_stats = None
        if 'correlation_matrix' not in st.session_state:
            st.session_state.correlation_matrix = None
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = None
        if 'progress' not in st.session_state:
            st.session_state.progress = 0
        if 'current_step' not in st.session_state:
            st.session_state.current_step = 'upload'  # upload, analyze, visualize, report
        if 'current_section' not in st.session_state:
            st.session_state.current_section = 'upload'  # æ·»åŠ current_sectionçš„åˆå§‹åŒ–
        if 'file_name' not in st.session_state:
            st.session_state.file_name = None
        if 'file_format' not in st.session_state:
            st.session_state.file_format = None
        if 'report_path' not in st.session_state:
            st.session_state.report_path = None
        if 'preprocessing_info' not in st.session_state:
            st.session_state.preprocessing_info = {}
        # AIåŠ©æ‰‹ç›¸å…³çŠ¶æ€
        if 'ai_assistant' not in st.session_state:
            st.session_state.ai_assistant = create_ai_assistant()
        if 'conversation_history' not in st.session_state:
            st.session_state.conversation_history = []
        if 'show_ai_assistant' not in st.session_state:
            st.session_state.show_ai_assistant = False
        
        # AIæŠ¥å‘Šå¢å¼ºç›¸å…³çŠ¶æ€
        if 'ai_enhancement_enabled' not in st.session_state:
            st.session_state.ai_enhancement_enabled = False
        if 'ai_provider' not in st.session_state:
            st.session_state.ai_provider = "openai"
        if 'ai_model' not in st.session_state:
            st.session_state.ai_model = "gpt-3.5-turbo"
        if 'ai_api_key' not in st.session_state:
            st.session_state.ai_api_key = ""
        if 'ai_api_base' not in st.session_state:
            st.session_state.ai_api_base = ""
        if 'ai_enhancement_type' not in st.session_state:
            st.session_state.ai_enhancement_type = "comprehensive"


def set_page_config():
    """è®¾ç½®Streamlité¡µé¢é…ç½®"""
    st.set_page_config(
        page_title="AIæ™ºèƒ½æ•°æ®åˆ†æç³»ç»Ÿ",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )


def run_app():
    """è¿è¡ŒStreamlitåº”ç”¨ï¼ˆä¾›main.pyè°ƒç”¨ï¼‰"""
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    AppState.initialize_session_state()
    # è®¾ç½®é¡µé¢é…ç½®
    set_page_config()
    # è¿™é‡Œä¼šæ‰§è¡ŒStreamlitåº”ç”¨çš„ä¸»è¦é€»è¾‘
    # Streamlitä¼šè‡ªåŠ¨æ‰§è¡Œæ–‡ä»¶ä¸­çš„ä»£ç 
    pass


def clean_temp_figures(max_age_seconds: int = 7 * 24 * 3600):
    """
    æ¸…ç† `temp/figures` ä¸‹è¶…è¿‡æŒ‡å®šå¹´é¾„çš„ä¸´æ—¶å›¾åƒæ–‡ä»¶ã€‚é»˜è®¤ä¿ç•™ 7 å¤©ã€‚
    åœ¨åº”ç”¨åˆå§‹åŒ–æ—¶è°ƒç”¨ä»¥é˜²ç›®å½•æ— é™è†¨èƒ€ã€‚
    """
    try:
        temp_dir = Path("temp/figures")
        if not temp_dir.exists():
            return
        now = time.time()
        for p in temp_dir.iterdir():
            try:
                if p.is_file():
                    mtime = p.stat().st_mtime
                    if now - mtime > max_age_seconds:
                        p.unlink()
            except Exception:
                # å¿½ç•¥å•ä¸ªæ–‡ä»¶åˆ é™¤é”™è¯¯
                pass
    except Exception:
        pass


def safe_display_figure(fig):
    """
    å®‰å…¨åœ°åœ¨ Streamlit ä¸­æ˜¾ç¤º matplotlib Figureã€‚
    å°è¯•ä½¿ç”¨ st.pyplot æ˜¾ç¤ºï¼›å¦‚æœé‡åˆ° Streamlit çš„åª’ä½“æ–‡ä»¶å­˜å‚¨é”™è¯¯æˆ–å…¶ä»–å¼‚å¸¸ï¼Œ
    å›é€€ä¸ºå°†å›¾åƒç¼–ç ä¸º base64 å¹¶ä½¿ç”¨ st.image æ˜¾ç¤ºï¼ˆè¿™æ ·å¯ä»¥é¿å… MediaFileStorageErrorï¼‰ã€‚
    æ­¤å‡½æ•°ä¼šåœ¨å¿…è¦æ—¶å…³é—­ figureã€‚
    """
    try:
        # ä¸ºäº†é¿å… Streamlit å†…éƒ¨åª’ä½“ id ä¸¢å¤±æˆ–å¹¶å‘å›æ”¶é—®é¢˜ï¼Œ
        # æˆ‘ä»¬æŠŠ figure ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶å¹¶ç›´æ¥é€šè¿‡æ–‡ä»¶è·¯å¾„æ˜¾ç¤ºï¼ˆst.imageï¼‰â€”â€”æ›´ç¨³å¥ã€‚
        temp_dir = Path("temp/figures")
        temp_dir.mkdir(parents=True, exist_ok=True)
        fname = f"fig_{int(time.time())}_{uuid.uuid4().hex}.png"
        file_path = temp_dir / fname
        try:
            fig.savefig(file_path, dpi=150, bbox_inches='tight')
        except Exception as e_save:
            # å¦‚æœç›´æ¥ä¿å­˜å¤±è´¥ï¼Œå›é€€åˆ° DataVisualizer çš„ base64 æ–¹æ³•
            try:
                from src.visualization.visualizer import DataVisualizer
                viz = DataVisualizer()
                img_b64 = viz.figure_to_base64(fig)
                st.image(base64.b64decode(img_b64), use_column_width=True)
                return
            except Exception:
                st.write(f"æ— æ³•æ¸²æŸ“å›¾è¡¨ï¼ˆä¿å­˜ä¸base64å›é€€å‡å¤±è´¥ï¼‰: {e_save}")
                try:
                    plt.close(fig)
                except Exception:
                    pass
                return

        # æ˜¾ç¤ºå›¾ç‰‡å¹¶è®°å½•ï¼ˆä¸ä¸»åŠ¨åˆ é™¤ï¼Œä¾¿äºè°ƒè¯•ï¼›å¯å‘¨æœŸæ€§æ¸…ç† temp/figuresï¼‰
        st.image(str(file_path), use_column_width=True)
    finally:
        try:
            plt.close(fig)
        except Exception:
            pass


def display_header():
    """æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜å’Œæè¿°ä»¥åŠAIåŠ©æ‰‹æŒ‰é’®"""
    col1, col2 = st.columns([4, 1])
    with col1:
        st.markdown("""
        <div style='text-align: center;'>
            <h1 style='color: #2c3e50;'>ğŸ“Š AIæ™ºèƒ½æ•°æ®åˆ†æå¤§æ¨¡å‹ç³»ç»Ÿ</h1>
            <p style='color: #7f8c8d; font-size: 18px;'>è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†ã€æ™ºèƒ½åˆ†æã€å¯è§†åŒ–ä¸ä¸“ä¸šæŠ¥å‘Šç”Ÿæˆ</p>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        # AIåŠ©æ‰‹åˆ‡æ¢æŒ‰é’®
        st.session_state.show_ai_assistant = st.toggle(
            "ğŸ’¬ AIåŠ©æ‰‹", 
            value=st.session_state.show_ai_assistant,
            help="æ‰“å¼€AIæ™ºèƒ½åŠ©æ‰‹ï¼Œè·å–æ•°æ®åˆ†æå»ºè®®"
        )
    
    # æ˜¾ç¤ºè¿›åº¦æŒ‡ç¤ºå™¨
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown(f"""
        <div style='text-align: center; padding: 10px; {'background-color: #2ecc71; color: white;' if st.session_state.current_step in ['analyze', 'visualize', 'report'] else 'background-color: #ecf0f1;'} border-radius: 5px;'>
            <h4>1ï¸âƒ£ å¯¼å…¥æ•°æ®</h4>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        st.markdown(f"""
        <div style='text-align: center; padding: 10px; {'background-color: #2ecc71; color: white;' if st.session_state.current_step in ['visualize', 'report'] else 'background-color: #ecf0f1;'} border-radius: 5px;'>
            <h4>2ï¸âƒ£ åˆ†ææ•°æ®</h4>
        </div>
        """, unsafe_allow_html=True)
    with col3:
        st.markdown(f"""
        <div style='text-align: center; padding: 10px; {'background-color: #2ecc71; color: white;' if st.session_state.current_step == 'report' else 'background-color: #ecf0f1;'} border-radius: 5px;'>
            <h4>3ï¸âƒ£ ç”Ÿæˆå›¾è¡¨</h4>
        </div>
        """, unsafe_allow_html=True)
    with col4:
        st.markdown(f"""
        <div style='text-align: center; padding: 10px; background-color: #ecf0f1; border-radius: 5px;'>
            <h4>4ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š</h4>
        </div>
        """, unsafe_allow_html=True)


def file_upload_section():
    """æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†"""
    st.subheader("ğŸ“¥ æ•°æ®å¯¼å…¥")
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    supported_formats = DataLoader.get_supported_formats()
    st.info(f"æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(supported_formats)}")
    
    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_file = st.file_uploader(
        "é€‰æ‹©æ•°æ®æ–‡ä»¶",
        type=[fmt[1:] for fmt in supported_formats],  # ç§»é™¤ç‚¹å·
        accept_multiple_files=False
    )
    
    if uploaded_file is not None:
        # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.uploaded_file = uploaded_file
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.success(f"å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
        st.write(f"æ–‡ä»¶å¤§å°: {uploaded_file.size / 1024:.2f} KB")
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_dir = Path("temp")
        temp_dir.mkdir(exist_ok=True)
        temp_file_path = temp_dir / uploaded_file.name
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # æ˜¾ç¤ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(progress):
            progress_bar.progress(progress)
            status_text.text(f"åŠ è½½ä¸­... {progress}%")
        
        # å°è¯•åŠ è½½æ•°æ®
        load_success = False
        try:
            # ä½¿ç”¨è¿›åº¦å›è°ƒåŠ è½½æ•°æ®
            df = DataLoader.load_data_with_progress(
                temp_file_path,
                progress_callback=update_progress
            )

            # éªŒè¯æ•°æ®
            data_info = DataValidator.validate_data(df)

            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.data = df
            st.session_state.data_info = data_info
            load_success = True

            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            st.subheader("ğŸ“Š æ•°æ®é¢„è§ˆ")
            col1, col2 = st.columns([1, 2])

            with col1:
                st.write("### æ•°æ®ä¿¡æ¯")
                st.write(f"è¡Œæ•°: {data_info['n_rows']}")
                st.write(f"åˆ—æ•°: {data_info['n_columns']}")
                st.write(f"æ•°å€¼å‹åˆ—æ•°: {len(df.select_dtypes(include=['number']).columns)}")
                st.write(f"ç¼ºå¤±å€¼æ€»æ•°: {sum(data_info['missing_values'].values())}")

            with col2:
                st.write("### å‰5è¡Œæ•°æ®")
                st.dataframe(df.head())

            # æ˜¾ç¤ºæ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼
            st.write("### åˆ—ä¿¡æ¯")
            info_df = pd.DataFrame({
                'æ•°æ®ç±»å‹': df.dtypes.astype(str),
                'ç¼ºå¤±å€¼': df.isnull().sum(),
                'å”¯ä¸€å€¼æ•°é‡': df.nunique()
            })
            st.dataframe(info_df)

            # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
            if data_info['issues']:
                st.warning("âš ï¸ æ•°æ®å¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
                for issue in data_info['issues']:
                    st.warning(f"- {issue}")

            # ä¸‹ä¸€æ­¥æŒ‰é’®
            if st.button("ğŸ” å¼€å§‹åˆ†æ", use_container_width=True):
                st.session_state.current_step = 'analyze'
                st.rerun()

        except Exception as e:
            # åœ¨ UI ç›´æ¥æ˜¾ç¤ºå®Œæ•´å¼‚å¸¸ï¼Œä¾¿äºç”¨æˆ·å¿«é€Ÿå®šä½é—®é¢˜
            st.error(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            try:
                st.exception(e)
            except Exception:
                # åœ¨æŸäº›ç¯å¢ƒä¸‹ st.exception å¯èƒ½æ— æ³•å‘ˆç°å¤æ‚å¯¹è±¡ï¼Œä¿è¯è‡³å°‘æ˜¾ç¤ºæ–‡æœ¬
                st.write(repr(e))
            logger.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
            # æç¤ºç”¨æˆ·å¯èƒ½çš„åŸå› å’Œæ’æŸ¥å»ºè®®
            st.info("æ’æŸ¥å»ºè®®: 1) æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦è¢«æ”¯æŒï¼ˆCSV/XLSX/JSON/TXT/Parquetï¼‰ï¼›2) å¦‚æœæ˜¯ Excelï¼Œè¯·ç¡®ä¿å·²å®‰è£… openpyxlï¼›3) æ–‡ä»¶æ˜¯å¦è¢«åŠ å¯†æˆ–æŸåã€‚")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼šå¦‚æœåŠ è½½æˆåŠŸåˆ™åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œå¦åˆ™ä¿ç•™ä»¥ä¾¿è°ƒè¯•ï¼Œå¹¶åœ¨ UI ä¸­ç»™å‡ºè·¯å¾„
            try:
                if load_success:
                    if temp_file_path.exists():
                        temp_file_path.unlink()
                else:
                    if temp_file_path.exists():
                        st.info(f"å·²ä¿ç•™ä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•: {temp_file_path}")
                        logger.info(f"ä¿ç•™ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶ç”¨äºè°ƒè¯•: {temp_file_path}")
            except Exception as e_cleanup:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e_cleanup}")
    else:
        # é‡ç½®ä¼šè¯çŠ¶æ€
        if st.session_state.data is not None:
            st.session_state.data = None
            st.session_state.data_info = None


def analyze_section():
    """æ•°æ®åˆ†æéƒ¨åˆ† - æŒ‰ç…§SPSSAUæ¨¡æ¿è®¾è®¡"""
    st.subheader("ï¿½ æ•°æ®åˆ†æ - SPSSAUæ¨¡æ¿")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if st.session_state.data is None:
        st.error("è¯·å…ˆä¸Šä¼ æ•°æ®!")
        if st.button("è¿”å›ä¸Šä¼ "):
            st.session_state.current_step = 'upload'
            st.rerun()
        return
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨å®ä¾‹
    processor = DataProcessor()
    
    # ä½¿ç”¨å¤„ç†åçš„æ•°æ®æˆ–åŸå§‹æ•°æ®
    current_data = st.session_state.processed_data if 'processed_data' in st.session_state and st.session_state.processed_data is not None else st.session_state.data
    
    # SPSSAUé£æ ¼çš„åˆ†ææ¨¡å—é€‰æ‹©
    st.write("### ğŸ“‹ é€‰æ‹©åˆ†ææ¨¡å—")
    
    analysis_modules = {
        "æ•°æ®å¤„ç†": {
            "icon": "ğŸ”§",
            "description": "æ•°æ®æ¸…æ´—ã€ç¼–ç ã€ç‰¹å¾å·¥ç¨‹ç­‰åŸºç¡€å¤„ç†",
            "options": ["æ•°æ®æ¸…æ´—", "æ•°æ®ç¼–ç ", "ç”Ÿæˆå˜é‡", "æ•°æ®æ ‡ç­¾è®¾ç½®"]
        },
        "é€šç”¨æ–¹æ³•": {
            "icon": "ğŸ“ˆ", 
            "description": "é¢‘æ•°åˆ†æã€æè¿°ç»Ÿè®¡ã€äº¤å‰åˆ†æç­‰å¸¸ç”¨æ–¹æ³•",
            "options": ["é¢‘æ•°åˆ†æ", "æè¿°ç»Ÿè®¡", "äº¤å‰åˆ†æ(å¡æ–¹)", "ç›¸å…³åˆ†æ", "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ", "é…å¯¹æ ·æœ¬tæ£€éªŒ"]
        },
        "é—®å·ç ”ç©¶": {
            "icon": "ğŸ“",
            "description": "ä¿¡åº¦åˆ†æã€æ•ˆåº¦åˆ†æã€å¤šé€‰é¢˜ç­‰é—®å·ä¸“ç”¨åˆ†æ",
            "options": ["ä¿¡åº¦åˆ†æ", "æ•ˆåº¦åˆ†æ", "å¤šé€‰é¢˜åˆ†æ", "é—®å·è´¨é‡è¯„ä¼°"]
        },
        "è¿›é˜¶æ–¹æ³•": {
            "icon": "ğŸ§ ",
            "description": "å›å½’åˆ†æã€èšç±»ã€å› å­åˆ†æç­‰é«˜çº§ç»Ÿè®¡æ–¹æ³•",
            "options": ["çº¿æ€§å›å½’", "é€»è¾‘å›å½’", "èšç±»åˆ†æ", "å› å­åˆ†æ", "ä¸»æˆåˆ†åˆ†æ", "æ–¹å·®åˆ†æ"]
        },
        "æœºå™¨å­¦ä¹ ": {
            "icon": "ğŸ¤–",
            "description": "å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€ç¥ç»ç½‘ç»œç­‰MLç®—æ³•",
            "options": ["å†³ç­–æ ‘", "éšæœºæ£®æ—", "æ”¯æŒå‘é‡æœº", "ç¥ç»ç½‘ç»œ", "æœ´ç´ è´å¶æ–¯", "KNNåˆ†ç±»"]
        },
        "æ—¶é—´åºåˆ—": {
            "icon": "ğŸ“Š",
            "description": "æ—¶é—´åºåˆ—åˆ†æå’Œé¢„æµ‹",
            "options": ["è¶‹åŠ¿åˆ†æ", "å­£èŠ‚æ€§åˆ†è§£", "ARIMAæ¨¡å‹", "æ—¶åºé¢„æµ‹"]
        }
    }
    
    # åˆ›å»ºæ¨¡å—é€‰æ‹©ç•Œé¢
    selected_module = st.selectbox(
        "é€‰æ‹©åˆ†ææ¨¡å—",
        list(analysis_modules.keys()),
        format_func=lambda x: f"{analysis_modules[x]['icon']} {x}"
    )
    
    # æ˜¾ç¤ºæ¨¡å—æè¿°
    st.info(f"ğŸ“ {analysis_modules[selected_module]['description']}")
    
    # æ˜¾ç¤ºé€‰å®šæ¨¡å—çš„åˆ†æé€‰é¡¹
    st.write(f"### {analysis_modules[selected_module]['icon']} {selected_module}")
    
    # ä½¿ç”¨å¤šé€‰æ¡†è®©ç”¨æˆ·é€‰æ‹©å¤šä¸ªåˆ†ææ–¹æ³•
    selected_analyses = st.multiselect(
        "é€‰æ‹©è¦æ‰§è¡Œçš„åˆ†ææ–¹æ³•ï¼ˆå¯å¤šé€‰ï¼‰",
        analysis_modules[selected_module]['options'],
        help="æ‚¨å¯ä»¥é€‰æ‹©å¤šä¸ªåˆ†ææ–¹æ³•ï¼Œç³»ç»Ÿå°†æŒ‰é¡ºåºä¾æ¬¡æ‰§è¡Œ"
    )
    
    # æ˜¾ç¤ºé€‰ä¸­çš„åˆ†ææ–¹æ³•
    if selected_analyses:
        st.write("**å·²é€‰æ‹©çš„åˆ†ææ–¹æ³•ï¼š**")
        for i, analysis in enumerate(selected_analyses, 1):
            st.write(f"{i}. {analysis}")
    
    # æ‰¹é‡åˆ†æé€‰é¡¹
    if selected_analyses:
        col1, col2 = st.columns(2)
        with col1:
            auto_proceed = st.checkbox("è‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰åˆ†æ", value=True, help="å‹¾é€‰åå°†ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰é€‰ä¸­çš„åˆ†æ")
        with col2:
            save_individual_results = st.checkbox("ä¿å­˜æ¯ä¸ªåˆ†æçš„ç»“æœ", value=True, help="ä¸ºæ¯ä¸ªåˆ†æå•ç‹¬ä¿å­˜ç»“æœ")
    
    # æ‰§è¡Œé€‰å®šçš„åˆ†æ
    if selected_analyses and st.button(f"ğŸš€ æ‰¹é‡æ‰§è¡Œåˆ†æ ({len(selected_analyses)}ä¸ª)", use_container_width=True):
        # åˆå§‹åŒ–æ‰¹é‡åˆ†æç»“æœ
        batch_results = {}
        success_count = 0
        total_count = len(selected_analyses)
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        with st.container():
            for i, analysis_option in enumerate(selected_analyses):
                progress = (i + 1) / total_count
                progress_bar.progress(progress)
                status_text.text(f"æ­£åœ¨æ‰§è¡Œ ({i+1}/{total_count}): {analysis_option}")
                
                try:
                    with st.spinner(f"æ­£åœ¨æ‰§è¡Œ {analysis_option}..."):
                        # æ‰§è¡Œåˆ†æ
                        analysis_result = execute_single_analysis(
                            selected_module, analysis_option, processor, current_data
                        )
                        
                        if analysis_result:
                            batch_results[analysis_option] = analysis_result
                            success_count += 1
                            
                            # å¦‚æœé€‰æ‹©ä¿å­˜å•ç‹¬ç»“æœï¼Œåˆ™æ˜¾ç¤ºç®€è¦ä¿¡æ¯
                            if save_individual_results:
                                with st.expander(f"âœ… {analysis_option} - å®Œæˆ"):
                                    display_analysis_summary(analysis_result)
                        else:
                            st.warning(f"âš ï¸ {analysis_option} æ‰§è¡Œå¤±è´¥æˆ–æ— ç»“æœ")
                            
                        # çŸ­æš‚æš‚åœï¼Œè®©ç”¨æˆ·çœ‹åˆ°è¿›åº¦
                        if auto_proceed and i < len(selected_analyses) - 1:
                            import time
                            time.sleep(0.5)
                            
                except Exception as e:
                    st.error(f"âŒ {analysis_option} æ‰§è¡Œå¤±è´¥: {str(e)}")
                    batch_results[analysis_option] = {"error": str(e)}
        
        # å®Œæˆæ‰¹é‡åˆ†æ
        progress_bar.progress(1.0)
        status_text.text(f"æ‰¹é‡åˆ†æå®Œæˆï¼æˆåŠŸæ‰§è¡Œ {success_count}/{total_count} ä¸ªåˆ†æ")
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        st.session_state.batch_analysis_results = batch_results
        st.session_state.analysis_completed = True
        st.session_state.analysis_type = f"æ‰¹é‡åˆ†æ-{selected_module}"
        st.session_state.current_analysis_data = current_data
        
        # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
        st.success(f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼æˆåŠŸæ‰§è¡Œäº† {success_count} ä¸ªåˆ†ææ–¹æ³•")
        
        # æ˜¾ç¤ºæ‰¹é‡åˆ†ææ±‡æ€»
        with st.expander("ğŸ“Š æ‰¹é‡åˆ†æç»“æœæ±‡æ€»", expanded=True):
            display_batch_results_summary(batch_results)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    if hasattr(st.session_state, 'batch_analysis_results') and st.session_state.batch_analysis_results:
        display_batch_analysis_results()
    elif hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
        display_analysis_results()
    
    # ä¸‹ä¸€æ­¥æŒ‰é’®
    if hasattr(st.session_state, 'analysis_completed') and st.session_state.analysis_completed:
        st.write("---")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“ˆ ä¸‹ä¸€æ­¥ï¼šæ•°æ®å¯è§†åŒ–", use_container_width=True):
                st.session_state.current_step = 'visualize'
                st.rerun()
        with col2:
            if st.button("ğŸ“„ ç›´æ¥ç”ŸæˆæŠ¥å‘Š", use_container_width=True):
                st.session_state.current_step = 'report'
                st.rerun()
    
    # ä¸Šä¸€æ­¥æŒ‰é’®
    if st.button("â¬…ï¸ è¿”å›æ•°æ®ä¸Šä¼ ", use_container_width=True):
        st.session_state.current_step = 'upload'
        st.rerun()


def execute_data_processing(analysis_option, processor, data):
    """æ‰§è¡Œæ•°æ®å¤„ç†åˆ†æ"""
    if analysis_option == "æ•°æ®æ¸…æ´—":
        execute_data_cleaning(processor, data)
    elif analysis_option == "æ•°æ®ç¼–ç ":
        execute_data_encoding(processor, data)
    elif analysis_option == "ç”Ÿæˆå˜é‡":
        execute_variable_generation(processor, data)
    elif analysis_option == "æ•°æ®æ ‡ç­¾è®¾ç½®":
        execute_data_labeling(processor, data)


def execute_data_cleaning(processor, data):
    """æ•°æ®æ¸…æ´—åŠŸèƒ½"""
    st.write("#### ğŸ”§ æ•°æ®æ¸…æ´—")
    
    # æ˜¾ç¤ºæ•°æ®è´¨é‡æ¦‚è§ˆ
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ€»è¡Œæ•°", data.shape[0])
    with col2:
        missing_count = data.isnull().sum().sum()
        st.metric("ç¼ºå¤±å€¼æ•°é‡", missing_count)
    with col3:
        duplicate_count = data.duplicated().sum()
        st.metric("é‡å¤è¡Œæ•°", duplicate_count)
    
    # ç¼ºå¤±å€¼å¤„ç†é€‰é¡¹
    st.write("##### ç¼ºå¤±å€¼å¤„ç†")
    missing_columns = data.columns[data.isnull().any()].tolist()
    if missing_columns:
        st.write("åŒ…å«ç¼ºå¤±å€¼çš„åˆ—ï¼š", ", ".join(missing_columns))
        missing_strategy = st.selectbox(
            "é€‰æ‹©å¤„ç†ç­–ç•¥",
            ["åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ", "å‡å€¼å¡«å……(æ•°å€¼åˆ—)", "ä¼—æ•°å¡«å……(æ‰€æœ‰åˆ—)", "å‰å‘å¡«å……", "åå‘å¡«å……"]
        )
        
        # æ‰§è¡Œç¼ºå¤±å€¼å¤„ç†
        if missing_strategy == "åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ":
            cleaned_data = data.dropna()
        elif missing_strategy == "å‡å€¼å¡«å……(æ•°å€¼åˆ—)":
            cleaned_data = data.copy()
            numeric_cols = data.select_dtypes(include=['number']).columns
            cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(cleaned_data[numeric_cols].mean())
        elif missing_strategy == "ä¼—æ•°å¡«å……(æ‰€æœ‰åˆ—)":
            cleaned_data = data.fillna(data.mode().iloc[0])
        elif missing_strategy == "å‰å‘å¡«å……":
            cleaned_data = data.fillna(method='ffill')
        else:  # åå‘å¡«å……
            cleaned_data = data.fillna(method='bfill')
            
        st.session_state.processed_data = cleaned_data
        st.success(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼æ•°æ®ä» {data.shape[0]} è¡Œå˜ä¸º {cleaned_data.shape[0]} è¡Œ")
    else:
        st.success("æ•°æ®ä¸­æ²¡æœ‰ç¼ºå¤±å€¼ï¼")
    
    # é‡å¤å€¼å¤„ç†
    if duplicate_count > 0:
        st.write("##### é‡å¤å€¼å¤„ç†")
        if st.button("åˆ é™¤é‡å¤è¡Œ"):
            cleaned_data = data.drop_duplicates()
            st.session_state.processed_data = cleaned_data
            st.success(f"å·²åˆ é™¤ {duplicate_count} è¡Œé‡å¤æ•°æ®")
    
    # æ˜¾ç¤ºæ¸…æ´—åçš„æ•°æ®é¢„è§ˆ
    current_data = st.session_state.processed_data if 'processed_data' in st.session_state else data
    st.write("##### æ•°æ®é¢„è§ˆ")
    st.dataframe(current_data.head())


def execute_general_methods(analysis_option, processor, data):
    """æ‰§è¡Œé€šç”¨æ–¹æ³•åˆ†æ"""
    if analysis_option == "é¢‘æ•°åˆ†æ":
        execute_frequency_analysis(data)
    elif analysis_option == "æè¿°ç»Ÿè®¡":
        execute_descriptive_statistics(data)
    elif analysis_option == "äº¤å‰åˆ†æ(å¡æ–¹)":
        execute_crosstab_analysis(data)
    elif analysis_option == "ç›¸å…³åˆ†æ":
        execute_correlation_analysis(data)
    elif analysis_option == "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ":
        execute_independent_ttest(data)
    elif analysis_option == "é…å¯¹æ ·æœ¬tæ£€éªŒ":
        execute_paired_ttest(data)


def execute_frequency_analysis(data):
    """é¢‘æ•°åˆ†æ"""
    st.write("#### ğŸ“Š é¢‘æ•°åˆ†æ")
    
    # é€‰æ‹©è¦åˆ†æçš„åˆ—
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    all_cols = categorical_cols + numeric_cols
    selected_col = st.selectbox("é€‰æ‹©è¦åˆ†æçš„å˜é‡", all_cols)
    
    if selected_col:
        # è®¡ç®—é¢‘æ•°
        if selected_col in categorical_cols:
            freq_table = data[selected_col].value_counts().reset_index()
            freq_table.columns = ['ç±»åˆ«', 'é¢‘æ•°']
            freq_table['ç™¾åˆ†æ¯”'] = (freq_table['é¢‘æ•°'] / freq_table['é¢‘æ•°'].sum() * 100).round(2)
        else:
            # æ•°å€¼åˆ—è¿›è¡Œåˆ†ç»„
            bins = st.slider("é€‰æ‹©åˆ†ç»„æ•°", 5, 20, 10)
            freq_table = pd.cut(data[selected_col], bins=bins).value_counts().reset_index()
            freq_table.columns = ['åŒºé—´', 'é¢‘æ•°'] 
            freq_table['ç™¾åˆ†æ¯”'] = (freq_table['é¢‘æ•°'] / freq_table['é¢‘æ•°'].sum() * 100).round(2)
        
        # æ˜¾ç¤ºç»“æœ
        st.write("##### é¢‘æ•°åˆ†å¸ƒè¡¨")
        st.dataframe(freq_table)
        
        # å­˜å‚¨ç»“æœ
        st.session_state.analysis_results = {
            'type': 'é¢‘æ•°åˆ†æ',
            'variable': selected_col,
            'frequency_table': freq_table
        }
        
        st.success("é¢‘æ•°åˆ†æå®Œæˆï¼")


def execute_descriptive_statistics(data):
    """æè¿°ç»Ÿè®¡"""
    st.write("#### ğŸ“ˆ æè¿°ç»Ÿè®¡")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if not numeric_cols:
        st.error("æ²¡æœ‰æ•°å€¼å‹å˜é‡å¯ä»¥è¿›è¡Œæè¿°ç»Ÿè®¡")
        return
    
    selected_cols = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°å€¼å˜é‡", numeric_cols, default=numeric_cols[:5])
    
    if selected_cols:
        # è®¡ç®—æè¿°ç»Ÿè®¡
        desc_stats = data[selected_cols].describe().round(3)
        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡é‡
        additional_stats = pd.DataFrame({
            col: {
                'ååº¦': data[col].skew(),
                'å³°åº¦': data[col].kurtosis(),
                'å˜å¼‚ç³»æ•°': data[col].std() / data[col].mean() if data[col].mean() != 0 else 0
            } for col in selected_cols
        }).round(3)
        
        # åˆå¹¶ç»Ÿè®¡ç»“æœ
        full_stats = pd.concat([desc_stats, additional_stats])
        
        st.write("##### æè¿°ç»Ÿè®¡ç»“æœ")
        st.dataframe(full_stats)
        
        # å­˜å‚¨ç»“æœ
        st.session_state.analysis_results = {
            'type': 'æè¿°ç»Ÿè®¡',
            'variables': selected_cols,
            'descriptive_stats': full_stats
        }
        
        st.success("æè¿°ç»Ÿè®¡åˆ†æå®Œæˆï¼")


def execute_questionnaire_analysis(analysis_option, processor, data):
    """æ‰§è¡Œé—®å·ç ”ç©¶åˆ†æ"""
    if analysis_option == "ä¿¡åº¦åˆ†æ":
        execute_reliability_analysis(processor, data)
    elif analysis_option == "æ•ˆåº¦åˆ†æ":
        execute_validity_analysis(processor, data)
    elif analysis_option == "å¤šé€‰é¢˜åˆ†æ":
        execute_multiple_choice_analysis(data)
    elif analysis_option == "é—®å·è´¨é‡è¯„ä¼°":
        execute_questionnaire_quality(data)


def execute_reliability_analysis(processor, data):
    """ä¿¡åº¦åˆ†æï¼ˆå…‹æœ—å·´èµ«Î±ç³»æ•°ï¼‰"""
    st.write("#### ğŸ“ ä¿¡åº¦åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 2:
        st.error("ä¿¡åº¦åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    selected_cols = st.multiselect(
        "é€‰æ‹©æ„æˆé‡è¡¨çš„é¢˜ç›®å˜é‡", 
        numeric_cols,
        help="é€‰æ‹©å±äºåŒä¸€ä¸ªé‡è¡¨æˆ–ç»´åº¦çš„é¢˜ç›®"
    )
    
    if len(selected_cols) >= 2:
        try:
            # æ‰§è¡Œä¿¡åº¦åˆ†æ
            reliability_results = processor.reliability_analysis(data[selected_cols])
            
            st.write("##### ä¿¡åº¦åˆ†æç»“æœ")
            
            # æ˜¾ç¤ºCronbach's Alpha
            st.metric("å…‹æœ—å·´èµ«Î±ç³»æ•°", f"{reliability_results['cronbach_alpha']:.4f}")
            
            # ä¿¡åº¦æ°´å¹³åˆ¤æ–­
            alpha = reliability_results['cronbach_alpha']
            if alpha >= 0.9:
                reliability_level = "ä¼˜ç§€"
                level_color = "ğŸŸ¢"
            elif alpha >= 0.8:
                reliability_level = "è‰¯å¥½" 
                level_color = "ğŸ”µ"
            elif alpha >= 0.7:
                reliability_level = "å¯æ¥å—"
                level_color = "ğŸŸ¡"
            else:
                reliability_level = "è¾ƒå·®"
                level_color = "ğŸ”´"
            
            st.write(f"**ä¿¡åº¦æ°´å¹³:** {level_color} {reliability_level}")
            
            # é¡¹ç›®åˆ†æè¡¨
            if 'item_analysis' in reliability_results:
                st.write("##### é¡¹ç›®åˆ†æ")
                st.dataframe(reliability_results['item_analysis'])
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'ä¿¡åº¦åˆ†æ',
                'variables': selected_cols,
                'reliability_results': reliability_results
            }
            
            st.success("ä¿¡åº¦åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"ä¿¡åº¦åˆ†æå¤±è´¥: {str(e)}")


def execute_advanced_methods(analysis_option, processor, data):
    """æ‰§è¡Œè¿›é˜¶æ–¹æ³•åˆ†æ"""
    if analysis_option == "çº¿æ€§å›å½’":
        execute_linear_regression(data)
    elif analysis_option == "é€»è¾‘å›å½’":
        execute_logistic_regression(data)
    elif analysis_option == "èšç±»åˆ†æ":
        execute_cluster_analysis(data)
    elif analysis_option == "å› å­åˆ†æ":
        execute_factor_analysis(data)
    elif analysis_option == "ä¸»æˆåˆ†åˆ†æ":
        execute_pca_analysis(data)
    elif analysis_option == "æ–¹å·®åˆ†æ":
        execute_anova_analysis(data)


def execute_linear_regression(data):
    """çº¿æ€§å›å½’åˆ†æ"""
    st.write("#### ğŸ“ˆ çº¿æ€§å›å½’åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 2:
        st.error("çº¿æ€§å›å½’éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©å› å˜é‡å’Œè‡ªå˜é‡
    y_var = st.selectbox("é€‰æ‹©å› å˜é‡(Y)", numeric_cols)
    x_vars = st.multiselect("é€‰æ‹©è‡ªå˜é‡(X)", [col for col in numeric_cols if col != y_var])
    
    if y_var and x_vars:
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score, mean_squared_error
        import numpy as np
        
        try:
            # å‡†å¤‡æ•°æ®
            X = data[x_vars].dropna()
            y = data[y_var].dropna()
            
            # ç¡®ä¿Xå’Œyçš„ç´¢å¼•ä¸€è‡´
            common_index = X.index.intersection(y.index)
            X = X.loc[common_index]
            y = y.loc[common_index]
            
            # æ‹Ÿåˆæ¨¡å‹
            model = LinearRegression()
            model.fit(X, y)
            
            # é¢„æµ‹
            y_pred = model.predict(X)
            
            # è®¡ç®—ç»Ÿè®¡é‡
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))
            
            # æ˜¾ç¤ºç»“æœ
            st.write("##### å›å½’åˆ†æç»“æœ")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("RÂ²å†³å®šç³»æ•°", f"{r2:.4f}")
            with col2:
                st.metric("RMSE", f"{rmse:.4f}")
            
            # ç³»æ•°è¡¨
            coef_df = pd.DataFrame({
                'å˜é‡': ['å¸¸æ•°é¡¹'] + x_vars,
                'ç³»æ•°': [model.intercept_] + model.coef_.tolist()
            })
            coef_df['ç³»æ•°'] = coef_df['ç³»æ•°'].round(4)
            
            st.write("##### å›å½’ç³»æ•°")
            st.dataframe(coef_df)
            
            # å›å½’æ–¹ç¨‹
            equation_parts = [f"{model.intercept_:.4f}"]
            for var, coef in zip(x_vars, model.coef_):
                sign = "+" if coef >= 0 else ""
                equation_parts.append(f"{sign}{coef:.4f}*{var}")
            
            equation = f"{y_var} = " + " ".join(equation_parts)
            st.write("##### å›å½’æ–¹ç¨‹")
            st.code(equation)
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'çº¿æ€§å›å½’',
                'dependent_var': y_var,
                'independent_vars': x_vars,
                'r2_score': r2,
                'rmse': rmse,
                'coefficients': coef_df,
                'equation': equation
            }
            
            st.success("çº¿æ€§å›å½’åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"å›å½’åˆ†æå¤±è´¥: {str(e)}")


def execute_machine_learning(analysis_option, processor, data):
    """æ‰§è¡Œæœºå™¨å­¦ä¹ åˆ†æ"""
    st.write(f"#### ğŸ¤– {analysis_option}")
    st.info("æœºå™¨å­¦ä¹ æ¨¡å—æ­£åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ï¼")


def execute_time_series(analysis_option, processor, data):
    """æ‰§è¡Œæ—¶é—´åºåˆ—åˆ†æ"""
    st.write(f"#### ğŸ“Š {analysis_option}")
    st.info("æ—¶é—´åºåˆ—åˆ†ææ¨¡å—æ­£åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ï¼")


def display_analysis_results():
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    if 'analysis_results' not in st.session_state:
        return
    
    results = st.session_state.analysis_results
    
    st.write("### ğŸ“‹ åˆ†æç»“æœæ‘˜è¦")
    st.write(f"**åˆ†æç±»å‹:** {results['type']}")
    
    if results['type'] == 'é¢‘æ•°åˆ†æ':
        st.write(f"**åˆ†æå˜é‡:** {results['variable']}")
        st.dataframe(results['frequency_table'])
    
    elif results['type'] == 'æè¿°ç»Ÿè®¡':
        st.write(f"**åˆ†æå˜é‡:** {', '.join(results['variables'])}")
        st.dataframe(results['descriptive_stats'])
    
    elif results['type'] == 'ä¿¡åº¦åˆ†æ':
        st.write(f"**é‡è¡¨å˜é‡:** {', '.join(results['variables'])}")
        st.metric("å…‹æœ—å·´èµ«Î±ç³»æ•°", f"{results['reliability_results']['cronbach_alpha']:.4f}")
    
    elif results['type'] == 'çº¿æ€§å›å½’':
        st.write(f"**å› å˜é‡:** {results['dependent_var']}")
        st.write(f"**è‡ªå˜é‡:** {', '.join(results['independent_vars'])}")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("RÂ²", f"{results['r2_score']:.4f}")
        with col2:
            st.metric("RMSE", f"{results['rmse']:.4f}")


# è¾…åŠ©å‡½æ•°å®ç°
def execute_data_encoding(processor, data):
    """æ•°æ®ç¼–ç """
    st.write("#### ğŸ”¢ æ•°æ®ç¼–ç ")
    st.info("æ•°æ®ç¼–ç åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_variable_generation(processor, data):
    """ç”Ÿæˆå˜é‡"""
    st.write("#### â• ç”Ÿæˆå˜é‡")
    st.info("å˜é‡ç”ŸæˆåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_data_labeling(processor, data):
    """æ•°æ®æ ‡ç­¾è®¾ç½®"""
    st.write("#### ğŸ·ï¸ æ•°æ®æ ‡ç­¾è®¾ç½®")
    st.info("æ•°æ®æ ‡ç­¾åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_crosstab_analysis(data):
    """äº¤å‰åˆ†æ(å¡æ–¹æ£€éªŒ)"""
    st.write("#### ï¿½ äº¤å‰åˆ†æ(å¡æ–¹æ£€éªŒ)")
    
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    if len(categorical_cols) < 2:
        st.error("äº¤å‰åˆ†æéœ€è¦è‡³å°‘2ä¸ªåˆ†ç±»å˜é‡")
        return
    
    # é€‰æ‹©åˆ†æå˜é‡
    col1, col2 = st.columns(2)
    with col1:
        var1 = st.selectbox("é€‰æ‹©è¡Œå˜é‡", categorical_cols)
    with col2:
        remaining_cols = [col for col in categorical_cols if col != var1]
        var2 = st.selectbox("é€‰æ‹©åˆ—å˜é‡", remaining_cols)
    
    # åˆ†æå‚æ•°
    alpha = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
    
    if st.button("æ‰§è¡Œäº¤å‰åˆ†æ"):
        try:
            from scipy.stats import chi2_contingency
            from scipy.stats.contingency import expected_freq
            
            # åˆ›å»ºäº¤å‰è¡¨
            crosstab = pd.crosstab(data[var1], data[var2], margins=True)
            
            # ç§»é™¤è¾¹é™…æ€»è®¡è¿›è¡Œå¡æ–¹æ£€éªŒ
            observed = crosstab.iloc[:-1, :-1]
            
            # æ‰§è¡Œå¡æ–¹æ£€éªŒ
            chi2, p_value, dof, expected = chi2_contingency(observed)
            
            # è®¡ç®—æ•ˆåº”é‡ (CramÃ©r's V)
            n = observed.sum().sum()
            cramers_v = np.sqrt(chi2 / (n * (min(observed.shape) - 1)))
            
            # æ˜¾ç¤ºäº¤å‰è¡¨
            st.write("##### é¢‘æ•°äº¤å‰è¡¨")
            st.dataframe(crosstab)
            
            # æ˜¾ç¤ºæœŸæœ›é¢‘æ•°
            st.write("##### æœŸæœ›é¢‘æ•°")
            expected_df = pd.DataFrame(expected, 
                                     columns=observed.columns, 
                                     index=observed.index)
            st.dataframe(expected_df.round(2))
            
            # æ˜¾ç¤ºç™¾åˆ†æ¯”äº¤å‰è¡¨
            st.write("##### ç™¾åˆ†æ¯”äº¤å‰è¡¨")
            percent_tab = pd.crosstab(data[var1], data[var2], normalize='index') * 100
            st.dataframe(percent_tab.round(2))
            
            # ç»Ÿè®¡æ£€éªŒç»“æœ
            st.write("##### å¡æ–¹æ£€éªŒç»“æœ")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("å¡æ–¹å€¼", f"{chi2:.4f}")
            with col2:
                st.metric("è‡ªç”±åº¦", dof)
            with col3:
                st.metric("på€¼", f"{p_value:.4f}")
            with col4:
                st.metric("CramÃ©r's V", f"{cramers_v:.4f}")
            
            # ç»“æœè§£é‡Š
            significance = "æ˜¾è‘—" if p_value <= alpha else "ä¸æ˜¾è‘—"
            effect_size = "å¤§" if cramers_v >= 0.5 else "ä¸­" if cramers_v >= 0.3 else "å°"
            
            st.write("##### ç»“æœè§£é‡Š")
            st.write(f"- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {significance} (Î± = {alpha})")
            st.write(f"- **æ•ˆåº”é‡**: {effect_size} (CramÃ©r's V = {cramers_v:.4f})")
            
            if p_value <= alpha:
                st.success(f"{var1} å’Œ {var2} ä¹‹é—´å­˜åœ¨æ˜¾è‘—å…³è”")
            else:
                st.info(f"{var1} å’Œ {var2} ä¹‹é—´æ— æ˜¾è‘—å…³è”")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'äº¤å‰åˆ†æ',
                'variables': [var1, var2],
                'crosstab': crosstab,
                'chi2': chi2,
                'p_value': p_value,
                'dof': dof,
                'cramers_v': cramers_v,
                'significance': significance
            }
            
            st.success("äº¤å‰åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"äº¤å‰åˆ†æå¤±è´¥: {str(e)}")


def execute_correlation_analysis(data):
    """ç›¸å…³åˆ†æ"""
    st.write("#### ğŸ”— ç›¸å…³åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 2:
        st.error("ç›¸å…³åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©åˆ†æå˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©è¦åˆ†æçš„æ•°å€¼å˜é‡", 
        numeric_cols,
        default=numeric_cols[:min(8, len(numeric_cols))],
        help="é€‰æ‹©ç”¨äºç›¸å…³åˆ†æçš„å˜é‡"
    )
    
    if len(selected_cols) < 2:
        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªå˜é‡è¿›è¡Œç›¸å…³åˆ†æ")
        return
    
    # ç›¸å…³åˆ†æå‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        method = st.selectbox("ç›¸å…³ç³»æ•°ç±»å‹", ["Pearson", "Spearman", "Kendall"])
        alpha = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
    
    with col2:
        min_corr = st.slider("æœ€å°ç›¸å…³ç³»æ•°é˜ˆå€¼", 0.0, 1.0, 0.3, 0.1)
        show_pvalues = st.checkbox("æ˜¾ç¤ºpå€¼", value=True)
    
    if st.button("æ‰§è¡Œç›¸å…³åˆ†æ"):
        try:
            import seaborn as sns
            import matplotlib.pyplot as plt
            from scipy.stats import pearsonr, spearmanr, kendalltau
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_data = data[selected_cols].dropna()
            
            if method == "Pearson":
                corr_matrix = corr_data.corr(method='pearson')
            elif method == "Spearman":
                corr_matrix = corr_data.corr(method='spearman')
            else:  # Kendall
                corr_matrix = corr_data.corr(method='kendall')
            
            # è®¡ç®—på€¼çŸ©é˜µ
            n = len(selected_cols)
            p_matrix = np.zeros((n, n))
            
            for i, col1 in enumerate(selected_cols):
                for j, col2 in enumerate(selected_cols):
                    if i != j:
                        x = corr_data[col1].dropna()
                        y = corr_data[col2].dropna()
                        # ç¡®ä¿ä¸¤ä¸ªå˜é‡æœ‰ç›¸åŒçš„ç´¢å¼•
                        common_idx = x.index.intersection(y.index)
                        x = x.loc[common_idx]
                        y = y.loc[common_idx]
                        
                        if len(x) > 2:
                            if method == "Pearson":
                                _, p_val = pearsonr(x, y)
                            elif method == "Spearman":
                                _, p_val = spearmanr(x, y)
                            else:  # Kendall
                                _, p_val = kendalltau(x, y)
                            p_matrix[i, j] = p_val
                        else:
                            p_matrix[i, j] = 1.0
                    else:
                        p_matrix[i, j] = 0.0
            
            # æ˜¾ç¤ºç›¸å…³ç³»æ•°çŸ©é˜µ
            st.write("##### ç›¸å…³ç³»æ•°çŸ©é˜µ")
            
            # åˆ›å»ºçƒ­åŠ›å›¾
            fig, ax = plt.subplots(figsize=(10, 8))
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
            ax.set_title(f'{method}ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾')
            st.pyplot(fig)
            plt.close(fig)
            
            # æ˜¾ç¤ºæ•°å€¼ç»“æœ
            st.dataframe(corr_matrix.round(3))
            
            if show_pvalues:
                st.write("##### på€¼çŸ©é˜µ")
                p_df = pd.DataFrame(p_matrix, columns=selected_cols, index=selected_cols)
                st.dataframe(p_df.round(4))
            
            # å¼ºç›¸å…³å…³ç³»è¯†åˆ«
            st.write("##### å¼ºç›¸å…³å…³ç³»è¯†åˆ«")
            strong_correlations = []
            
            for i in range(len(selected_cols)):
                for j in range(i+1, len(selected_cols)):
                    corr_val = corr_matrix.iloc[i, j]
                    p_val = p_matrix[i, j]
                    
                    if abs(corr_val) >= min_corr and p_val <= alpha:
                        strong_correlations.append({
                            'å˜é‡1': selected_cols[i],
                            'å˜é‡2': selected_cols[j],
                            'ç›¸å…³ç³»æ•°': round(corr_val, 4),
                            'på€¼': round(p_val, 4),
                            'ç›¸å…³å¼ºåº¦': 'å¼º' if abs(corr_val) >= 0.7 else 'ä¸­' if abs(corr_val) >= 0.5 else 'å¼±',
                            'æ˜¾è‘—æ€§': '***' if p_val <= 0.001 else '**' if p_val <= 0.01 else '*' if p_val <= 0.05 else 'ns'
                        })
            
            if strong_correlations:
                strong_df = pd.DataFrame(strong_correlations)
                st.dataframe(strong_df)
                st.info(f"å‘ç° {len(strong_correlations)} å¯¹æ˜¾è‘—ç›¸å…³å…³ç³»")
            else:
                st.info("åœ¨å½“å‰é˜ˆå€¼ä¸‹æœªå‘ç°æ˜¾è‘—ç›¸å…³å…³ç³»")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'ç›¸å…³åˆ†æ',
                'method': method,
                'variables': selected_cols,
                'correlation_matrix': corr_matrix,
                'p_values': p_df if show_pvalues else None,
                'strong_correlations': strong_correlations,
                'n_samples': len(corr_data)
            }
            
            st.success("ç›¸å…³åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"ç›¸å…³åˆ†æå¤±è´¥: {str(e)}")


def execute_independent_ttest(data):
    """ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ"""
    st.write("#### ï¿½ ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if not numeric_cols or not categorical_cols:
        st.error("tæ£€éªŒéœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼å‹å˜é‡å’Œä¸€ä¸ªåˆ†ç±»å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        dependent_var = st.selectbox("é€‰æ‹©å› å˜é‡(æ•°å€¼å‹)", numeric_cols)
    with col2:
        # è¿‡æ»¤åªæœ‰2ä¸ªå”¯ä¸€å€¼çš„åˆ†ç±»å˜é‡
        valid_cats = []
        for col in categorical_cols:
            unique_vals = data[col].dropna().nunique()
            if unique_vals == 2:
                valid_cats.append(col)
        
        if not valid_cats:
            st.error("éœ€è¦ä¸€ä¸ªåªæœ‰ä¸¤ä¸ªç±»åˆ«çš„åˆ†ç»„å˜é‡")
            return
            
        group_var = st.selectbox("é€‰æ‹©åˆ†ç»„å˜é‡(2ç±»åˆ«)", valid_cats)
    
    # æ£€éªŒå‚æ•°
    alpha = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
    equal_var = st.checkbox("å‡è®¾æ–¹å·®ç›¸ç­‰", value=True)
    
    if st.button("æ‰§è¡Œtæ£€éªŒ"):
        try:
            from scipy.stats import ttest_ind, levene
            from scipy import stats
            
            # å‡†å¤‡æ•°æ®
            clean_data = data[[dependent_var, group_var]].dropna()
            groups = clean_data[group_var].unique()
            
            group1_data = clean_data[clean_data[group_var] == groups[0]][dependent_var]
            group2_data = clean_data[clean_data[group_var] == groups[1]][dependent_var]
            
            # æè¿°ç»Ÿè®¡
            st.write("##### ç»„é—´æè¿°ç»Ÿè®¡")
            desc_stats = pd.DataFrame({
                'ç»„åˆ«': [str(groups[0]), str(groups[1])],
                'æ ·æœ¬é‡': [len(group1_data), len(group2_data)],
                'å‡å€¼': [group1_data.mean(), group2_data.mean()],
                'æ ‡å‡†å·®': [group1_data.std(), group2_data.std()],
                'æ ‡å‡†è¯¯': [group1_data.sem(), group2_data.sem()],
                'æœ€å°å€¼': [group1_data.min(), group2_data.min()],
                'æœ€å¤§å€¼': [group1_data.max(), group2_data.max()]
            })
            st.dataframe(desc_stats.round(4))
            
            # æ–¹å·®é½æ€§æ£€éªŒ (Levene's test)
            levene_stat, levene_p = levene(group1_data, group2_data)
            
            st.write("##### æ–¹å·®é½æ€§æ£€éªŒ (Levene)")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Leveneç»Ÿè®¡é‡", f"{levene_stat:.4f}")
            with col2:
                st.metric("på€¼", f"{levene_p:.4f}")
            
            if levene_p <= 0.05:
                st.warning("âš ï¸ æ–¹å·®ä¸é½ (p â‰¤ 0.05)ï¼Œå»ºè®®ä½¿ç”¨Welch tæ£€éªŒ")
                equal_var = False
            else:
                st.success("âœ… æ–¹å·®é½æ€§å‡è®¾æˆç«‹ (p > 0.05)")
            
            # æ‰§è¡Œtæ£€éªŒ
            t_stat, p_value = ttest_ind(group1_data, group2_data, equal_var=equal_var)
            
            # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
            pooled_std = np.sqrt(((len(group1_data) - 1) * group1_data.var() + 
                                 (len(group2_data) - 1) * group2_data.var()) / 
                                (len(group1_data) + len(group2_data) - 2))
            cohens_d = (group1_data.mean() - group2_data.mean()) / pooled_std
            
            # è‡ªç”±åº¦
            if equal_var:
                df = len(group1_data) + len(group2_data) - 2
            else:
                # Welch-Satterthwaiteæ–¹ç¨‹
                s1, s2 = group1_data.var(), group2_data.var()
                n1, n2 = len(group1_data), len(group2_data)
                df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))
            
            # æ˜¾ç¤ºtæ£€éªŒç»“æœ
            st.write("##### tæ£€éªŒç»“æœ")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("tç»Ÿè®¡é‡", f"{t_stat:.4f}")
            with col2:
                st.metric("è‡ªç”±åº¦", f"{df:.1f}")
            with col3:
                st.metric("på€¼", f"{p_value:.4f}")
            with col4:
                st.metric("Cohen's d", f"{cohens_d:.4f}")
            
            # ç»“æœè§£é‡Š
            significance = "æ˜¾è‘—" if p_value <= alpha else "ä¸æ˜¾è‘—"
            effect_size = "å¤§" if abs(cohens_d) >= 0.8 else "ä¸­" if abs(cohens_d) >= 0.5 else "å°"
            
            st.write("##### ç»“æœè§£é‡Š")
            st.write(f"- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {significance} (Î± = {alpha})")
            st.write(f"- **æ•ˆåº”é‡**: {effect_size} (|Cohen's d| = {abs(cohens_d):.4f})")
            st.write(f"- **æ£€éªŒç±»å‹**: {'Student tæ£€éªŒ' if equal_var else 'Welch tæ£€éªŒ'}")
            
            if p_value <= alpha:
                st.success(f"ä¸¤ç»„åœ¨{dependent_var}ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚")
            else:
                st.info(f"ä¸¤ç»„åœ¨{dependent_var}ä¸Šæ— æ˜¾è‘—å·®å¼‚")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ',
                'dependent_variable': dependent_var,
                'group_variable': group_var,
                'groups': groups.tolist(),
                't_statistic': t_stat,
                'p_value': p_value,
                'degrees_of_freedom': df,
                'cohens_d': cohens_d,
                'equal_var': equal_var,
                'levene_p': levene_p,
                'significance': significance
            }
            
            st.success("tæ£€éªŒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"tæ£€éªŒå¤±è´¥: {str(e)}")


def execute_paired_ttest(data):
    """é…å¯¹æ ·æœ¬tæ£€éªŒ"""
    st.write("#### ï¿½ é…å¯¹æ ·æœ¬tæ£€éªŒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    if len(numeric_cols) < 2:
        st.error("é…å¯¹tæ£€éªŒéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©é…å¯¹å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        var1 = st.selectbox("é€‰æ‹©å˜é‡1", numeric_cols)
    with col2:
        remaining_vars = [col for col in numeric_cols if col != var1]
        var2 = st.selectbox("é€‰æ‹©å˜é‡2", remaining_vars)
    
    # æ£€éªŒå‚æ•°
    alpha = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
    
    if st.button("æ‰§è¡Œé…å¯¹tæ£€éªŒ"):
        try:
            from scipy.stats import ttest_rel, shapiro
            
            # å‡†å¤‡æ•°æ®
            paired_data = data[[var1, var2]].dropna()
            
            if len(paired_data) < 3:
                st.error("é…å¯¹æ•°æ®å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæ£€éªŒ")
                return
            
            # è®¡ç®—å·®å€¼
            diff = paired_data[var1] - paired_data[var2]
            
            # æè¿°ç»Ÿè®¡
            st.write("##### é…å¯¹æ ·æœ¬æè¿°ç»Ÿè®¡")
            desc_stats = pd.DataFrame({
                'å˜é‡': [var1, var2, 'å·®å€¼'],
                'æ ·æœ¬é‡': [len(paired_data[var1]), len(paired_data[var2]), len(diff)],
                'å‡å€¼': [paired_data[var1].mean(), paired_data[var2].mean(), diff.mean()],
                'æ ‡å‡†å·®': [paired_data[var1].std(), paired_data[var2].std(), diff.std()],
                'æ ‡å‡†è¯¯': [paired_data[var1].sem(), paired_data[var2].sem(), diff.sem()],
                'æœ€å°å€¼': [paired_data[var1].min(), paired_data[var2].min(), diff.min()],
                'æœ€å¤§å€¼': [paired_data[var1].max(), paired_data[var2].max(), diff.max()]
            })
            st.dataframe(desc_stats.round(4))
            
            # æ­£æ€æ€§æ£€éªŒï¼ˆé’ˆå¯¹å·®å€¼ï¼‰
            if len(diff) >= 3:
                shapiro_stat, shapiro_p = shapiro(diff)
                
                st.write("##### å·®å€¼æ­£æ€æ€§æ£€éªŒ (Shapiro-Wilk)")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Wç»Ÿè®¡é‡", f"{shapiro_stat:.4f}")
                with col2:
                    st.metric("på€¼", f"{shapiro_p:.4f}")
                
                if shapiro_p <= 0.05:
                    st.warning("âš ï¸ å·®å€¼ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒ (p â‰¤ 0.05)ï¼Œç»“æœéœ€è°¨æ…è§£é‡Š")
                else:
                    st.success("âœ… å·®å€¼ç¬¦åˆæ­£æ€åˆ†å¸ƒå‡è®¾ (p > 0.05)")
            
            # æ‰§è¡Œé…å¯¹tæ£€éªŒ
            t_stat, p_value = ttest_rel(paired_data[var1], paired_data[var2])
            
            # è®¡ç®—æ•ˆåº”é‡ (Cohen's d for paired samples)
            cohens_d = diff.mean() / diff.std()
            
            # è‡ªç”±åº¦
            df = len(paired_data) - 1
            
            # 95%ç½®ä¿¡åŒºé—´
            from scipy.stats import t as t_dist
            ci_margin = t_dist.ppf(0.975, df) * diff.sem()
            ci_lower = diff.mean() - ci_margin
            ci_upper = diff.mean() + ci_margin
            
            # æ˜¾ç¤ºtæ£€éªŒç»“æœ
            st.write("##### é…å¯¹tæ£€éªŒç»“æœ")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("tç»Ÿè®¡é‡", f"{t_stat:.4f}")
            with col2:
                st.metric("è‡ªç”±åº¦", df)
            with col3:
                st.metric("på€¼", f"{p_value:.4f}")
            with col4:
                st.metric("Cohen's d", f"{cohens_d:.4f}")
            
            # å·®å€¼çš„ç½®ä¿¡åŒºé—´
            st.write("##### å·®å€¼çš„95%ç½®ä¿¡åŒºé—´")
            st.write(f"[{ci_lower:.4f}, {ci_upper:.4f}]")
            
            # ç»“æœè§£é‡Š
            significance = "æ˜¾è‘—" if p_value <= alpha else "ä¸æ˜¾è‘—"
            effect_size = "å¤§" if abs(cohens_d) >= 0.8 else "ä¸­" if abs(cohens_d) >= 0.5 else "å°"
            
            st.write("##### ç»“æœè§£é‡Š")
            st.write(f"- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {significance} (Î± = {alpha})")
            st.write(f"- **æ•ˆåº”é‡**: {effect_size} (|Cohen's d| = {abs(cohens_d):.4f})")
            st.write(f"- **å¹³å‡å·®å€¼**: {diff.mean():.4f}")
            
            if p_value <= alpha:
                direction = "æ˜¾è‘—å¢åŠ " if diff.mean() > 0 else "æ˜¾è‘—å‡å°‘"
                st.success(f"ä»{var2}åˆ°{var1}{direction}")
            else:
                st.info(f"{var1}å’Œ{var2}ä¹‹é—´æ— æ˜¾è‘—å·®å¼‚")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'é…å¯¹æ ·æœ¬tæ£€éªŒ',
                'variable1': var1,
                'variable2': var2,
                't_statistic': t_stat,
                'p_value': p_value,
                'degrees_of_freedom': df,
                'cohens_d': cohens_d,
                'mean_difference': diff.mean(),
                'confidence_interval': [ci_lower, ci_upper],
                'shapiro_p': shapiro_p if len(diff) >= 3 else None,
                'significance': significance
            }
            
            st.success("é…å¯¹tæ£€éªŒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"é…å¯¹tæ£€éªŒå¤±è´¥: {str(e)}")


def execute_validity_analysis(processor, data):
    """æ•ˆåº¦åˆ†æ"""
    st.write("#### ğŸ“ æ•ˆåº¦åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 3:
        st.error("æ•ˆåº¦åˆ†æéœ€è¦è‡³å°‘3ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©åˆ†æå˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©ç”¨äºæ•ˆåº¦åˆ†æçš„å˜é‡", 
        numeric_cols,
        default=numeric_cols[:min(10, len(numeric_cols))],
        help="é€‰æ‹©ç”¨äºæ•ˆåº¦åˆ†æçš„æ•°å€¼å‹å˜é‡"
    )
    
    if len(selected_cols) < 3:
        st.warning("è¯·è‡³å°‘é€‰æ‹©3ä¸ªå˜é‡è¿›è¡Œæ•ˆåº¦åˆ†æ")
        return
    
    # æ•ˆåº¦åˆ†æç±»å‹
    validity_type = st.selectbox(
        "é€‰æ‹©æ•ˆåº¦åˆ†æç±»å‹",
        ["å†…å®¹æ•ˆåº¦", "ç»“æ„æ•ˆåº¦(æ¢ç´¢æ€§å› å­åˆ†æ)", "èšåˆæ•ˆåº¦", "åŒºåˆ†æ•ˆåº¦"]
    )
    
    if st.button("æ‰§è¡Œæ•ˆåº¦åˆ†æ"):
        try:
            import numpy as np
            from sklearn.decomposition import PCA, FactorAnalysis
            from factor_analyzer import FactorAnalyzer
            from scipy.stats import pearsonr
            
            # å‡†å¤‡æ•°æ®
            validity_data = data[selected_cols].dropna()
            
            if validity_type == "å†…å®¹æ•ˆåº¦":
                st.write("##### å†…å®¹æ•ˆåº¦åˆ†æ")
                st.info("å†…å®¹æ•ˆåº¦ä¸»è¦é€šè¿‡ä¸“å®¶åˆ¤æ–­è¿›è¡Œè¯„ä¼°ï¼Œæ­¤å¤„æä¾›ç›¸å…³ç»Ÿè®¡ä¿¡æ¯ï¼š")
                
                # å˜é‡æè¿°ç»Ÿè®¡
                desc_stats = validity_data.describe()
                st.dataframe(desc_stats.round(4))
                
                # å˜é‡é—´ç›¸å…³æ€§
                corr_matrix = validity_data.corr()
                
                import matplotlib.pyplot as plt
                import seaborn as sns
                fig, ax = plt.subplots(figsize=(10, 8))
                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
                ax.set_title('å˜é‡é—´ç›¸å…³ç³»æ•°çŸ©é˜µ')
                st.pyplot(fig)
                plt.close(fig)
                
            elif validity_type == "ç»“æ„æ•ˆåº¦(æ¢ç´¢æ€§å› å­åˆ†æ)":
                st.write("##### ç»“æ„æ•ˆåº¦ - æ¢ç´¢æ€§å› å­åˆ†æ(EFA)")
                
                # æ•°æ®é€‚ç”¨æ€§æ£€éªŒ
                from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
                
                # Bartlett'sæ£€éªŒ
                chi_square_value, p_value = calculate_bartlett_sphericity(validity_data)
                st.write("**Bartlettçƒå½¢æ£€éªŒ:**")
                st.write(f"- å¡æ–¹å€¼: {chi_square_value:.4f}")
                st.write(f"- på€¼: {p_value:.4f}")
                
                if p_value < 0.05:
                    st.success("âœ… Bartlettæ£€éªŒæ˜¾è‘—ï¼Œæ•°æ®é€‚åˆå› å­åˆ†æ")
                else:
                    st.warning("âš ï¸ Bartlettæ£€éªŒä¸æ˜¾è‘—ï¼Œæ•°æ®å¯èƒ½ä¸é€‚åˆå› å­åˆ†æ")
                
                # KMOæ£€éªŒ
                kmo_all, kmo_model = calculate_kmo(validity_data)
                st.write("**KMOé‡‡æ ·é€‚åº¦é‡æ£€éªŒ:**")
                st.write(f"- æ€»ä½“KMOå€¼: {kmo_model:.4f}")
                
                if kmo_model >= 0.8:
                    st.success("âœ… KMO > 0.8ï¼Œéå¸¸é€‚åˆå› å­åˆ†æ")
                elif kmo_model >= 0.7:
                    st.success("âœ… KMO > 0.7ï¼Œé€‚åˆå› å­åˆ†æ")
                elif kmo_model >= 0.6:
                    st.info("â„¹ï¸ KMO > 0.6ï¼Œå‹‰å¼ºé€‚åˆå› å­åˆ†æ")
                else:
                    st.warning("âš ï¸ KMO < 0.6ï¼Œä¸é€‚åˆå› å­åˆ†æ")
                
                # å› å­æ•°é‡é€‰æ‹©
                n_factors = st.slider("é€‰æ‹©å› å­æ•°é‡", 1, min(len(selected_cols)-1, 8), 2)
                
                # æ‰§è¡Œå› å­åˆ†æ
                fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')
                fa.fit(validity_data)
                
                # å› å­è½½è·çŸ©é˜µ
                loadings = fa.loadings_
                loadings_df = pd.DataFrame(loadings, 
                                         columns=[f'å› å­{i+1}' for i in range(n_factors)],
                                         index=selected_cols)
                
                st.write("**å› å­è½½è·çŸ©é˜µ:**")
                st.dataframe(loadings_df.round(4))
                
                # ç‰¹å¾å€¼
                eigenvalues = fa.get_eigenvalues()[0]
                st.write("**ç‰¹å¾å€¼:**")
                eigenvalue_df = pd.DataFrame({
                    'å› å­': [f'å› å­{i+1}' for i in range(len(eigenvalues))],
                    'ç‰¹å¾å€¼': eigenvalues,
                    'æ–¹å·®è´¡çŒ®ç‡(%)': eigenvalues / len(selected_cols) * 100,
                    'ç´¯ç§¯è´¡çŒ®ç‡(%)': np.cumsum(eigenvalues) / len(selected_cols) * 100
                })
                st.dataframe(eigenvalue_df.round(4))
                
                # ç¢çŸ³å›¾
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(range(1, len(eigenvalues)+1), eigenvalues, 'bo-')
                ax.axhline(y=1, color='r', linestyle='--', label='ç‰¹å¾å€¼=1')
                ax.set_xlabel('å› å­åºå·')
                ax.set_ylabel('ç‰¹å¾å€¼')
                ax.set_title('ç¢çŸ³å›¾')
                ax.legend()
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                plt.close(fig)
                
            elif validity_type == "èšåˆæ•ˆåº¦":
                st.write("##### èšåˆæ•ˆåº¦åˆ†æ")
                
                # è®¡ç®—å¹³å‡æ–¹å·®æå–é‡(AVE)å’Œç»„åˆä¿¡åº¦(CR)
                # å‡è®¾æ‰€æœ‰å˜é‡å±äºåŒä¸€æ„å¿µ
                corr_matrix = validity_data.corr()
                
                # è®¡ç®—Cronbach's Alpha
                n_items = len(selected_cols)
                item_variances = validity_data.var()
                total_variance = validity_data.sum(axis=1).var()
                
                alpha = (n_items / (n_items - 1)) * (1 - item_variances.sum() / total_variance)
                
                st.write(f"**Cronbach's Î±ç³»æ•°**: {alpha:.4f}")
                
                if alpha >= 0.9:
                    st.success("âœ… Î± â‰¥ 0.9ï¼Œå†…éƒ¨ä¸€è‡´æ€§éå¸¸å¥½")
                elif alpha >= 0.8:
                    st.success("âœ… Î± â‰¥ 0.8ï¼Œå†…éƒ¨ä¸€è‡´æ€§è‰¯å¥½")
                elif alpha >= 0.7:
                    st.info("â„¹ï¸ Î± â‰¥ 0.7ï¼Œå†…éƒ¨ä¸€è‡´æ€§å¯æ¥å—")
                else:
                    st.warning("âš ï¸ Î± < 0.7ï¼Œå†…éƒ¨ä¸€è‡´æ€§è¾ƒå·®")
                
                # é¡¹ç›®-æ€»åˆ†ç›¸å…³
                st.write("**é¡¹ç›®-æ€»åˆ†ç›¸å…³åˆ†æ:**")
                total_score = validity_data.sum(axis=1)
                item_total_corr = []
                
                for col in selected_cols:
                    corr, p_val = pearsonr(validity_data[col], total_score - validity_data[col])
                    item_total_corr.append({
                        'é¡¹ç›®': col,
                        'é¡¹ç›®-æ€»åˆ†ç›¸å…³': corr,
                        'på€¼': p_val,
                        'åˆ é™¤è¯¥é¡¹ç›®åçš„Î±': np.nan  # å¯ä»¥è¿›ä¸€æ­¥è®¡ç®—
                    })
                
                item_corr_df = pd.DataFrame(item_total_corr)
                st.dataframe(item_corr_df.round(4))
                
            elif validity_type == "åŒºåˆ†æ•ˆåº¦":
                st.write("##### åŒºåˆ†æ•ˆåº¦åˆ†æ")
                
                # è®¡ç®—å˜é‡é—´ç›¸å…³ç³»æ•°
                corr_matrix = validity_data.corr()
                
                st.write("**å˜é‡é—´ç›¸å…³ç³»æ•°çŸ©é˜µ:**")
                st.dataframe(corr_matrix.round(4))
                
                # è¯†åˆ«é«˜ç›¸å…³é¡¹ç›®ï¼ˆå¯èƒ½ç¼ºä¹åŒºåˆ†æ•ˆåº¦ï¼‰
                high_corr_pairs = []
                threshold = 0.8
                
                for i in range(len(selected_cols)):
                    for j in range(i+1, len(selected_cols)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) >= threshold:
                            high_corr_pairs.append({
                                'å˜é‡1': selected_cols[i],
                                'å˜é‡2': selected_cols[j],
                                'ç›¸å…³ç³»æ•°': corr_val
                            })
                
                if high_corr_pairs:
                    st.write(f"**é«˜ç›¸å…³å˜é‡å¯¹ (|r| â‰¥ {threshold}):**")
                    high_corr_df = pd.DataFrame(high_corr_pairs)
                    st.dataframe(high_corr_df.round(4))
                    st.warning("âš ï¸ ä»¥ä¸Šå˜é‡å¯¹ç›¸å…³è¿‡é«˜ï¼Œå¯èƒ½ç¼ºä¹åŒºåˆ†æ•ˆåº¦")
                else:
                    st.success("âœ… æœªå‘ç°é«˜ç›¸å…³å˜é‡å¯¹ï¼ŒåŒºåˆ†æ•ˆåº¦è‰¯å¥½")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'æ•ˆåº¦åˆ†æ',
                'validity_type': validity_type,
                'variables': selected_cols,
                'sample_size': len(validity_data)
            }
            
            st.success("æ•ˆåº¦åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"æ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}")


def execute_multiple_choice_analysis(data):
    """å¤šé€‰é¢˜åˆ†æ"""
    st.write("#### â˜‘ï¸ å¤šé€‰é¢˜åˆ†æ")
    
    # è·å–æ‰€æœ‰åˆ—
    all_cols = data.columns.tolist()
    
    # é€‰æ‹©å¤šé€‰é¢˜ç›¸å…³åˆ—
    st.write("##### é€‰æ‹©å¤šé€‰é¢˜å˜é‡")
    selected_cols = st.multiselect(
        "é€‰æ‹©å¤šé€‰é¢˜çš„å„ä¸ªé€‰é¡¹å˜é‡",
        all_cols,
        help="é€‰æ‹©ä»£è¡¨å¤šé€‰é¢˜å„ä¸ªé€‰é¡¹çš„å˜é‡ï¼ˆé€šå¸¸ä¸º0/1ç¼–ç æˆ–æ˜¯/å¦ï¼‰"
    )
    
    if len(selected_cols) < 2:
        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªå¤šé€‰é¢˜é€‰é¡¹å˜é‡")
        return
    
    # å¤šé€‰é¢˜åˆ†æå‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        value_type = st.selectbox("æ•°æ®ç¼–ç ç±»å‹", ["0/1ç¼–ç ", "æ˜¯/å¦", "True/False", "è‡ªå®šä¹‰"])
        show_combination = st.checkbox("æ˜¾ç¤ºé€‰é¡¹ç»„åˆåˆ†æ", value=True)
    
    with col2:
        if value_type == "è‡ªå®šä¹‰":
            positive_value = st.text_input("æ­£å‘å€¼ï¼ˆé€‰ä¸­ï¼‰", "1")
            negative_value = st.text_input("è´Ÿå‘å€¼ï¼ˆæœªé€‰ä¸­ï¼‰", "0")
        else:
            positive_value = {"0/1ç¼–ç ": "1", "æ˜¯/å¦": "æ˜¯", "True/False": "True"}[value_type]
            negative_value = {"0/1ç¼–ç ": "0", "æ˜¯/å¦": "å¦", "True/False": "False"}[value_type]
    
    if st.button("æ‰§è¡Œå¤šé€‰é¢˜åˆ†æ"):
        try:
            # å‡†å¤‡æ•°æ®
            multi_data = data[selected_cols].copy()
            
            # è½¬æ¢ä¸ºç»Ÿä¸€çš„0/1ç¼–ç 
            for col in selected_cols:
                multi_data[col] = (multi_data[col].astype(str) == str(positive_value)).astype(int)
            
            # åŸºæœ¬ç»Ÿè®¡
            st.write("##### å„é€‰é¡¹é€‰æ‹©æƒ…å†µ")
            
            option_stats = []
            total_responses = len(multi_data)
            
            for col in selected_cols:
                selected_count = multi_data[col].sum()
                percentage = (selected_count / total_responses) * 100
                
                option_stats.append({
                    'é€‰é¡¹': col,
                    'é€‰æ‹©äººæ•°': selected_count,
                    'é€‰æ‹©ç‡(%)': percentage,
                    'æœªé€‰æ‹©äººæ•°': total_responses - selected_count,
                    'æœªé€‰æ‹©ç‡(%)': 100 - percentage
                })
            
            stats_df = pd.DataFrame(option_stats)
            st.dataframe(stats_df.round(2))
            
            # å¯è§†åŒ–é€‰æ‹©ç‡
            import matplotlib.pyplot as plt
            fig, ax = plt.subplots(figsize=(12, 6))
            
            bars = ax.bar(stats_df['é€‰é¡¹'], stats_df['é€‰æ‹©ç‡(%)'])
            ax.set_xlabel('é€‰é¡¹')
            ax.set_ylabel('é€‰æ‹©ç‡ (%)')
            ax.set_title('å„é€‰é¡¹é€‰æ‹©ç‡')
            ax.tick_params(axis='x', rotation=45)
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, percentage in zip(bars, stats_df['é€‰æ‹©ç‡(%)']):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{percentage:.1f}%', ha='center', va='bottom')
            
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)
            
            # é€‰é¡¹ç»„åˆåˆ†æ
            if show_combination:
                st.write("##### é€‰é¡¹ç»„åˆåˆ†æ")
                
                # è®¡ç®—æ¯ä¸ªäººé€‰æ‹©çš„é€‰é¡¹æ•°é‡
                multi_data['total_selected'] = multi_data[selected_cols].sum(axis=1)
                
                # é€‰æ‹©æ•°é‡åˆ†å¸ƒ
                selection_counts = multi_data['total_selected'].value_counts().sort_index()
                
                st.write("**é€‰æ‹©æ•°é‡åˆ†å¸ƒ:**")
                count_stats = pd.DataFrame({
                    'é€‰æ‹©æ•°é‡': selection_counts.index,
                    'äººæ•°': selection_counts.values,
                    'æ¯”ä¾‹(%)': (selection_counts.values / total_responses) * 100
                })
                st.dataframe(count_stats.round(2))
                
                # é€‰æ‹©æ•°é‡åˆ†å¸ƒå›¾
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.bar(count_stats['é€‰æ‹©æ•°é‡'], count_stats['æ¯”ä¾‹(%)'])
                ax.set_xlabel('é€‰æ‹©æ•°é‡')
                ax.set_ylabel('æ¯”ä¾‹ (%)')
                ax.set_title('é€‰æ‹©æ•°é‡åˆ†å¸ƒ')
                st.pyplot(fig)
                plt.close(fig)
                
                # é€‰é¡¹å…±ç°åˆ†æ
                if len(selected_cols) <= 10:  # é¿å…ç»„åˆè¿‡å¤š
                    st.write("**é€‰é¡¹å…±ç°çŸ©é˜µ:**")
                    
                    # è®¡ç®—é€‰é¡¹é—´çš„å…±ç°æ¬¡æ•°
                    cooccurrence = np.zeros((len(selected_cols), len(selected_cols)))
                    
                    for i, col1 in enumerate(selected_cols):
                        for j, col2 in enumerate(selected_cols):
                            if i != j:
                                # è®¡ç®—åŒæ—¶é€‰æ‹©ä¸¤ä¸ªé€‰é¡¹çš„äººæ•°
                                cooccurrence[i, j] = ((multi_data[col1] == 1) & (multi_data[col2] == 1)).sum()
                            else:
                                cooccurrence[i, j] = multi_data[col1].sum()
                    
                    cooccur_df = pd.DataFrame(cooccurrence, 
                                            columns=selected_cols, 
                                            index=selected_cols)
                    st.dataframe(cooccur_df.astype(int))
                    
                    # å…±ç°çƒ­åŠ›å›¾
                    import seaborn as sns
                    fig, ax = plt.subplots(figsize=(10, 8))
                    sns.heatmap(cooccur_df, annot=True, fmt='d', cmap='Blues', ax=ax)
                    ax.set_title('é€‰é¡¹å…±ç°çƒ­åŠ›å›¾')
                    st.pyplot(fig)
                    plt.close(fig)
                
                # æœ€å¸¸è§çš„é€‰é¡¹ç»„åˆ
                st.write("**æœ€å¸¸è§çš„é€‰é¡¹ç»„åˆ (Top 10):**")
                
                # åˆ›å»ºç»„åˆæ¨¡å¼
                multi_data['pattern'] = multi_data[selected_cols].apply(
                    lambda row: '+'.join([col for col, val in row.items() if col in selected_cols and val == 1]),
                    axis=1
                )
                
                pattern_counts = multi_data['pattern'].value_counts().head(10)
                
                pattern_stats = pd.DataFrame({
                    'é€‰é¡¹ç»„åˆ': pattern_counts.index,
                    'å‡ºç°æ¬¡æ•°': pattern_counts.values,
                    'æ¯”ä¾‹(%)': (pattern_counts.values / total_responses) * 100
                })
                
                # å¤„ç†ç©ºç»„åˆ
                pattern_stats['é€‰é¡¹ç»„åˆ'] = pattern_stats['é€‰é¡¹ç»„åˆ'].replace('', '(æœªé€‰æ‹©ä»»ä½•é€‰é¡¹)')
                
                st.dataframe(pattern_stats.round(2))
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'å¤šé€‰é¢˜åˆ†æ',
                'variables': selected_cols,
                'total_responses': total_responses,
                'option_stats': stats_df.to_dict('records'),
                'selection_distribution': count_stats.to_dict('records') if show_combination else None
            }
            
            st.success("å¤šé€‰é¢˜åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"å¤šé€‰é¢˜åˆ†æå¤±è´¥: {str(e)}")


def execute_questionnaire_quality(data):
    """é—®å·è´¨é‡è¯„ä¼°"""
    st.write("#### ğŸ“‹ é—®å·è´¨é‡è¯„ä¼°")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 3:
        st.error("é—®å·è´¨é‡è¯„ä¼°éœ€è¦è‡³å°‘3ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©é—®å·é¢˜ç›®å˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©é—®å·é¢˜ç›®å˜é‡",
        numeric_cols,
        default=numeric_cols[:min(20, len(numeric_cols))],
        help="é€‰æ‹©ä»£è¡¨é—®å·é¢˜ç›®çš„æ•°å€¼å‹å˜é‡"
    )
    
    if len(selected_cols) < 3:
        st.warning("è¯·è‡³å°‘é€‰æ‹©3ä¸ªé—®å·é¢˜ç›®å˜é‡")
        return
    
    # è¯„ä¼°å‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        scale_type = st.selectbox("é‡è¡¨ç±»å‹", ["Likerté‡è¡¨", "è¯­ä¹‰å·®å¼‚é‡è¡¨", "å…¶ä»–"])
        if scale_type == "Likerté‡è¡¨":
            scale_range = st.selectbox("é‡è¡¨èŒƒå›´", ["1-5", "1-7", "1-10", "è‡ªå®šä¹‰"])
            if scale_range == "è‡ªå®šä¹‰":
                min_val = st.number_input("æœ€å°å€¼", value=1)
                max_val = st.number_input("æœ€å¤§å€¼", value=5)
            else:
                min_val, max_val = map(int, scale_range.split('-'))
    
    with col2:
        check_outliers = st.checkbox("æ£€æŸ¥å¼‚å¸¸å€¼", value=True)
        check_missing = st.checkbox("æ£€æŸ¥ç¼ºå¤±å€¼æ¨¡å¼", value=True)
    
    if st.button("æ‰§è¡Œé—®å·è´¨é‡è¯„ä¼°"):
        try:
            # å‡†å¤‡æ•°æ®
            quality_data = data[selected_cols].copy()
            
            st.write("##### ğŸ“Š åŸºæœ¬æ•°æ®è´¨é‡")
            
            # 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            basic_stats = []
            total_responses = len(quality_data)
            
            for col in selected_cols:
                col_data = quality_data[col]
                
                basic_stats.append({
                    'é¢˜ç›®': col,
                    'æœ‰æ•ˆå›ç­”æ•°': col_data.count(),
                    'ç¼ºå¤±å€¼æ•°': col_data.isnull().sum(),
                    'ç¼ºå¤±ç‡(%)': (col_data.isnull().sum() / total_responses) * 100,
                    'å‡å€¼': col_data.mean(),
                    'æ ‡å‡†å·®': col_data.std(),
                    'æœ€å°å€¼': col_data.min(),
                    'æœ€å¤§å€¼': col_data.max()
                })
            
            basic_df = pd.DataFrame(basic_stats)
            st.dataframe(basic_df.round(4))
            
            # 2. ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ
            if check_missing:
                st.write("##### ğŸ“ ç¼ºå¤±å€¼æ¨¡å¼åˆ†æ")
                
                missing_pattern = quality_data.isnull().sum()
                if missing_pattern.sum() > 0:
                    st.write("**å„é¢˜ç›®ç¼ºå¤±å€¼ç»Ÿè®¡:**")
                    missing_df = pd.DataFrame({
                        'é¢˜ç›®': missing_pattern.index,
                        'ç¼ºå¤±å€¼æ•°': missing_pattern.values,
                        'ç¼ºå¤±ç‡(%)': (missing_pattern.values / total_responses) * 100
                    })
                    st.dataframe(missing_df.round(2))
                    
                    # ç¼ºå¤±å€¼å¯è§†åŒ–
                    import matplotlib.pyplot as plt
                    fig, ax = plt.subplots(figsize=(12, 6))
                    bars = ax.bar(missing_df['é¢˜ç›®'], missing_df['ç¼ºå¤±ç‡(%)'])
                    ax.set_xlabel('é¢˜ç›®')
                    ax.set_ylabel('ç¼ºå¤±ç‡ (%)')
                    ax.set_title('å„é¢˜ç›®ç¼ºå¤±ç‡')
                    ax.tick_params(axis='x', rotation=45)
                    
                    # æ ‡è®°é«˜ç¼ºå¤±ç‡çš„é¢˜ç›®
                    for bar, rate in zip(bars, missing_df['ç¼ºå¤±ç‡(%)']):
                        if rate > 10:  # ç¼ºå¤±ç‡è¶…è¿‡10%
                            bar.set_color('red')
                            height = bar.get_height()
                            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                                   f'{rate:.1f}%', ha='center', va='bottom', color='red')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close(fig)
                    
                    # ç¼ºå¤±å€¼å»ºè®®
                    high_missing = missing_df[missing_df['ç¼ºå¤±ç‡(%)'] > 10]
                    if not high_missing.empty:
                        st.warning(f"âš ï¸ ä»¥ä¸‹é¢˜ç›®ç¼ºå¤±ç‡è¶…è¿‡10%ï¼Œå»ºè®®æ£€æŸ¥: {', '.join(high_missing['é¢˜ç›®'].tolist())}")
                else:
                    st.success("âœ… æ‰€æœ‰é¢˜ç›®éƒ½æ²¡æœ‰ç¼ºå¤±å€¼")
            
            # 3. å¼‚å¸¸å€¼æ£€æµ‹
            if check_outliers:
                st.write("##### ğŸ¯ å¼‚å¸¸å€¼æ£€æµ‹")
                
                # å¦‚æœæ˜¯Likerté‡è¡¨ï¼Œæ£€æŸ¥è¶…å‡ºèŒƒå›´çš„å€¼
                if scale_type == "Likerté‡è¡¨":
                    out_of_range = []
                    for col in selected_cols:
                        col_data = quality_data[col].dropna()
                        below_min = (col_data < min_val).sum()
                        above_max = (col_data > max_val).sum()
                        
                        if below_min > 0 or above_max > 0:
                            out_of_range.append({
                                'é¢˜ç›®': col,
                                f'å°äº{min_val}çš„å€¼': below_min,
                                f'å¤§äº{max_val}çš„å€¼': above_max,
                                'å¼‚å¸¸å€¼æ€»æ•°': below_min + above_max
                            })
                    
                    if out_of_range:
                        st.write("**è¶…å‡ºé‡è¡¨èŒƒå›´çš„å€¼:**")
                        out_range_df = pd.DataFrame(out_of_range)
                        st.dataframe(out_range_df)
                        st.warning("âš ï¸ å‘ç°è¶…å‡ºé‡è¡¨èŒƒå›´çš„å¼‚å¸¸å€¼ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å½•å…¥")
                    else:
                        st.success("âœ… æ‰€æœ‰å€¼éƒ½åœ¨é‡è¡¨èŒƒå›´å†…")
                
                # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
                outlier_stats = []
                for col in selected_cols:
                    col_data = quality_data[col].dropna()
                    if len(col_data) > 0:
                        Q1 = col_data.quantile(0.25)
                        Q3 = col_data.quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        
                        outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
                        
                        outlier_stats.append({
                            'é¢˜ç›®': col,
                            'å¼‚å¸¸å€¼æ•°é‡': len(outliers),
                            'å¼‚å¸¸å€¼æ¯”ä¾‹(%)': (len(outliers) / len(col_data)) * 100,
                            'ä¸‹ç•Œ': lower_bound,
                            'ä¸Šç•Œ': upper_bound
                        })
                
                outlier_df = pd.DataFrame(outlier_stats)
                st.write("**IQRæ–¹æ³•å¼‚å¸¸å€¼æ£€æµ‹:**")
                st.dataframe(outlier_df.round(4))
            
            # 4. åå‘é¢˜æ£€æµ‹
            st.write("##### ğŸ”„ åå‘é¢˜ä¸€è‡´æ€§æ£€æŸ¥")
            
            # è®¡ç®—é¢˜ç›®é—´ç›¸å…³ç³»æ•°
            corr_matrix = quality_data.corr()
            
            # è¯†åˆ«å¯èƒ½çš„åå‘é¢˜ï¼ˆä¸å…¶ä»–é¢˜ç›®æ™®éè´Ÿç›¸å…³ï¼‰
            negative_corr_items = []
            for col in selected_cols:
                col_corrs = corr_matrix[col].drop(col)  # æ’é™¤è‡ªç›¸å…³
                avg_corr = col_corrs.mean()
                negative_corr_count = (col_corrs < 0).sum()
                
                if avg_corr < 0 or negative_corr_count > len(col_corrs) * 0.5:
                    negative_corr_items.append({
                        'é¢˜ç›®': col,
                        'å¹³å‡ç›¸å…³ç³»æ•°': avg_corr,
                        'è´Ÿç›¸å…³é¢˜ç›®æ•°': negative_corr_count,
                        'è´Ÿç›¸å…³æ¯”ä¾‹(%)': (negative_corr_count / len(col_corrs)) * 100
                    })
            
            if negative_corr_items:
                st.write("**å¯èƒ½çš„åå‘é¢˜ç›®:**")
                reverse_df = pd.DataFrame(negative_corr_items)
                st.dataframe(reverse_df.round(4))
                st.info("â„¹ï¸ ä»¥ä¸Šé¢˜ç›®å¯èƒ½ä¸ºåå‘é¢˜ï¼Œè¯·æ£€æŸ¥æ˜¯å¦éœ€è¦åå‘ç¼–ç ")
            else:
                st.success("âœ… æœªå‘ç°æ˜æ˜¾çš„åå‘é¢˜ç›®")
            
            # 5. å†…éƒ¨ä¸€è‡´æ€§è¯„ä¼°
            st.write("##### ğŸ“ˆ å†…éƒ¨ä¸€è‡´æ€§è¯„ä¼°")
            
            # Cronbach's Alpha
            valid_data = quality_data.dropna()
            if len(valid_data) > 0 and len(selected_cols) > 1:
                n_items = len(selected_cols)
                item_variances = valid_data.var()
                total_variance = valid_data.sum(axis=1).var()
                
                if total_variance > 0:
                    alpha = (n_items / (n_items - 1)) * (1 - item_variances.sum() / total_variance)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Cronbach's Î±", f"{alpha:.4f}")
                    with col2:
                        st.metric("é¢˜ç›®æ•°é‡", n_items)
                    with col3:
                        st.metric("æœ‰æ•ˆæ ·æœ¬", len(valid_data))
                    
                    # Alphaè§£é‡Š
                    if alpha >= 0.9:
                        st.success("âœ… Î± â‰¥ 0.9ï¼Œå†…éƒ¨ä¸€è‡´æ€§æä½³")
                    elif alpha >= 0.8:
                        st.success("âœ… Î± â‰¥ 0.8ï¼Œå†…éƒ¨ä¸€è‡´æ€§è‰¯å¥½")
                    elif alpha >= 0.7:
                        st.info("â„¹ï¸ Î± â‰¥ 0.7ï¼Œå†…éƒ¨ä¸€è‡´æ€§å¯æ¥å—")
                    elif alpha >= 0.6:
                        st.warning("âš ï¸ Î± â‰¥ 0.6ï¼Œå†…éƒ¨ä¸€è‡´æ€§è¾ƒä½")
                    else:
                        st.error("âŒ Î± < 0.6ï¼Œå†…éƒ¨ä¸€è‡´æ€§å·®")
            
            # 6. ç»¼åˆè´¨é‡è¯„åˆ†
            st.write("##### ğŸ† ç»¼åˆè´¨é‡è¯„åˆ†")
            
            # è®¡ç®—å„ç»´åº¦å¾—åˆ†
            missing_score = max(0, 100 - (missing_pattern.mean() / total_responses * 100 * 2))  # ç¼ºå¤±å€¼è¶Šå°‘åˆ†æ•°è¶Šé«˜
            consistency_score = min(100, alpha * 100) if 'alpha' in locals() else 50  # å†…éƒ¨ä¸€è‡´æ€§åˆ†æ•°
            
            if check_outliers and outlier_df['å¼‚å¸¸å€¼æ¯”ä¾‹(%)'].mean() < 5:
                outlier_score = 90
            elif check_outliers and outlier_df['å¼‚å¸¸å€¼æ¯”ä¾‹(%)'].mean() < 10:
                outlier_score = 70
            else:
                outlier_score = 50
            
            overall_score = (missing_score + consistency_score + outlier_score) / 3
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ•°æ®å®Œæ•´æ€§", f"{missing_score:.1f}/100")
            with col2:
                st.metric("å†…éƒ¨ä¸€è‡´æ€§", f"{consistency_score:.1f}/100")
            with col3:
                st.metric("æ•°æ®è´¨é‡", f"{outlier_score:.1f}/100")
            with col4:
                st.metric("ç»¼åˆè¯„åˆ†", f"{overall_score:.1f}/100")
            
            # è´¨é‡ç­‰çº§
            if overall_score >= 90:
                st.success("ğŸŒŸ é—®å·è´¨é‡: ä¼˜ç§€")
            elif overall_score >= 80:
                st.success("âœ… é—®å·è´¨é‡: è‰¯å¥½")
            elif overall_score >= 70:
                st.info("â„¹ï¸ é—®å·è´¨é‡: ä¸€èˆ¬")
            elif overall_score >= 60:
                st.warning("âš ï¸ é—®å·è´¨é‡: è¾ƒå·®")
            else:
                st.error("âŒ é—®å·è´¨é‡: å·®")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'é—®å·è´¨é‡è¯„ä¼°',
                'variables': selected_cols,
                'total_responses': total_responses,
                'basic_stats': basic_df.to_dict('records'),
                'missing_rate': missing_pattern.mean() / total_responses * 100,
                'cronbach_alpha': alpha if 'alpha' in locals() else None,
                'overall_score': overall_score
            }
            
            st.success("é—®å·è´¨é‡è¯„ä¼°å®Œæˆï¼")
            
        except Exception as e:
            st.error(f"é—®å·è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")


def execute_logistic_regression(data):
    """é€»è¾‘å›å½’åˆ†æ"""
    st.write("#### ğŸ“ˆ é€»è¾‘å›å½’åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if not categorical_cols:
        st.error("é€»è¾‘å›å½’éœ€è¦è‡³å°‘ä¸€ä¸ªåˆ†ç±»å‹å› å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        # åªæ˜¾ç¤ºäºŒåˆ†ç±»å˜é‡ä½œä¸ºå› å˜é‡
        binary_vars = []
        for col in categorical_cols:
            if data[col].nunique() == 2:
                binary_vars.append(col)
        
        if not binary_vars:
            st.error("é€»è¾‘å›å½’éœ€è¦ä¸€ä¸ªäºŒåˆ†ç±»å› å˜é‡")
            return
            
        y_var = st.selectbox("é€‰æ‹©å› å˜é‡(äºŒåˆ†ç±»)", binary_vars)
    
    with col2:
        x_vars = st.multiselect("é€‰æ‹©è‡ªå˜é‡", 
                               [col for col in numeric_cols if col != y_var],
                               help="é€‰æ‹©æ•°å€¼å‹è‡ªå˜é‡")
    
    if not x_vars:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè‡ªå˜é‡")
        return
    
    # é€»è¾‘å›å½’å‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.3)
        solver = st.selectbox("æ±‚è§£å™¨", ["liblinear", "lbfgs", "newton-cg", "sag", "saga"])
    
    with col2:
        max_iter = st.number_input("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 10000, 1000)
        random_state = st.number_input("éšæœºç§å­", 0, 1000, 42)
    
    if st.button("æ‰§è¡Œé€»è¾‘å›å½’"):
        try:
            from sklearn.linear_model import LogisticRegression
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
            from sklearn.preprocessing import LabelEncoder
            import matplotlib.pyplot as plt
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            clean_data = data[[y_var] + x_vars].dropna()
            
            if len(clean_data) < 20:
                st.error("æ ·æœ¬é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œé€»è¾‘å›å½’")
                return
            
            # ç¼–ç å› å˜é‡
            le = LabelEncoder()
            y = le.fit_transform(clean_data[y_var])
            X = clean_data[x_vars]
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=y
            )
            
            # æ‹Ÿåˆæ¨¡å‹
            model = LogisticRegression(solver=solver, max_iter=max_iter, random_state=random_state)
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # æ˜¾ç¤ºç»“æœ
            st.write("##### æ¨¡å‹æ€§èƒ½")
            
            # åŸºæœ¬æŒ‡æ ‡
            train_score = model.score(X_train, y_train)
            test_score = model.score(X_test, y_test)
            auc_score = roc_auc_score(y_test, y_pred_proba)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("è®­ç»ƒé›†å‡†ç¡®ç‡", f"{train_score:.4f}")
            with col2:
                st.metric("æµ‹è¯•é›†å‡†ç¡®ç‡", f"{test_score:.4f}")
            with col3:
                st.metric("AUCå€¼", f"{auc_score:.4f}")
            
            # å›å½’ç³»æ•°
            st.write("##### å›å½’ç³»æ•°")
            coef_df = pd.DataFrame({
                'å˜é‡': ['å¸¸æ•°é¡¹'] + x_vars,
                'ç³»æ•°': [model.intercept_[0]] + model.coef_[0].tolist(),
                'ä¼˜åŠ¿æ¯”(OR)': [np.exp(model.intercept_[0])] + [np.exp(coef) for coef in model.coef_[0]]
            })
            st.dataframe(coef_df.round(4))
            
            # æ··æ·†çŸ©é˜µ
            st.write("##### æ··æ·†çŸ©é˜µ")
            cm = confusion_matrix(y_test, y_pred)
            cm_df = pd.DataFrame(cm, 
                               columns=[f'é¢„æµ‹_{label}' for label in le.classes_],
                               index=[f'å®é™…_{label}' for label in le.classes_])
            st.dataframe(cm_df)
            
            # åˆ†ç±»æŠ¥å‘Š
            st.write("##### åˆ†ç±»æŠ¥å‘Š")
            report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)
            report_df = pd.DataFrame(report).transpose().round(4)
            st.dataframe(report_df)
            
            # ROCæ›²çº¿
            st.write("##### ROCæ›²çº¿")
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            
            fig, ax = plt.subplots(figsize=(8, 6))
            ax.plot(fpr, tpr, label=f'ROCæ›²çº¿ (AUC = {auc_score:.3f})')
            ax.plot([0, 1], [0, 1], 'k--', label='éšæœºçŒœæµ‹')
            ax.set_xlabel('å‡æ­£ç‡ (FPR)')
            ax.set_ylabel('çœŸæ­£ç‡ (TPR)')
            ax.set_title('ROCæ›²çº¿')
            ax.legend()
            ax.grid(True, alpha=0.3)
            st.pyplot(fig)
            plt.close(fig)
            
            # æ¨¡å‹è§£é‡Š
            st.write("##### æ¨¡å‹è§£é‡Š")
            st.write("**ç³»æ•°è§£é‡Š:**")
            for var, coef, odds_ratio in zip(x_vars, model.coef_[0], [np.exp(c) for c in model.coef_[0]]):
                if coef > 0:
                    effect = "å¢åŠ "
                    direction = "æ­£å‘"
                else:
                    effect = "å‡å°‘"
                    direction = "è´Ÿå‘"
                
                st.write(f"- **{var}**: {direction}å½±å“ï¼Œç³»æ•°={coef:.4f}ï¼Œä¼˜åŠ¿æ¯”={odds_ratio:.4f}")
                st.write(f"  {var}æ¯å¢åŠ 1ä¸ªå•ä½ï¼Œ{y_var}çš„å¯¹æ•°å‡ æ¯”{effect}{abs(coef):.4f}")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'é€»è¾‘å›å½’',
                'dependent_var': y_var,
                'independent_vars': x_vars,
                'train_accuracy': train_score,
                'test_accuracy': test_score,
                'auc_score': auc_score,
                'coefficients': coef_df.to_dict('records'),
                'confusion_matrix': cm.tolist(),
                'classification_report': report
            }
            
            st.success("é€»è¾‘å›å½’åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"é€»è¾‘å›å½’åˆ†æå¤±è´¥: {str(e)}")


def execute_machine_learning(analysis_option, processor, data):
    """æ‰§è¡Œæœºå™¨å­¦ä¹ åˆ†æ"""
    st.write(f"#### ğŸ¤– {analysis_option}")
    
    if analysis_option == "åˆ†ç±»ç®—æ³•":
        execute_classification_algorithms(data)
    elif analysis_option == "å›å½’ç®—æ³•":
        execute_regression_algorithms(data)
    elif analysis_option == "èšç±»ç®—æ³•":
        execute_clustering_algorithms(data)
    elif analysis_option == "é™ç»´ç®—æ³•":
        execute_dimensionality_reduction(data)
    elif analysis_option == "æ¨¡å‹è¯„ä¼°":
        execute_model_evaluation(data)
    else:
        st.info(f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_classification_algorithms(data):
    """åˆ†ç±»ç®—æ³•"""
    st.write("##### åˆ†ç±»ç®—æ³•æ¯”è¾ƒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if not categorical_cols or not numeric_cols:
        st.error("åˆ†ç±»ç®—æ³•éœ€è¦æ•°å€¼å‹ç‰¹å¾å’Œåˆ†ç±»å‹ç›®æ ‡å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        target_var = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡(åˆ†ç±»)", categorical_cols)
    with col2:
        feature_vars = st.multiselect("é€‰æ‹©ç‰¹å¾å˜é‡", 
                                    [col for col in numeric_cols],
                                    default=numeric_cols[:min(5, len(numeric_cols))])
    
    if not feature_vars:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç‰¹å¾å˜é‡")
        return
    
    # ç®—æ³•é€‰æ‹©
    algorithms = st.multiselect(
        "é€‰æ‹©åˆ†ç±»ç®—æ³•",
        ["éšæœºæ£®æ—", "æ”¯æŒå‘é‡æœº", "æœ´ç´ è´å¶æ–¯", "Kè¿‘é‚»", "å†³ç­–æ ‘", "æ¢¯åº¦æå‡"],
        default=["éšæœºæ£®æ—", "æ”¯æŒå‘é‡æœº", "æœ´ç´ è´å¶æ–¯"]
    )
    
    if not algorithms:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç®—æ³•")
        return
    
    # å‚æ•°è®¾ç½®
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.3)
    cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)
    
    if st.button("æ‰§è¡Œåˆ†ç±»ç®—æ³•æ¯”è¾ƒ"):
        try:
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.svm import SVC
            from sklearn.naive_bayes import GaussianNB
            from sklearn.neighbors import KNeighborsClassifier
            from sklearn.tree import DecisionTreeClassifier
            from sklearn.model_selection import train_test_split, cross_val_score
            from sklearn.preprocessing import LabelEncoder, StandardScaler
            from sklearn.metrics import classification_report, accuracy_score
            
            # å‡†å¤‡æ•°æ®
            clean_data = data[feature_vars + [target_var]].dropna()
            
            if len(clean_data) < 20:
                st.error("æ ·æœ¬é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœºå™¨å­¦ä¹ ")
                return
            
            # ç¼–ç ç›®æ ‡å˜é‡
            le = LabelEncoder()
            y = le.fit_transform(clean_data[target_var])
            X = clean_data[feature_vars]
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # åˆ’åˆ†æ•°æ®é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=test_size, random_state=42, stratify=y
            )
            
            # å®šä¹‰ç®—æ³•
            models = {}
            if "éšæœºæ£®æ—" in algorithms:
                models["éšæœºæ£®æ—"] = RandomForestClassifier(n_estimators=100, random_state=42)
            if "æ”¯æŒå‘é‡æœº" in algorithms:
                models["æ”¯æŒå‘é‡æœº"] = SVC(random_state=42)
            if "æœ´ç´ è´å¶æ–¯" in algorithms:
                models["æœ´ç´ è´å¶æ–¯"] = GaussianNB()
            if "Kè¿‘é‚»" in algorithms:
                models["Kè¿‘é‚»"] = KNeighborsClassifier(n_neighbors=5)
            if "å†³ç­–æ ‘" in algorithms:
                models["å†³ç­–æ ‘"] = DecisionTreeClassifier(random_state=42)
            if "æ¢¯åº¦æå‡" in algorithms:
                models["æ¢¯åº¦æå‡"] = GradientBoostingClassifier(random_state=42)
            
            # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
            results = []
            
            st.write("##### æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
            
            for name, model in models.items():
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(X_test)
                
                # è®¡ç®—æŒ‡æ ‡
                train_score = model.score(X_train, y_train)
                test_score = accuracy_score(y_test, y_pred)
                cv_scores = cross_val_score(model, X_scaled, y, cv=cv_folds)
                
                results.append({
                    'ç®—æ³•': name,
                    'è®­ç»ƒå‡†ç¡®ç‡': f"{train_score:.4f}",
                    'æµ‹è¯•å‡†ç¡®ç‡': f"{test_score:.4f}",
                    'äº¤å‰éªŒè¯å‡å€¼': f"{cv_scores.mean():.4f}",
                    'äº¤å‰éªŒè¯æ ‡å‡†å·®': f"{cv_scores.std():.4f}"
                })
            
            results_df = pd.DataFrame(results)
            st.dataframe(results_df)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model_name = results_df.loc[results_df['æµ‹è¯•å‡†ç¡®ç‡'].astype(float).idxmax(), 'ç®—æ³•']
            st.success(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
            
            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
            best_model = models[best_model_name]
            if hasattr(best_model, 'feature_importances_'):
                st.write("##### ç‰¹å¾é‡è¦æ€§")
                importance_df = pd.DataFrame({
                    'ç‰¹å¾': feature_vars,
                    'é‡è¦æ€§': best_model.feature_importances_
                }).sort_values('é‡è¦æ€§', ascending=False)
                
                st.dataframe(importance_df.round(4))
                
                # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
                import matplotlib.pyplot as plt
                fig, ax = plt.subplots(figsize=(10, 6))
                bars = ax.barh(importance_df['ç‰¹å¾'], importance_df['é‡è¦æ€§'])
                ax.set_xlabel('é‡è¦æ€§')
                ax.set_title(f'{best_model_name} - ç‰¹å¾é‡è¦æ€§')
                plt.tight_layout()
                st.pyplot(fig)
                plt.close(fig)
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'åˆ†ç±»ç®—æ³•æ¯”è¾ƒ',
                'target_variable': target_var,
                'feature_variables': feature_vars,
                'algorithms': algorithms,
                'results': results_df.to_dict('records'),
                'best_model': best_model_name
            }
            
            st.success("åˆ†ç±»ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"åˆ†ç±»ç®—æ³•æ¯”è¾ƒå¤±è´¥: {str(e)}")


def execute_regression_algorithms(data):
    """å›å½’ç®—æ³•"""
    st.write("##### å›å½’ç®—æ³•æ¯”è¾ƒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    if len(numeric_cols) < 2:
        st.error("å›å½’ç®—æ³•éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        target_var = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡", numeric_cols)
    with col2:
        feature_vars = st.multiselect("é€‰æ‹©ç‰¹å¾å˜é‡", 
                                    [col for col in numeric_cols if col != target_var],
                                    default=[col for col in numeric_cols if col != target_var][:min(5, len(numeric_cols)-1)])
    
    if not feature_vars:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç‰¹å¾å˜é‡")
        return
    
    # ç®—æ³•é€‰æ‹©
    algorithms = st.multiselect(
        "é€‰æ‹©å›å½’ç®—æ³•",
        ["çº¿æ€§å›å½’", "éšæœºæ£®æ—å›å½’", "æ”¯æŒå‘é‡å›å½’", "å†³ç­–æ ‘å›å½’", "æ¢¯åº¦æå‡å›å½’", "å²­å›å½’"],
        default=["çº¿æ€§å›å½’", "éšæœºæ£®æ—å›å½’", "æ¢¯åº¦æå‡å›å½’"]
    )
    
    if not algorithms:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç®—æ³•")
        return
    
    # å‚æ•°è®¾ç½®
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.3)
    cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)
    
    if st.button("æ‰§è¡Œå›å½’ç®—æ³•æ¯”è¾ƒ"):
        try:
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            from sklearn.svm import SVR
            from sklearn.tree import DecisionTreeRegressor
            from sklearn.linear_model import LinearRegression, Ridge
            from sklearn.model_selection import train_test_split, cross_val_score
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            clean_data = data[feature_vars + [target_var]].dropna()
            
            if len(clean_data) < 20:
                st.error("æ ·æœ¬é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœºå™¨å­¦ä¹ ")
                return
            
            X = clean_data[feature_vars]
            y = clean_data[target_var]
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # åˆ’åˆ†æ•°æ®é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=test_size, random_state=42
            )
            
            # å®šä¹‰ç®—æ³•
            models = {}
            if "çº¿æ€§å›å½’" in algorithms:
                models["çº¿æ€§å›å½’"] = LinearRegression()
            if "éšæœºæ£®æ—å›å½’" in algorithms:
                models["éšæœºæ£®æ—å›å½’"] = RandomForestRegressor(n_estimators=100, random_state=42)
            if "æ”¯æŒå‘é‡å›å½’" in algorithms:
                models["æ”¯æŒå‘é‡å›å½’"] = SVR()
            if "å†³ç­–æ ‘å›å½’" in algorithms:
                models["å†³ç­–æ ‘å›å½’"] = DecisionTreeRegressor(random_state=42)
            if "æ¢¯åº¦æå‡å›å½’" in algorithms:
                models["æ¢¯åº¦æå‡å›å½’"] = GradientBoostingRegressor(random_state=42)
            if "å²­å›å½’" in algorithms:
                models["å²­å›å½’"] = Ridge(random_state=42)
            
            # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
            results = []
            
            st.write("##### æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
            
            for name, model in models.items():
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(X_test)
                
                # è®¡ç®—æŒ‡æ ‡
                r2 = r2_score(y_test, y_pred)
                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                mae = mean_absolute_error(y_test, y_pred)
                cv_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='r2')
                
                results.append({
                    'ç®—æ³•': name,
                    'RÂ²': f"{r2:.4f}",
                    'RMSE': f"{rmse:.4f}",
                    'MAE': f"{mae:.4f}",
                    'äº¤å‰éªŒè¯RÂ²å‡å€¼': f"{cv_scores.mean():.4f}",
                    'äº¤å‰éªŒè¯RÂ²æ ‡å‡†å·®': f"{cv_scores.std():.4f}"
                })
            
            results_df = pd.DataFrame(results)
            st.dataframe(results_df)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºäºRÂ²ï¼‰
            best_model_name = results_df.loc[results_df['RÂ²'].astype(float).idxmax(), 'ç®—æ³•']
            st.success(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'å›å½’ç®—æ³•æ¯”è¾ƒ',
                'target_variable': target_var,
                'feature_variables': feature_vars,
                'algorithms': algorithms,
                'results': results_df.to_dict('records'),
                'best_model': best_model_name
            }
            
            st.success("å›å½’ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"å›å½’ç®—æ³•æ¯”è¾ƒå¤±è´¥: {str(e)}")


def execute_time_series(analysis_option, processor, data):
    """æ‰§è¡Œæ—¶é—´åºåˆ—åˆ†æ"""
    st.write(f"#### ğŸ“… {analysis_option}")
    
    if analysis_option == "è¶‹åŠ¿åˆ†æ":
        execute_trend_analysis(data)
    elif analysis_option == "å­£èŠ‚æ€§åˆ†æ":
        execute_seasonal_analysis(data)
    elif analysis_option == "é¢„æµ‹åˆ†æ":
        execute_forecasting_analysis(data)
    elif analysis_option == "å¼‚å¸¸æ£€æµ‹":
        execute_anomaly_detection(data)
    else:
        st.info(f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_trend_analysis(data):
    """è¶‹åŠ¿åˆ†æ"""
    st.write("##### è¶‹åŠ¿åˆ†æ")
    
    # æ£€æŸ¥æ—¶é—´åˆ—
    datetime_cols = []
    for col in data.columns:
        if data[col].dtype == 'object':
            try:
                pd.to_datetime(data[col].head())
                datetime_cols.append(col)
            except:
                pass
        elif 'datetime' in str(data[col].dtype):
            datetime_cols.append(col)
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    if not datetime_cols:
        st.error("æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œè¯·ç¡®ä¿æ•°æ®åŒ…å«æ—¶é—´ä¿¡æ¯")
        return
    
    if not numeric_cols:
        st.error("æœªæ‰¾åˆ°æ•°å€¼åˆ—ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return
    
    # é€‰æ‹©å˜é‡
    col1, col2 = st.columns(2)
    with col1:
        time_col = st.selectbox("é€‰æ‹©æ—¶é—´åˆ—", datetime_cols)
    with col2:
        value_col = st.selectbox("é€‰æ‹©æ•°å€¼åˆ—", numeric_cols)
    
    # è¶‹åŠ¿åˆ†æå‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        method = st.selectbox("è¶‹åŠ¿æ£€æµ‹æ–¹æ³•", ["ç§»åŠ¨å¹³å‡", "çº¿æ€§å›å½’", "å¤šé¡¹å¼æ‹Ÿåˆ"])
        if method == "ç§»åŠ¨å¹³å‡":
            window = st.slider("ç§»åŠ¨çª—å£å¤§å°", 3, 30, 7)
        elif method == "å¤šé¡¹å¼æ‹Ÿåˆ":
            degree = st.slider("å¤šé¡¹å¼é˜¶æ•°", 1, 5, 2)
    
    with col2:
        show_decomposition = st.checkbox("æ˜¾ç¤ºåˆ†è§£å›¾", value=True)
        detect_changepoints = st.checkbox("æ£€æµ‹å˜ç‚¹", value=False)
    
    if st.button("æ‰§è¡Œè¶‹åŠ¿åˆ†æ"):
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            from scipy import stats
            
            # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
            ts_data = data[[time_col, value_col]].dropna().copy()
            ts_data[time_col] = pd.to_datetime(ts_data[time_col])
            ts_data = ts_data.sort_values(time_col).reset_index(drop=True)
            
            if len(ts_data) < 10:
                st.error("æ—¶é—´åºåˆ—æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                return
            
            # åˆ›å»ºæ—¶é—´ç´¢å¼•ç”¨äºè®¡ç®—
            ts_data['time_index'] = range(len(ts_data))
            
            st.write("##### æ—¶é—´åºåˆ—å¯è§†åŒ–")
            
            # åŸå§‹æ•°æ®å›¾
            fig, axes = plt.subplots(2, 1, figsize=(12, 10))
            
            # åŸå§‹æ—¶é—´åºåˆ—
            axes[0].plot(ts_data[time_col], ts_data[value_col], 'b-', alpha=0.7, label='åŸå§‹æ•°æ®')
            
            # è¶‹åŠ¿åˆ†æ
            if method == "ç§»åŠ¨å¹³å‡":
                trend = ts_data[value_col].rolling(window=window, center=True).mean()
                axes[0].plot(ts_data[time_col], trend, 'r-', linewidth=2, label=f'{window}æœŸç§»åŠ¨å¹³å‡')
                
                # è®¡ç®—è¶‹åŠ¿æ–¹å‘
                trend_slope = (trend.iloc[-1] - trend.iloc[0]) / len(trend)
                
            elif method == "çº¿æ€§å›å½’":
                slope, intercept, r_value, p_value, std_err = stats.linregress(ts_data['time_index'], ts_data[value_col])
                trend = slope * ts_data['time_index'] + intercept
                axes[0].plot(ts_data[time_col], trend, 'r-', linewidth=2, label='çº¿æ€§è¶‹åŠ¿')
                
                trend_slope = slope
                
                # æ˜¾ç¤ºå›å½’ç»Ÿè®¡
                st.write("**çº¿æ€§å›å½’ç»Ÿè®¡:**")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ–œç‡", f"{slope:.4f}")
                with col2:
                    st.metric("RÂ²", f"{r_value**2:.4f}")
                with col3:
                    st.metric("på€¼", f"{p_value:.4f}")
                
            elif method == "å¤šé¡¹å¼æ‹Ÿåˆ":
                coeffs = np.polyfit(ts_data['time_index'], ts_data[value_col], degree)
                trend = np.polyval(coeffs, ts_data['time_index'])
                axes[0].plot(ts_data[time_col], trend, 'r-', linewidth=2, label=f'{degree}é˜¶å¤šé¡¹å¼æ‹Ÿåˆ')
                
                # è®¡ç®—æ€»ä½“è¶‹åŠ¿æ–¹å‘
                trend_slope = (trend[-1] - trend[0]) / len(trend)
            
            axes[0].set_xlabel('æ—¶é—´')
            axes[0].set_ylabel(value_col)
            axes[0].set_title(f'{value_col} æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # æ®‹å·®åˆ†æ
            if 'trend' in locals():
                residuals = ts_data[value_col] - trend
                axes[1].plot(ts_data[time_col], residuals, 'g-', alpha=0.7, label='æ®‹å·®')
                axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                axes[1].set_xlabel('æ—¶é—´')
                axes[1].set_ylabel('æ®‹å·®')
                axes[1].set_title('æ®‹å·®åˆ†æ')
                axes[1].legend()
                axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)
            
            # è¶‹åŠ¿æ€»ç»“
            st.write("##### è¶‹åŠ¿åˆ†æç»“æœ")
            
            if trend_slope > 0:
                trend_direction = "ä¸Šå‡"
                trend_icon = "ğŸ“ˆ"
            elif trend_slope < 0:
                trend_direction = "ä¸‹é™"
                trend_icon = "ğŸ“‰"
            else:
                trend_direction = "å¹³ç¨³"
                trend_icon = "â¡ï¸"
            
            st.write(f"{trend_icon} **æ€»ä½“è¶‹åŠ¿**: {trend_direction}")
            st.write(f"**è¶‹åŠ¿å¼ºåº¦**: {abs(trend_slope):.4f} å•ä½/æœŸ")
            
            # åŸºæœ¬ç»Ÿè®¡
            st.write("##### åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
            stats_df = pd.DataFrame({
                'æŒ‡æ ‡': ['è§‚æµ‹æ•°', 'å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼', 'å˜å¼‚ç³»æ•°'],
                'å€¼': [
                    len(ts_data),
                    f"{ts_data[value_col].mean():.4f}",
                    f"{ts_data[value_col].std():.4f}",
                    f"{ts_data[value_col].min():.4f}",
                    f"{ts_data[value_col].max():.4f}",
                    f"{ts_data[value_col].std() / ts_data[value_col].mean():.4f}"
                ]
            })
            st.dataframe(stats_df)
            
            # å˜ç‚¹æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if detect_changepoints:
                st.write("##### å˜ç‚¹æ£€æµ‹")
                try:
                    # ä½¿ç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å˜ç‚¹
                    window_size = min(10, len(ts_data) // 4)
                    if window_size >= 3:
                        changes = []
                        for i in range(window_size, len(ts_data) - window_size):
                            before = ts_data[value_col].iloc[i-window_size:i].mean()
                            after = ts_data[value_col].iloc[i:i+window_size].mean()
                            change_magnitude = abs(after - before)
                            if change_magnitude > ts_data[value_col].std():
                                changes.append({
                                    'æ—¶é—´ç‚¹': ts_data[time_col].iloc[i],
                                    'å˜åŒ–å¹…åº¦': change_magnitude,
                                    'å˜åŒ–ç±»å‹': 'ä¸Šå‡' if after > before else 'ä¸‹é™'
                                })
                        
                        if changes:
                            changes_df = pd.DataFrame(changes)
                            st.dataframe(changes_df)
                            st.info(f"æ£€æµ‹åˆ° {len(changes)} ä¸ªæ½œåœ¨å˜ç‚¹")
                        else:
                            st.info("æœªæ£€æµ‹åˆ°æ˜æ˜¾å˜ç‚¹")
                    else:
                        st.info("æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå˜ç‚¹æ£€æµ‹")
                except Exception as e:
                    st.warning(f"å˜ç‚¹æ£€æµ‹å¤±è´¥: {str(e)}")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'è¶‹åŠ¿åˆ†æ',
                'time_column': time_col,
                'value_column': value_col,
                'method': method,
                'trend_direction': trend_direction,
                'trend_slope': trend_slope,
                'data_points': len(ts_data)
            }
            
            st.success("è¶‹åŠ¿åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")


def execute_cluster_analysis(data):
    """èšç±»åˆ†æ"""
    st.write("#### ğŸ¯ èšç±»åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 2:
        st.error("èšç±»åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©ç”¨äºèšç±»çš„å˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©ç”¨äºèšç±»çš„å˜é‡", 
        numeric_cols,
        default=numeric_cols[:min(5, len(numeric_cols))],
        help="é€‰æ‹©ç”¨äºèšç±»çš„æ•°å€¼å‹å˜é‡"
    )
    
    if len(selected_cols) < 2:
        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªå˜é‡è¿›è¡Œèšç±»åˆ†æ")
        return
    
    # èšç±»å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        n_clusters = st.slider("èšç±»æ•°é‡", 2, 10, 3)
        cluster_method = st.selectbox("èšç±»æ–¹æ³•", ["K-Means", "å±‚æ¬¡èšç±»"])
    
    with col2:
        standardize = st.checkbox("æ ‡å‡†åŒ–æ•°æ®", value=True, help="å»ºè®®å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–")
        show_details = st.checkbox("æ˜¾ç¤ºè¯¦ç»†ç»“æœ", value=True)
    
    if st.button("æ‰§è¡Œèšç±»åˆ†æ"):
        try:
            from sklearn.cluster import KMeans, AgglomerativeClustering
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import silhouette_score
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            cluster_data = data[selected_cols].dropna()
            
            if cluster_data.empty:
                st.error("é€‰æ‹©çš„å˜é‡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return
            
            # æ ‡å‡†åŒ–
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(cluster_data)
                cluster_features = pd.DataFrame(X_scaled, columns=selected_cols, index=cluster_data.index)
            else:
                cluster_features = cluster_data
            
            # æ‰§è¡Œèšç±»
            if cluster_method == "K-Means":
                clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            else:
                clusterer = AgglomerativeClustering(n_clusters=n_clusters)
            
            labels = clusterer.fit_predict(cluster_features)
            
            # è®¡ç®—èšç±»è¯„ä»·æŒ‡æ ‡
            silhouette_avg = silhouette_score(cluster_features, labels)
            
            # æ˜¾ç¤ºç»“æœ
            st.write("##### èšç±»ç»“æœ")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("èšç±»æ•°é‡", n_clusters)
            with col2:
                st.metric("è½®å»“ç³»æ•°", f"{silhouette_avg:.4f}")
            with col3:
                st.metric("æœ‰æ•ˆæ ·æœ¬æ•°", len(cluster_data))
            
            # èšç±»åˆ†å¸ƒç»Ÿè®¡
            cluster_counts = pd.Series(labels).value_counts().sort_index()
            st.write("##### å„èšç±»æ ·æœ¬åˆ†å¸ƒ")
            cluster_df = pd.DataFrame({
                'èšç±»': [f"èšç±»{i+1}" for i in range(n_clusters)],
                'æ ·æœ¬æ•°': [cluster_counts.get(i, 0) for i in range(n_clusters)],
                'å æ¯”(%)': [(cluster_counts.get(i, 0) / len(labels) * 100) for i in range(n_clusters)]
            })
            st.dataframe(cluster_df)
            
            if show_details:
                # å„èšç±»ä¸­å¿ƒ
                if cluster_method == "K-Means":
                    centers = clusterer.cluster_centers_
                    if standardize:
                        # åæ ‡å‡†åŒ–èšç±»ä¸­å¿ƒ
                        centers = scaler.inverse_transform(centers)
                    
                    centers_df = pd.DataFrame(centers, columns=selected_cols)
                    centers_df.index = [f"èšç±»{i+1}" for i in range(n_clusters)]
                    
                    st.write("##### èšç±»ä¸­å¿ƒ")
                    st.dataframe(centers_df.round(3))
                
                # å„èšç±»çš„æè¿°ç»Ÿè®¡
                cluster_data_with_labels = cluster_data.copy()
                cluster_data_with_labels['èšç±»'] = labels
                
                st.write("##### å„èšç±»æè¿°ç»Ÿè®¡")
                for i in range(n_clusters):
                    with st.expander(f"èšç±»{i+1} è¯¦ç»†ç»Ÿè®¡"):
                        cluster_i_data = cluster_data_with_labels[cluster_data_with_labels['èšç±»'] == i][selected_cols]
                        st.dataframe(cluster_i_data.describe().round(3))
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'èšç±»åˆ†æ',
                'method': cluster_method,
                'variables': selected_cols,
                'n_clusters': n_clusters,
                'silhouette_score': silhouette_avg,
                'cluster_labels': labels,
                'cluster_centers': centers_df if cluster_method == "K-Means" else None,
                'status': 'completed'
            }
            
            st.success("èšç±»åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"èšç±»åˆ†æå¤±è´¥: {str(e)}")


def execute_factor_analysis(data):
    """å› å­åˆ†æ"""
    st.write("#### ğŸ” å› å­åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 3:
        st.error("å› å­åˆ†æéœ€è¦è‡³å°‘3ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©ç”¨äºå› å­åˆ†æçš„å˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©ç”¨äºå› å­åˆ†æçš„å˜é‡", 
        numeric_cols,
        default=numeric_cols[:min(10, len(numeric_cols))],
        help="é€‰æ‹©ç”¨äºå› å­åˆ†æçš„æ•°å€¼å‹å˜é‡ï¼Œå»ºè®®é€‰æ‹©3-15ä¸ªå˜é‡"
    )
    
    if len(selected_cols) < 3:
        st.warning("è¯·è‡³å°‘é€‰æ‹©3ä¸ªå˜é‡è¿›è¡Œå› å­åˆ†æ")
        return
    
    # å› å­åˆ†æå‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        n_factors = st.slider("å› å­æ•°é‡", 1, min(len(selected_cols)-1, 8), min(3, len(selected_cols)-1))
        rotation = st.selectbox("æ—‹è½¬æ–¹æ³•", ["varimax", "quartimax", "none"])
    
    with col2:
        kmo_test = st.checkbox("KMOé€‚åˆæ€§æ£€éªŒ", value=True)
        show_communalities = st.checkbox("æ˜¾ç¤ºå…±åŒåº¦", value=True)
    
    if st.button("æ‰§è¡Œå› å­åˆ†æ"):
        try:
            from sklearn.decomposition import FactorAnalysis
            from sklearn.preprocessing import StandardScaler
            import numpy as np
            from scipy.stats import chi2
            
            # å‡†å¤‡æ•°æ®
            factor_data = data[selected_cols].dropna()
            
            if factor_data.empty:
                st.error("é€‰æ‹©çš„å˜é‡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return
            
            if len(factor_data) < len(selected_cols) * 2:
                st.warning("æ ·æœ¬é‡å¯èƒ½ä¸è¶³ï¼Œå»ºè®®æ ·æœ¬é‡è‡³å°‘æ˜¯å˜é‡æ•°çš„2å€")
            
            # æ ‡å‡†åŒ–æ•°æ®
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(factor_data)
            factor_features = pd.DataFrame(X_scaled, columns=selected_cols)
            
            # KMOæ£€éªŒ
            if kmo_test:
                try:
                    from factor_analyzer.factor_analyzer import calculate_kmo
                    kmo_all, kmo_model = calculate_kmo(factor_features)
                    
                    st.write("##### KMOé€‚åˆæ€§æ£€éªŒ")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("KMOç³»æ•°", f"{kmo_model:.4f}")
                    with col2:
                        if kmo_model >= 0.8:
                            kmo_level = "ä¼˜ç§€"
                            color = "ğŸŸ¢"
                        elif kmo_model >= 0.7:
                            kmo_level = "è‰¯å¥½"
                            color = "ğŸ”µ"
                        elif kmo_model >= 0.6:
                            kmo_level = "å¯æ¥å—"
                            color = "ğŸŸ¡"
                        else:
                            kmo_level = "ä¸é€‚åˆ"
                            color = "ğŸ”´"
                        st.write(f"**é€‚åˆæ€§:** {color} {kmo_level}")
                    
                    if kmo_model < 0.6:
                        st.warning("KMOç³»æ•°è¾ƒä½ï¼Œæ•°æ®å¯èƒ½ä¸é€‚åˆè¿›è¡Œå› å­åˆ†æ")
                        
                except ImportError:
                    st.info("æœªå®‰è£…factor_analyzeråŒ…ï¼Œè·³è¿‡KMOæ£€éªŒ")
            
            # æ‰§è¡Œå› å­åˆ†æ
            fa = FactorAnalysis(n_components=n_factors, random_state=42)
            fa.fit(factor_features)
            
            # å› å­è½½è·çŸ©é˜µ
            loadings = fa.components_.T
            loadings_df = pd.DataFrame(
                loadings, 
                columns=[f"å› å­{i+1}" for i in range(n_factors)],
                index=selected_cols
            )
            
            # åº”ç”¨æ—‹è½¬ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…æ”¯æŒvarimaxï¼‰
            if rotation == "varimax":
                try:
                    from scipy.stats import orthogonal_procrustes
                    # ç®€åŒ–çš„varimaxæ—‹è½¬å®ç°
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=n_factors)
                    pca.fit(factor_features)
                    loadings_df = pd.DataFrame(
                        pca.components_.T,
                        columns=[f"å› å­{i+1}" for i in range(n_factors)],
                        index=selected_cols
                    )
                except:
                    st.info("æ—‹è½¬è®¡ç®—å‡ºç°é—®é¢˜ï¼Œä½¿ç”¨æœªæ—‹è½¬ç»“æœ")
            
            # è®¡ç®—å…±åŒåº¦
            communalities = np.sum(loadings_df.values**2, axis=1)
            
            # è®¡ç®—æ–¹å·®è´¡çŒ®
            eigenvalues = np.sum(loadings_df.values**2, axis=0)
            variance_explained = eigenvalues / len(selected_cols) * 100
            cumulative_variance = np.cumsum(variance_explained)
            
            # æ˜¾ç¤ºç»“æœ
            st.write("##### å› å­åˆ†æç»“æœ")
            
            # æ–¹å·®è§£é‡Šè¡¨
            variance_df = pd.DataFrame({
                'å› å­': [f"å› å­{i+1}" for i in range(n_factors)],
                'ç‰¹å¾å€¼': eigenvalues,
                'æ–¹å·®è´¡çŒ®ç‡(%)': variance_explained,
                'ç´¯ç§¯è´¡çŒ®ç‡(%)': cumulative_variance
            })
            
            st.write("**æ–¹å·®è§£é‡Š:**")
            st.dataframe(variance_df.round(3))
            
            # å› å­è½½è·çŸ©é˜µ
            st.write("**å› å­è½½è·çŸ©é˜µ:**")
            # çªå‡ºæ˜¾ç¤ºé«˜è½½è·ï¼ˆç»å¯¹å€¼>0.5ï¼‰
            def highlight_loadings(val):
                if abs(val) >= 0.5:
                    return 'background-color: lightgreen'
                elif abs(val) >= 0.3:
                    return 'background-color: lightyellow'
                return ''
            
            styled_loadings = loadings_df.round(3).style.map(highlight_loadings)
            st.dataframe(styled_loadings)
            
            # å…±åŒåº¦
            if show_communalities:
                communalities_df = pd.DataFrame({
                    'å˜é‡': selected_cols,
                    'å…±åŒåº¦': communalities
                })
                communalities_df = communalities_df.sort_values('å…±åŒåº¦', ascending=False)
                
                st.write("**å…±åŒåº¦:**")
                st.dataframe(communalities_df.round(3))
                
                # å…±åŒåº¦è§£é‡Š
                low_communality = communalities_df[communalities_df['å…±åŒåº¦'] < 0.4]
                if not low_communality.empty:
                    st.warning(f"ä»¥ä¸‹å˜é‡çš„å…±åŒåº¦è¾ƒä½(<0.4)ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘ç§»é™¤: {', '.join(low_communality['å˜é‡'].tolist())}")
            
            # å› å­å‘½åå»ºè®®
            st.write("##### å› å­è§£é‡Šå»ºè®®")
            for i in range(n_factors):
                factor_name = f"å› å­{i+1}"
                high_loadings = loadings_df[abs(loadings_df[factor_name]) >= 0.5]
                if not high_loadings.empty:
                    st.write(f"**{factor_name}** (è´¡çŒ®ç‡: {variance_explained[i]:.1f}%)")
                    st.write("ä¸»è¦å˜é‡:", ", ".join(high_loadings.index.tolist()))
                else:
                    st.write(f"**{factor_name}**: æ— æ˜æ˜¾é«˜è½½è·å˜é‡")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'å› å­åˆ†æ',
                'variables': selected_cols,
                'n_factors': n_factors,
                'loadings_matrix': loadings_df,
                'communalities': communalities_df if show_communalities else None,
                'variance_explained': variance_df,
                'rotation': rotation,
                'kmo_score': kmo_model if kmo_test else None,
                'status': 'completed'
            }
            
            st.success("å› å­åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"å› å­åˆ†æå¤±è´¥: {str(e)}")
            st.write("è¯·ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„ç»Ÿè®¡åŒ…ï¼Œæˆ–å°è¯•å‡å°‘å› å­æ•°é‡")


def execute_pca_analysis(data):
    """ä¸»æˆåˆ†åˆ†æ"""
    st.write("#### ğŸ¯ ä¸»æˆåˆ†åˆ†æ (PCA)")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) < 2:
        st.error("ä¸»æˆåˆ†åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©ç”¨äºPCAçš„å˜é‡
    selected_cols = st.multiselect(
        "é€‰æ‹©ç”¨äºä¸»æˆåˆ†åˆ†æçš„å˜é‡", 
        numeric_cols,
        default=numeric_cols[:min(10, len(numeric_cols))],
        help="é€‰æ‹©ç”¨äºä¸»æˆåˆ†åˆ†æçš„æ•°å€¼å‹å˜é‡"
    )
    
    if len(selected_cols) < 2:
        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªå˜é‡è¿›è¡Œä¸»æˆåˆ†åˆ†æ")
        return
    
    # PCAå‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        n_components = st.slider("ä¸»æˆåˆ†æ•°é‡", 1, len(selected_cols), min(3, len(selected_cols)))
        standardize = st.checkbox("æ ‡å‡†åŒ–æ•°æ®", value=True, help="å»ºè®®å¯¹ä¸åŒé‡çº²çš„æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–")
    
    with col2:
        show_biplot = st.checkbox("æ˜¾ç¤ºåŒæ ‡å›¾", value=True)
        show_loadings = st.checkbox("æ˜¾ç¤ºè½½è·çŸ©é˜µ", value=True)
    
    if st.button("æ‰§è¡Œä¸»æˆåˆ†åˆ†æ"):
        try:
            from sklearn.decomposition import PCA
            from sklearn.preprocessing import StandardScaler
            import matplotlib.pyplot as plt
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            pca_data = data[selected_cols].dropna()
            
            if pca_data.empty:
                st.error("é€‰æ‹©çš„å˜é‡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return
            
            # æ ‡å‡†åŒ–æ•°æ®ï¼ˆå¦‚æœé€‰æ‹©ï¼‰
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(pca_data)
                X_for_pca = pd.DataFrame(X_scaled, columns=selected_cols, index=pca_data.index)
            else:
                X_for_pca = pca_data
            
            # æ‰§è¡ŒPCA
            pca = PCA(n_components=n_components)
            pca_result = pca.fit_transform(X_for_pca)
            
            # åˆ›å»ºä¸»æˆåˆ†DataFrame
            pc_columns = [f'PC{i+1}' for i in range(n_components)]
            pca_df = pd.DataFrame(pca_result, columns=pc_columns, index=pca_data.index)
            
            # æ˜¾ç¤ºç»“æœ
            st.write("##### ä¸»æˆåˆ†åˆ†æç»“æœ")
            
            # æ–¹å·®è§£é‡Šè¡¨
            variance_ratio = pca.explained_variance_ratio_ * 100
            cumulative_variance = np.cumsum(variance_ratio)
            
            variance_df = pd.DataFrame({
                'ä¸»æˆåˆ†': pc_columns,
                'ç‰¹å¾å€¼': pca.explained_variance_,
                'æ–¹å·®è´¡çŒ®ç‡(%)': variance_ratio,
                'ç´¯ç§¯è´¡çŒ®ç‡(%)': cumulative_variance
            })
            
            st.write("**æ–¹å·®è§£é‡Š:**")
            st.dataframe(variance_df.round(3))
            
            # å¯è§†åŒ–æ–¹å·®è´¡çŒ®
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ç¢çŸ³å›¾
            ax1.plot(range(1, n_components + 1), pca.explained_variance_, 'bo-')
            ax1.set_xlabel('ä¸»æˆåˆ†')
            ax1.set_ylabel('ç‰¹å¾å€¼')
            ax1.set_title('ç¢çŸ³å›¾')
            ax1.grid(True, alpha=0.3)
            
            # æ–¹å·®è´¡çŒ®ç‡å›¾
            ax2.bar(range(1, n_components + 1), variance_ratio, alpha=0.7, label='å•ç‹¬è´¡çŒ®')
            ax2.plot(range(1, n_components + 1), cumulative_variance, 'ro-', label='ç´¯ç§¯è´¡çŒ®')
            ax2.set_xlabel('ä¸»æˆåˆ†')
            ax2.set_ylabel('æ–¹å·®è´¡çŒ®ç‡ (%)')
            ax2.set_title('æ–¹å·®è´¡çŒ®ç‡')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)
            
            # è½½è·çŸ©é˜µ
            if show_loadings:
                st.write("**ä¸»æˆåˆ†è½½è·çŸ©é˜µ:**")
                loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
                loadings_df = pd.DataFrame(
                    loadings,
                    columns=pc_columns,
                    index=selected_cols
                )
                
                # çªå‡ºæ˜¾ç¤ºé«˜è½½è·
                def highlight_loadings(val):
                    if abs(val) >= 0.7:
                        return 'background-color: lightgreen'
                    elif abs(val) >= 0.5:
                        return 'background-color: lightyellow'
                    return ''
                
                styled_loadings = loadings_df.round(3).style.map(highlight_loadings)
                st.dataframe(styled_loadings)
                
                # è½½è·è§£é‡Š
                st.write("**ä¸»æˆåˆ†è§£é‡Š:**")
                for i, pc in enumerate(pc_columns):
                    high_positive = loadings_df[loadings_df[pc] >= 0.5].index.tolist()
                    high_negative = loadings_df[loadings_df[pc] <= -0.5].index.tolist()
                    
                    st.write(f"**{pc}** (è´¡çŒ®ç‡: {variance_ratio[i]:.1f}%)")
                    if high_positive:
                        st.write(f"  æ­£å‘é«˜è½½è·: {', '.join(high_positive)}")
                    if high_negative:
                        st.write(f"  è´Ÿå‘é«˜è½½è·: {', '.join(high_negative)}")
                    if not high_positive and not high_negative:
                        st.write("  æ— æ˜æ˜¾é«˜è½½è·å˜é‡")
            
            # åŒæ ‡å›¾
            if show_biplot and n_components >= 2:
                st.write("##### åŒæ ‡å›¾ (å‰ä¸¤ä¸ªä¸»æˆåˆ†)")
                
                fig, ax = plt.subplots(figsize=(10, 8))
                
                # ç»˜åˆ¶è§‚æµ‹ç‚¹
                scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6, s=50)
                
                # ç»˜åˆ¶å˜é‡å‘é‡
                if show_loadings:
                    for i, var in enumerate(selected_cols):
                        ax.arrow(0, 0, loadings_df.iloc[i, 0]*3, loadings_df.iloc[i, 1]*3,
                                head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.8)
                        ax.text(loadings_df.iloc[i, 0]*3.2, loadings_df.iloc[i, 1]*3.2, var,
                               fontsize=10, ha='center', va='center')
                
                ax.set_xlabel(f'PC1 ({variance_ratio[0]:.1f}%)')
                ax.set_ylabel(f'PC2 ({variance_ratio[1]:.1f}%)')
                ax.set_title('PCAåŒæ ‡å›¾')
                ax.grid(True, alpha=0.3)
                ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
                ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                plt.close(fig)
            
            # ä¸»æˆåˆ†å¾—åˆ†
            st.write("##### ä¸»æˆåˆ†å¾—åˆ† (å‰10è¡Œ)")
            st.dataframe(pca_df.head(10).round(4))
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            st.write("##### æ•°æ®è´¨é‡è¯„ä¼°")
            
            # è®¡ç®—Kaiserå‡†åˆ™ï¼ˆç‰¹å¾å€¼>1ï¼‰
            kaiser_components = np.sum(pca.explained_variance_ > 1)
            st.write(f"**Kaiserå‡†åˆ™**: {kaiser_components} ä¸ªä¸»æˆåˆ†çš„ç‰¹å¾å€¼å¤§äº1")
            
            # è®¡ç®—ç´¯ç§¯æ–¹å·®è¾¾åˆ°80%çš„ä¸»æˆåˆ†æ•°
            variance_80_components = np.argmax(cumulative_variance >= 80) + 1
            st.write(f"**80%æ–¹å·®å‡†åˆ™**: å‰ {variance_80_components} ä¸ªä¸»æˆåˆ†å¯è§£é‡Š80%çš„æ–¹å·®")
            
            # ç›¸å…³çŸ©é˜µ
            if len(selected_cols) <= 10:
                st.write("##### åŸå§‹å˜é‡ç›¸å…³çŸ©é˜µ")
                corr_matrix = pca_data.corr()
                
                # ç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾
                fig, ax = plt.subplots(figsize=(10, 8))
                import seaborn as sns
                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                           square=True, linewidths=.5, ax=ax)
                ax.set_title('åŸå§‹å˜é‡ç›¸å…³çŸ©é˜µ')
                st.pyplot(fig)
                plt.close(fig)
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'ä¸»æˆåˆ†åˆ†æ',
                'variables': selected_cols,
                'n_components': n_components,
                'variance_explained': variance_df.to_dict('records'),
                'loadings_matrix': loadings_df.to_dict() if show_loadings else None,
                'pca_scores': pca_df.to_dict('records'),
                'kaiser_components': kaiser_components,
                'variance_80_components': variance_80_components,
                'standardized': standardize
            }
            
            st.success("ä¸»æˆåˆ†åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"ä¸»æˆåˆ†åˆ†æå¤±è´¥: {str(e)}")


def execute_clustering_algorithms(data):
    """èšç±»ç®—æ³•æ¯”è¾ƒ"""
    st.write("##### èšç±»ç®—æ³•æ¯”è¾ƒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    if len(numeric_cols) < 2:
        st.error("èšç±»åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    feature_vars = st.multiselect("é€‰æ‹©èšç±»å˜é‡", 
                                numeric_cols,
                                default=numeric_cols[:min(5, len(numeric_cols))])
    
    if len(feature_vars) < 2:
        st.warning("è¯·é€‰æ‹©è‡³å°‘2ä¸ªå˜é‡è¿›è¡Œèšç±»")
        return
    
    # ç®—æ³•é€‰æ‹©
    algorithms = st.multiselect(
        "é€‰æ‹©èšç±»ç®—æ³•",
        ["K-Means", "å±‚æ¬¡èšç±»", "DBSCAN", "Gaussianæ··åˆæ¨¡å‹"],
        default=["K-Means", "å±‚æ¬¡èšç±»"]
    )
    
    if not algorithms:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç®—æ³•")
        return
    
    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        n_clusters = st.slider("èšç±»æ•°é‡", 2, 10, 3)
        standardize = st.checkbox("æ ‡å‡†åŒ–æ•°æ®", value=True)
    
    with col2:
        show_plots = st.checkbox("æ˜¾ç¤ºèšç±»å›¾", value=True)
        evaluate_metrics = st.checkbox("è¯„ä¼°æŒ‡æ ‡", value=True)
    
    if st.button("æ‰§è¡Œèšç±»ç®—æ³•æ¯”è¾ƒ"):
        try:
            from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
            from sklearn.mixture import GaussianMixture
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
            import matplotlib.pyplot as plt
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            cluster_data = data[feature_vars].dropna()
            
            if len(cluster_data) < 10:
                st.error("æ ·æœ¬é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
                return
            
            # æ ‡å‡†åŒ–æ•°æ®
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(cluster_data)
                X_for_cluster = pd.DataFrame(X_scaled, columns=feature_vars, index=cluster_data.index)
            else:
                X_for_cluster = cluster_data
            
            # å®šä¹‰ç®—æ³•
            models = {}
            if "K-Means" in algorithms:
                models["K-Means"] = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            if "å±‚æ¬¡èšç±»" in algorithms:
                models["å±‚æ¬¡èšç±»"] = AgglomerativeClustering(n_clusters=n_clusters)
            if "DBSCAN" in algorithms:
                eps = st.slider("DBSCAN epså‚æ•°", 0.1, 2.0, 0.5, 0.1, key="dbscan_eps")
                min_samples = st.slider("DBSCAN min_sampleså‚æ•°", 2, 20, 5, key="dbscan_min_samples")
                models["DBSCAN"] = DBSCAN(eps=eps, min_samples=min_samples)
            if "Gaussianæ··åˆæ¨¡å‹" in algorithms:
                models["Gaussianæ··åˆæ¨¡å‹"] = GaussianMixture(n_components=n_clusters, random_state=42)
            
            # æ‰§è¡Œèšç±»å¹¶è¯„ä¼°
            results = []
            cluster_results = {}
            
            st.write("##### èšç±»ç»“æœæ¯”è¾ƒ")
            
            for name, model in models.items():
                try:
                    # èšç±»
                    if name == "Gaussianæ··åˆæ¨¡å‹":
                        labels = model.fit_predict(X_for_cluster)
                    else:
                        labels = model.fit_predict(X_for_cluster)
                    
                    cluster_results[name] = labels
                    
                    # è¯„ä¼°æŒ‡æ ‡
                    if evaluate_metrics and len(np.unique(labels)) > 1:
                        if len(np.unique(labels[labels != -1])) > 1:  # æ’é™¤å™ªå£°ç‚¹
                            valid_mask = labels != -1  # æ’é™¤DBSCANçš„å™ªå£°ç‚¹
                            if np.sum(valid_mask) > 1:
                                silhouette = silhouette_score(X_for_cluster[valid_mask], labels[valid_mask])
                                calinski = calinski_harabasz_score(X_for_cluster[valid_mask], labels[valid_mask])
                                davies_bouldin = davies_bouldin_score(X_for_cluster[valid_mask], labels[valid_mask])
                            else:
                                silhouette = calinski = davies_bouldin = np.nan
                        else:
                            silhouette = calinski = davies_bouldin = np.nan
                    else:
                        silhouette = calinski = davies_bouldin = np.nan
                    
                    # ç»Ÿè®¡èšç±»æ•°é‡
                    unique_labels = np.unique(labels)
                    n_clusters_found = len(unique_labels)
                    n_noise = np.sum(labels == -1) if -1 in unique_labels else 0
                    
                    results.append({
                        'ç®—æ³•': name,
                        'èšç±»æ•°é‡': n_clusters_found - (1 if -1 in unique_labels else 0),
                        'å™ªå£°ç‚¹æ•°': n_noise,
                        'è½®å»“ç³»æ•°': f"{silhouette:.4f}" if not np.isnan(silhouette) else "N/A",
                        'Calinski-Harabasz': f"{calinski:.4f}" if not np.isnan(calinski) else "N/A",
                        'Davies-Bouldin': f"{davies_bouldin:.4f}" if not np.isnan(davies_bouldin) else "N/A"
                    })
                    
                except Exception as e:
                    st.warning(f"{name} èšç±»å¤±è´¥: {str(e)}")
                    continue
            
            if results:
                results_df = pd.DataFrame(results)
                st.dataframe(results_df)
                
                # å¯è§†åŒ–èšç±»ç»“æœ
                if show_plots and len(feature_vars) >= 2:
                    st.write("##### èšç±»å¯è§†åŒ– (å‰ä¸¤ä¸ªå˜é‡)")
                    
                    n_algorithms = len(cluster_results)
                    if n_algorithms > 0:
                        cols_per_row = min(2, n_algorithms)
                        n_rows = (n_algorithms + cols_per_row - 1) // cols_per_row
                        
                        fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(15, 5*n_rows))
                        if n_rows == 1 and cols_per_row == 1:
                            axes = [axes]
                        elif n_rows == 1:
                            axes = [axes]
                        else:
                            axes = axes.flatten()
                        
                        for idx, (name, labels) in enumerate(cluster_results.items()):
                            ax = axes[idx] if n_algorithms > 1 else axes[0]
                            
                            # ä½¿ç”¨å‰ä¸¤ä¸ªå˜é‡ç»˜å›¾
                            x_col, y_col = feature_vars[0], feature_vars[1]
                            
                            # ä¸ºæ¯ä¸ªèšç±»åˆ†é…é¢œè‰²
                            unique_labels = np.unique(labels)
                            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))
                            
                            for label, color in zip(unique_labels, colors):
                                if label == -1:  # å™ªå£°ç‚¹
                                    mask = labels == label
                                    ax.scatter(cluster_data.iloc[mask][x_col], 
                                             cluster_data.iloc[mask][y_col], 
                                             c='black', marker='x', s=50, alpha=0.6, label='å™ªå£°')
                                else:
                                    mask = labels == label
                                    ax.scatter(cluster_data.iloc[mask][x_col], 
                                             cluster_data.iloc[mask][y_col], 
                                             c=[color], s=50, alpha=0.7, label=f'ç°‡{label}')
                            
                            ax.set_xlabel(x_col)
                            ax.set_ylabel(y_col)
                            ax.set_title(f'{name} èšç±»ç»“æœ')
                            ax.legend()
                            ax.grid(True, alpha=0.3)
                        
                        # éšè—å¤šä½™çš„å­å›¾
                        for idx in range(len(cluster_results), len(axes)):
                            axes[idx].set_visible(False)
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                        plt.close(fig)
                
                # èšç±»è§£é‡Š
                st.write("##### èšç±»è§£é‡Š")
                st.write("**è¯„ä¼°æŒ‡æ ‡è¯´æ˜:**")
                st.write("- **è½®å»“ç³»æ•°**: èŒƒå›´[-1,1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½")
                st.write("- **Calinski-HarabaszæŒ‡æ•°**: è¶Šå¤§è¶Šå¥½")
                st.write("- **Davies-BouldinæŒ‡æ•°**: è¶Šå°è¶Šå¥½")
                
                # æ¨èæœ€ä½³ç®—æ³•
                if evaluate_metrics:
                    valid_results = [r for r in results if r['è½®å»“ç³»æ•°'] != "N/A"]
                    if valid_results:
                        best_silhouette = max(valid_results, key=lambda x: float(x['è½®å»“ç³»æ•°']))
                        st.success(f"ğŸ† åŸºäºè½®å»“ç³»æ•°çš„æ¨èç®—æ³•: {best_silhouette['ç®—æ³•']}")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'èšç±»ç®—æ³•æ¯”è¾ƒ',
                'feature_variables': feature_vars,
                'algorithms': algorithms,
                'n_clusters': n_clusters,
                'results': results_df.to_dict('records') if results else [],
                'cluster_labels': {name: labels.tolist() for name, labels in cluster_results.items()},
                'standardized': standardize
            }
            
            st.success("èšç±»ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"èšç±»ç®—æ³•æ¯”è¾ƒå¤±è´¥: {str(e)}")


def execute_dimensionality_reduction(data):
    """é™ç»´ç®—æ³•"""
    st.write("##### é™ç»´ç®—æ³•æ¯”è¾ƒ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    
    if len(numeric_cols) < 3:
        st.error("é™ç»´åˆ†æéœ€è¦è‡³å°‘3ä¸ªæ•°å€¼å‹å˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    feature_vars = st.multiselect("é€‰æ‹©ç‰¹å¾å˜é‡", 
                                numeric_cols,
                                default=numeric_cols[:min(10, len(numeric_cols))])
    
    if len(feature_vars) < 3:
        st.warning("è¯·é€‰æ‹©è‡³å°‘3ä¸ªå˜é‡è¿›è¡Œé™ç»´")
        return
    
    # ç®—æ³•é€‰æ‹©
    algorithms = st.multiselect(
        "é€‰æ‹©é™ç»´ç®—æ³•",
        ["PCA", "t-SNE", "UMAP", "å› å­åˆ†æ"],
        default=["PCA", "t-SNE"]
    )
    
    if not algorithms:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç®—æ³•")
        return
    
    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        n_components = st.slider("é™ç»´åç»´æ•°", 2, min(5, len(feature_vars)-1), 2)
        standardize = st.checkbox("æ ‡å‡†åŒ–æ•°æ®", value=True)
    
    with col2:
        show_plots = st.checkbox("æ˜¾ç¤ºé™ç»´å›¾", value=True)
        show_variance = st.checkbox("æ˜¾ç¤ºæ–¹å·®è§£é‡Š", value=True)
    
    if st.button("æ‰§è¡Œé™ç»´ç®—æ³•æ¯”è¾ƒ"):
        try:
            from sklearn.decomposition import PCA, FactorAnalysis
            from sklearn.manifold import TSNE
            from sklearn.preprocessing import StandardScaler
            import matplotlib.pyplot as plt
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            dim_data = data[feature_vars].dropna()
            
            if len(dim_data) < 10:
                st.error("æ ·æœ¬é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œé™ç»´åˆ†æ")
                return
            
            # æ ‡å‡†åŒ–æ•°æ®
            if standardize:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(dim_data)
                X_for_dim = pd.DataFrame(X_scaled, columns=feature_vars, index=dim_data.index)
            else:
                X_for_dim = dim_data
            
            # æ‰§è¡Œé™ç»´ç®—æ³•
            results = {}
            variance_info = {}
            
            st.write("##### é™ç»´ç»“æœæ¯”è¾ƒ")
            
            for algorithm in algorithms:
                try:
                    if algorithm == "PCA":
                        model = PCA(n_components=n_components, random_state=42)
                        transformed = model.fit_transform(X_for_dim)
                        variance_info[algorithm] = {
                            'explained_variance_ratio': model.explained_variance_ratio_,
                            'cumulative_variance': np.cumsum(model.explained_variance_ratio_)
                        }
                        
                    elif algorithm == "t-SNE":
                        perplexity = min(30, len(X_for_dim) - 1)
                        model = TSNE(n_components=n_components, random_state=42, perplexity=perplexity)
                        transformed = model.fit_transform(X_for_dim)
                        
                    elif algorithm == "å› å­åˆ†æ":
                        model = FactorAnalysis(n_components=n_components, random_state=42)
                        transformed = model.fit_transform(X_for_dim)
                        
                    elif algorithm == "UMAP":
                        try:
                            import umap
                            model = umap.UMAP(n_components=n_components, random_state=42)
                            transformed = model.fit_transform(X_for_dim)
                        except ImportError:
                            st.warning("UMAPéœ€è¦å®‰è£…umap-learnåŒ…ï¼Œè·³è¿‡æ­¤ç®—æ³•")
                            continue
                    
                    # å­˜å‚¨ç»“æœ
                    results[algorithm] = transformed
                    
                    st.write(f"**{algorithm}**: æˆåŠŸé™ç»´åˆ° {n_components} ç»´")
                    
                except Exception as e:
                    st.warning(f"{algorithm} é™ç»´å¤±è´¥: {str(e)}")
                    continue
            
            # æ˜¾ç¤ºæ–¹å·®è§£é‡Šï¼ˆå¯¹äºæ”¯æŒçš„ç®—æ³•ï¼‰
            if show_variance and variance_info:
                st.write("##### æ–¹å·®è§£é‡Š")
                for alg, info in variance_info.items():
                    st.write(f"**{alg}**:")
                    for i, (var_ratio, cum_var) in enumerate(zip(info['explained_variance_ratio'], 
                                                               info['cumulative_variance'])):
                        st.write(f"  PC{i+1}: {var_ratio:.3f} ({cum_var:.3f} ç´¯ç§¯)")
            
            # å¯è§†åŒ–é™ç»´ç»“æœ
            if show_plots and results and n_components >= 2:
                st.write("##### é™ç»´å¯è§†åŒ–")
                
                n_algorithms = len(results)
                if n_algorithms > 0:
                    cols_per_row = min(2, n_algorithms)
                    n_rows = (n_algorithms + cols_per_row - 1) // cols_per_row
                    
                    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(15, 5*n_rows))
                    if n_rows == 1 and cols_per_row == 1:
                        axes = [axes]
                    elif n_rows == 1:
                        axes = [axes]
                    else:
                        axes = axes.flatten()
                    
                    for idx, (name, transformed_data) in enumerate(results.items()):
                        ax = axes[idx] if n_algorithms > 1 else axes[0]
                        
                        # ç»˜åˆ¶å‰ä¸¤ä¸ªç»´åº¦
                        scatter = ax.scatter(transformed_data[:, 0], transformed_data[:, 1], 
                                           alpha=0.6, s=50, c=range(len(transformed_data)), cmap='viridis')
                        
                        ax.set_xlabel('ç»´åº¦ 1')
                        ax.set_ylabel('ç»´åº¦ 2')
                        ax.set_title(f'{name} é™ç»´ç»“æœ')
                        ax.grid(True, alpha=0.3)
                        
                        # æ·»åŠ é¢œè‰²æ¡
                        plt.colorbar(scatter, ax=ax, label='æ ·æœ¬ç´¢å¼•')
                    
                    # éšè—å¤šä½™çš„å­å›¾
                    for idx in range(len(results), len(axes)):
                        axes[idx].set_visible(False)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close(fig)
            
            # ç®—æ³•æ¯”è¾ƒå»ºè®®
            st.write("##### ç®—æ³•é€‰æ‹©å»ºè®®")
            st.write("**ç®—æ³•ç‰¹ç‚¹:**")
            st.write("- **PCA**: çº¿æ€§é™ç»´ï¼Œä¿æŒæ–¹å·®æœ€å¤§åŒ–ï¼Œé€‚åˆçº¿æ€§å…³ç³»")
            st.write("- **t-SNE**: éçº¿æ€§é™ç»´ï¼Œä¿æŒå±€éƒ¨ç»“æ„ï¼Œé€‚åˆå¯è§†åŒ–")
            st.write("- **UMAP**: éçº¿æ€§é™ç»´ï¼Œä¿æŒå…¨å±€å’Œå±€éƒ¨ç»“æ„ï¼Œé€Ÿåº¦è¾ƒå¿«")
            st.write("- **å› å­åˆ†æ**: å‡è®¾æ½œåœ¨å› å­æ¨¡å‹ï¼Œé€‚åˆå¿ƒç†å­¦å’Œç¤¾ä¼šç§‘å­¦")
            
            if variance_info and "PCA" in variance_info:
                pca_var = variance_info["PCA"]["cumulative_variance"][-1]
                if pca_var >= 0.8:
                    st.success(f"âœ… PCAèƒ½è§£é‡Š {pca_var:.1%} çš„æ–¹å·®ï¼Œçº¿æ€§é™ç»´æ•ˆæœè‰¯å¥½")
                else:
                    st.info(f"â„¹ï¸ PCAåªèƒ½è§£é‡Š {pca_var:.1%} çš„æ–¹å·®ï¼Œå¯èƒ½éœ€è¦éçº¿æ€§æ–¹æ³•")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'é™ç»´ç®—æ³•æ¯”è¾ƒ',
                'feature_variables': feature_vars,
                'algorithms': list(results.keys()),
                'n_components': n_components,
                'variance_info': variance_info,
                'transformed_shapes': {name: data.shape for name, data in results.items()},
                'standardized': standardize
            }
            
            st.success("é™ç»´ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"é™ç»´ç®—æ³•æ¯”è¾ƒå¤±è´¥: {str(e)}")


def execute_model_evaluation(data):
    """æ¨¡å‹è¯„ä¼°"""
    st.write("##### æ¨¡å‹è¯„ä¼°å·¥å…·")
    st.info("æ¨¡å‹è¯„ä¼°åŠŸèƒ½å°†å¸®åŠ©æ‚¨è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹è¯„ä¼°çš„å…·ä½“åŠŸèƒ½
    # ä¾‹å¦‚äº¤å‰éªŒè¯ã€å­¦ä¹ æ›²çº¿ã€ROCæ›²çº¿ç­‰
    st.write("åŠŸèƒ½å¼€å‘ä¸­...")
    st.write("#### ğŸ“Š ä¸»æˆåˆ†åˆ†æ")
    st.info("ä¸»æˆåˆ†åˆ†æåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")


def execute_anova_analysis(data):
    """æ–¹å·®åˆ†æ"""
    st.write("#### ğŸ“ˆ æ–¹å·®åˆ†æ")
    
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if not numeric_cols:
        st.error("æ–¹å·®åˆ†æéœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼å‹å› å˜é‡")
        return
    
    if not categorical_cols:
        st.error("æ–¹å·®åˆ†æéœ€è¦è‡³å°‘ä¸€ä¸ªåˆ†ç±»å‹è‡ªå˜é‡")
        return
    
    # é€‰æ‹©å˜é‡
    st.write("##### å˜é‡é€‰æ‹©")
    dependent_var = st.selectbox("é€‰æ‹©å› å˜é‡ï¼ˆæ•°å€¼å‹ï¼‰", numeric_cols)
    independent_vars = st.multiselect(
        "é€‰æ‹©è‡ªå˜é‡ï¼ˆåˆ†ç±»å‹ï¼‰", 
        categorical_cols,
        help="å¯ä»¥é€‰æ‹©å¤šä¸ªåˆ†ç±»å˜é‡è¿›è¡Œå¤šå› ç´ æ–¹å·®åˆ†æ"
    )
    
    if not independent_vars:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†ç±»å‹è‡ªå˜é‡")
        return
    
    # åˆ†æç±»å‹é€‰æ‹©
    if len(independent_vars) == 1:
        analysis_type = "å•å› ç´ æ–¹å·®åˆ†æ"
    else:
        analysis_type = st.selectbox(
            "åˆ†æç±»å‹",
            ["å¤šå› ç´ æ–¹å·®åˆ†æ", "å•å› ç´ æ–¹å·®åˆ†æï¼ˆé€ä¸ªæ£€éªŒï¼‰"]
        )
    
    # å…¶ä»–é€‰é¡¹
    col1, col2 = st.columns(2)
    with col1:
        alpha_level = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
        post_hoc = st.checkbox("äº‹åæ¯”è¾ƒæ£€éªŒ", value=True)
    
    with col2:
        descriptive_stats = st.checkbox("æè¿°ç»Ÿè®¡", value=True)
        homogeneity_test = st.checkbox("æ–¹å·®é½æ€§æ£€éªŒ", value=True)
    
    if st.button("æ‰§è¡Œæ–¹å·®åˆ†æ"):
        try:
            from scipy import stats
            import numpy as np
            
            # å‡†å¤‡æ•°æ®
            analysis_data = data[[dependent_var] + independent_vars].dropna()
            
            if analysis_data.empty:
                st.error("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œåˆ†æ")
                return
            
            if len(analysis_data) < 3:
                st.error("æ ·æœ¬é‡å¤ªå°ï¼Œæ— æ³•è¿›è¡Œæ–¹å·®åˆ†æ")
                return
            
            st.write("##### æ–¹å·®åˆ†æç»“æœ")
            
            # æè¿°ç»Ÿè®¡
            if descriptive_stats:
                st.write("**æè¿°ç»Ÿè®¡:**")
                if len(independent_vars) == 1:
                    desc_stats = analysis_data.groupby(independent_vars[0])[dependent_var].agg([
                        'count', 'mean', 'std', 'min', 'max'
                    ]).round(3)
                    desc_stats.columns = ['æ ·æœ¬é‡', 'å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼']
                    st.dataframe(desc_stats)
                else:
                    # å¤šå› ç´ æè¿°ç»Ÿè®¡
                    for var in independent_vars:
                        st.write(f"æŒ‰ {var} åˆ†ç»„:")
                        desc_stats = analysis_data.groupby(var)[dependent_var].agg([
                            'count', 'mean', 'std'
                        ]).round(3)
                        desc_stats.columns = ['æ ·æœ¬é‡', 'å‡å€¼', 'æ ‡å‡†å·®']
                        st.dataframe(desc_stats)
            
            # æ‰§è¡Œæ–¹å·®åˆ†æ
            if len(independent_vars) == 1 or analysis_type == "å•å› ç´ æ–¹å·®åˆ†æï¼ˆé€ä¸ªæ£€éªŒï¼‰":
                # å•å› ç´ æ–¹å·®åˆ†æ
                for var in independent_vars:
                    st.write(f"**{var} çš„å•å› ç´ æ–¹å·®åˆ†æ:**")
                    
                    groups = []
                    group_names = []
                    for group_name in analysis_data[var].unique():
                        group_data = analysis_data[analysis_data[var] == group_name][dependent_var]
                        if len(group_data) > 0:
                            groups.append(group_data)
                            group_names.append(str(group_name))
                    
                    if len(groups) < 2:
                        st.warning(f"å˜é‡ {var} çš„æœ‰æ•ˆç»„åˆ«å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œæ–¹å·®åˆ†æ")
                        continue
                    
                    # æ–¹å·®é½æ€§æ£€éªŒï¼ˆLeveneæ£€éªŒï¼‰
                    if homogeneity_test and len(groups) >= 2:
                        try:
                            levene_stat, levene_p = stats.levene(*groups)
                            st.write(f"**Leveneæ–¹å·®é½æ€§æ£€éªŒ:**")
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("Leveneç»Ÿè®¡é‡", f"{levene_stat:.4f}")
                            with col2:
                                st.metric("på€¼", f"{levene_p:.4f}")
                            
                            if levene_p < alpha_level:
                                st.warning("âš ï¸ æ–¹å·®ä¸é½ï¼Œè¿åäº†æ–¹å·®åˆ†æçš„å‡è®¾")
                            else:
                                st.success("âœ… æ–¹å·®é½æ€§å‡è®¾æ»¡è¶³")
                        except:
                            st.info("æ— æ³•è¿›è¡Œæ–¹å·®é½æ€§æ£€éªŒ")
                    
                    # æ‰§è¡Œå•å› ç´ æ–¹å·®åˆ†æ
                    try:
                        f_stat, p_value = stats.f_oneway(*groups)
                        
                        # è®¡ç®—æ•ˆåº”é‡ï¼ˆeta squaredï¼‰
                        ss_between = sum(len(group) * (np.mean(group) - np.mean(analysis_data[dependent_var]))**2 for group in groups)
                        ss_total = np.sum((analysis_data[dependent_var] - np.mean(analysis_data[dependent_var]))**2)
                        eta_squared = ss_between / ss_total if ss_total > 0 else 0
                        
                        # æ˜¾ç¤ºç»“æœ
                        st.write("**ANOVAç»“æœ:**")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Fç»Ÿè®¡é‡", f"{f_stat:.4f}")
                        with col2:
                            st.metric("på€¼", f"{p_value:.4f}")
                        with col3:
                            st.metric("Î·Â²", f"{eta_squared:.4f}")
                        
                        # æ˜¾è‘—æ€§åˆ¤æ–­
                        if p_value < alpha_level:
                            st.success(f"âœ… ç»“æœæ˜¾è‘— (p < {alpha_level})ï¼Œç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚")
                            
                            # äº‹åæ¯”è¾ƒ
                            if post_hoc and len(groups) > 2:
                                st.write("**äº‹åæ¯”è¾ƒ (Tukey HSD):**")
                                try:
                                    from scipy.stats import tukey_hsd
                                    tukey_result = tukey_hsd(*groups)
                                    
                                    # åˆ›å»ºäº‹åæ¯”è¾ƒè¡¨
                                    comparisons = []
                                    for i in range(len(group_names)):
                                        for j in range(i+1, len(group_names)):
                                            p_adj = tukey_result.pvalue[i, j]
                                            mean_diff = np.mean(groups[i]) - np.mean(groups[j])
                                            comparisons.append({
                                                'æ¯”è¾ƒ': f"{group_names[i]} vs {group_names[j]}",
                                                'å‡å€¼å·®': f"{mean_diff:.3f}",
                                                'è°ƒæ•´på€¼': f"{p_adj:.4f}",
                                                'æ˜¾è‘—æ€§': "æ˜¯" if p_adj < alpha_level else "å¦"
                                            })
                                    
                                    comparison_df = pd.DataFrame(comparisons)
                                    st.dataframe(comparison_df)
                                    
                                except ImportError:
                                    st.info("äº‹åæ¯”è¾ƒéœ€è¦æ›´æ–°çš„scipyç‰ˆæœ¬ï¼Œä½¿ç”¨ç®€å•çš„æˆå¯¹tæ£€éªŒ")
                                    # ç®€å•çš„æˆå¯¹æ¯”è¾ƒ
                                    comparisons = []
                                    for i in range(len(group_names)):
                                        for j in range(i+1, len(group_names)):
                                            t_stat, p_val = stats.ttest_ind(groups[i], groups[j])
                                            mean_diff = np.mean(groups[i]) - np.mean(groups[j])
                                            comparisons.append({
                                                'æ¯”è¾ƒ': f"{group_names[i]} vs {group_names[j]}",
                                                'å‡å€¼å·®': f"{mean_diff:.3f}",
                                                'tç»Ÿè®¡é‡': f"{t_stat:.3f}",
                                                'på€¼': f"{p_val:.4f}",
                                                'æ˜¾è‘—æ€§': "æ˜¯" if p_val < alpha_level else "å¦"
                                            })
                                    
                                    comparison_df = pd.DataFrame(comparisons)
                                    st.dataframe(comparison_df)
                        else:
                            st.info(f"ç»“æœä¸æ˜¾è‘— (p â‰¥ {alpha_level})ï¼Œç»„é—´æ— æ˜¾è‘—å·®å¼‚")
                        
                    except Exception as e:
                        st.error(f"æ–¹å·®åˆ†æè®¡ç®—å¤±è´¥: {str(e)}")
            
            else:
                # å¤šå› ç´ æ–¹å·®åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
                st.info("å¤šå› ç´ æ–¹å·®åˆ†æåŠŸèƒ½éœ€è¦æ›´é«˜çº§çš„ç»Ÿè®¡åº“æ”¯æŒï¼Œå½“å‰ä½¿ç”¨ç®€åŒ–åˆ†æ")
                for var in independent_vars:
                    st.write(f"**{var} çš„ç‹¬ç«‹æ•ˆåº”:**")
                    groups = [analysis_data[analysis_data[var] == group][dependent_var] 
                             for group in analysis_data[var].unique()]
                    groups = [g for g in groups if len(g) > 0]
                    
                    if len(groups) >= 2:
                        f_stat, p_value = stats.f_oneway(*groups)
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Fç»Ÿè®¡é‡", f"{f_stat:.4f}")
                        with col2:
                            st.metric("på€¼", f"{p_value:.4f}")
                        
                        if p_value < alpha_level:
                            st.success("âœ… æ˜¾è‘—")
                        else:
                            st.info("ä¸æ˜¾è‘—")
            
            # å­˜å‚¨ç»“æœ
            st.session_state.analysis_results = {
                'type': 'æ–¹å·®åˆ†æ',
                'dependent_var': dependent_var,
                'independent_vars': independent_vars,
                'analysis_type': analysis_type,
                'alpha_level': alpha_level,
                'status': 'completed'
            }
            
            st.success("æ–¹å·®åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            st.error(f"æ–¹å·®åˆ†æå¤±è´¥: {str(e)}")
            st.write("é”™è¯¯è¯¦æƒ…:", str(e))


def execute_single_analysis(module, analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªåˆ†ææ–¹æ³•å¹¶è¿”å›ç»“æœ"""
    try:
        # æ ¹æ®æ¨¡å—æ‰§è¡Œå¯¹åº”çš„åˆ†æ
        if module == "æ•°æ®å¤„ç†":
            return execute_data_processing_single(analysis_option, processor, data)
        elif module == "é€šç”¨æ–¹æ³•":
            return execute_general_methods_single(analysis_option, processor, data)
        elif module == "é—®å·ç ”ç©¶":
            return execute_questionnaire_analysis_single(analysis_option, processor, data)
        elif module == "è¿›é˜¶æ–¹æ³•":
            return execute_advanced_methods_single(analysis_option, processor, data)
        elif module == "æœºå™¨å­¦ä¹ ":
            return execute_machine_learning_single(analysis_option, processor, data)
        elif module == "æ—¶é—´åºåˆ—":
            return execute_time_series_single(analysis_option, processor, data)
        else:
            return None
    except Exception as e:
        st.error(f"æ‰§è¡Œ {analysis_option} æ—¶å‡ºé”™: {str(e)}")
        return {"error": str(e)}


def execute_data_processing_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªæ•°æ®å¤„ç†åˆ†æ"""
    if analysis_option == "æ•°æ®æ¸…æ´—":
        # æ‰§è¡Œæ•°æ®æ¸…æ´—å¹¶è¿”å›ç»“æœ
        missing_count = data.isnull().sum().sum()
        duplicate_count = data.duplicated().sum()
        return {
            'type': 'æ•°æ®æ¸…æ´—',
            'original_shape': data.shape,
            'missing_count': missing_count,
            'duplicate_count': duplicate_count,
            'status': 'completed'
        }
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def execute_general_methods_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªé€šç”¨æ–¹æ³•åˆ†æ"""
    if analysis_option == "é¢‘æ•°åˆ†æ":
        # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªåˆé€‚çš„åˆ—è¿›è¡Œé¢‘æ•°åˆ†æ
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        
        if categorical_cols:
            col = categorical_cols[0]
            freq_table = data[col].value_counts().reset_index()
            freq_table.columns = ['ç±»åˆ«', 'é¢‘æ•°']
            freq_table['ç™¾åˆ†æ¯”'] = (freq_table['é¢‘æ•°'] / freq_table['é¢‘æ•°'].sum() * 100).round(2)
        elif numeric_cols:
            col = numeric_cols[0]
            freq_table = pd.cut(data[col], bins=10).value_counts().reset_index()
            freq_table.columns = ['åŒºé—´', 'é¢‘æ•°']
            freq_table['ç™¾åˆ†æ¯”'] = (freq_table['é¢‘æ•°'] / freq_table['é¢‘æ•°'].sum() * 100).round(2)
        else:
            return {"error": "æ²¡æœ‰åˆé€‚çš„åˆ—è¿›è¡Œé¢‘æ•°åˆ†æ"}
            
        return {
            'type': 'é¢‘æ•°åˆ†æ',
            'variable': col,
            'frequency_table': freq_table,
            'status': 'completed'
        }
    
    elif analysis_option == "æè¿°ç»Ÿè®¡":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        if not numeric_cols:
            return {"error": "æ²¡æœ‰æ•°å€¼å‹å˜é‡è¿›è¡Œæè¿°ç»Ÿè®¡"}
        
        desc_stats = data[numeric_cols[:5]].describe().round(3)
        return {
            'type': 'æè¿°ç»Ÿè®¡',
            'variables': numeric_cols[:5],
            'descriptive_stats': desc_stats,
            'status': 'completed'
        }
    
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def execute_questionnaire_analysis_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªé—®å·ç ”ç©¶åˆ†æ"""
    if analysis_option == "ä¿¡åº¦åˆ†æ":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        if len(numeric_cols) < 2:
            return {"error": "ä¿¡åº¦åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡"}
        
        try:
            reliability_results = processor.reliability_analysis(data[numeric_cols[:min(10, len(numeric_cols))]])
            return {
                'type': 'ä¿¡åº¦åˆ†æ',
                'variables': numeric_cols[:min(10, len(numeric_cols))],
                'reliability_results': reliability_results,
                'status': 'completed'
            }
        except Exception as e:
            return {"error": f"ä¿¡åº¦åˆ†æå¤±è´¥: {str(e)}"}
    
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def execute_advanced_methods_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªè¿›é˜¶æ–¹æ³•åˆ†æ"""
    if analysis_option == "çº¿æ€§å›å½’":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        if len(numeric_cols) < 2:
            return {"error": "çº¿æ€§å›å½’éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡"}
        
        try:
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import r2_score, mean_squared_error
            import numpy as np
            
            # è‡ªåŠ¨é€‰æ‹©å‰ä¸¤ä¸ªæ•°å€¼åˆ—
            y_var = numeric_cols[0]
            x_vars = numeric_cols[1:min(4, len(numeric_cols))]
            
            X = data[x_vars].dropna()
            y = data[y_var].dropna()
            
            common_index = X.index.intersection(y.index)
            X = X.loc[common_index]
            y = y.loc[common_index]
            
            model = LinearRegression()
            model.fit(X, y)
            y_pred = model.predict(X)
            
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))
            
            return {
                'type': 'çº¿æ€§å›å½’',
                'dependent_var': y_var,
                'independent_vars': x_vars,
                'r2_score': r2,
                'rmse': rmse,
                'status': 'completed'
            }
        except Exception as e:
            return {"error": f"çº¿æ€§å›å½’åˆ†æå¤±è´¥: {str(e)}"}
    
    elif analysis_option == "èšç±»åˆ†æ":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        if len(numeric_cols) < 2:
            return {"error": "èšç±»åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡"}
        
        try:
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            from sklearn.metrics import silhouette_score
            
            # è‡ªåŠ¨é€‰æ‹©å‰5ä¸ªæ•°å€¼åˆ—
            selected_cols = numeric_cols[:min(5, len(numeric_cols))]
            cluster_data = data[selected_cols].dropna()
            
            if len(cluster_data) < 4:
                return {"error": "æ ·æœ¬é‡å¤ªå°ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ"}
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(cluster_data)
            
            # K-meansèšç±»
            n_clusters = 3  # é»˜è®¤3ä¸ªèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)
            
            # è®¡ç®—è½®å»“ç³»æ•°
            silhouette_avg = silhouette_score(X_scaled, labels)
            
            return {
                'type': 'èšç±»åˆ†æ',
                'variables': selected_cols,
                'n_clusters': n_clusters,
                'silhouette_score': silhouette_avg,
                'n_samples': len(cluster_data),
                'status': 'completed'
            }
        except Exception as e:
            return {"error": f"èšç±»åˆ†æå¤±è´¥: {str(e)}"}
    
    elif analysis_option == "å› å­åˆ†æ":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        if len(numeric_cols) < 3:
            return {"error": "å› å­åˆ†æéœ€è¦è‡³å°‘3ä¸ªæ•°å€¼å‹å˜é‡"}
        
        try:
            from sklearn.decomposition import FactorAnalysis
            from sklearn.preprocessing import StandardScaler
            import numpy as np
            
            # è‡ªåŠ¨é€‰æ‹©å‰8ä¸ªæ•°å€¼åˆ—
            selected_cols = numeric_cols[:min(8, len(numeric_cols))]
            factor_data = data[selected_cols].dropna()
            
            if len(factor_data) < len(selected_cols) * 2:
                return {"error": "æ ·æœ¬é‡ä¸è¶³ï¼Œå»ºè®®æ ·æœ¬é‡è‡³å°‘æ˜¯å˜é‡æ•°çš„2å€"}
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(factor_data)
            
            # å› å­åˆ†æ
            n_factors = min(3, len(selected_cols) - 1)
            fa = FactorAnalysis(n_components=n_factors, random_state=42)
            fa.fit(X_scaled)
            
            # è®¡ç®—æ–¹å·®è§£é‡Šæ¯”ä¾‹
            eigenvalues = np.sum(fa.components_**2, axis=1)
            variance_explained = eigenvalues / len(selected_cols) * 100
            
            return {
                'type': 'å› å­åˆ†æ',
                'variables': selected_cols,
                'n_factors': n_factors,
                'variance_explained': variance_explained.tolist(),
                'total_variance': np.sum(variance_explained),
                'status': 'completed'
            }
        except Exception as e:
            return {"error": f"å› å­åˆ†æå¤±è´¥: {str(e)}"}
    
    elif analysis_option == "æ–¹å·®åˆ†æ":
        numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if not numeric_cols:
            return {"error": "æ–¹å·®åˆ†æéœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼å‹å› å˜é‡"}
        if not categorical_cols:
            return {"error": "æ–¹å·®åˆ†æéœ€è¦è‡³å°‘ä¸€ä¸ªåˆ†ç±»å‹è‡ªå˜é‡"}
        
        try:
            from scipy import stats
            import numpy as np
            
            # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—å’Œç¬¬ä¸€ä¸ªåˆ†ç±»åˆ—
            dependent_var = numeric_cols[0]
            independent_var = categorical_cols[0]
            
            analysis_data = data[[dependent_var, independent_var]].dropna()
            
            if len(analysis_data) < 3:
                return {"error": "æ ·æœ¬é‡å¤ªå°ï¼Œæ— æ³•è¿›è¡Œæ–¹å·®åˆ†æ"}
            
            # æŒ‰ç»„åˆ†å‰²æ•°æ®
            groups = []
            group_names = []
            for group_name in analysis_data[independent_var].unique():
                group_data = analysis_data[analysis_data[independent_var] == group_name][dependent_var]
                if len(group_data) > 0:
                    groups.append(group_data)
                    group_names.append(str(group_name))
            
            if len(groups) < 2:
                return {"error": "æœ‰æ•ˆç»„åˆ«å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œæ–¹å·®åˆ†æ"}
            
            # æ‰§è¡Œå•å› ç´ æ–¹å·®åˆ†æ
            f_stat, p_value = stats.f_oneway(*groups)
            
            return {
                'type': 'æ–¹å·®åˆ†æ',
                'dependent_var': dependent_var,
                'independent_var': independent_var,
                'f_statistic': f_stat,
                'p_value': p_value,
                'n_groups': len(groups),
                'significant': p_value < 0.05,
                'status': 'completed'
            }
        except Exception as e:
            return {"error": f"æ–¹å·®åˆ†æå¤±è´¥: {str(e)}"}
    
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def execute_machine_learning_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªæœºå™¨å­¦ä¹ åˆ†æ"""
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def execute_time_series_single(analysis_option, processor, data):
    """æ‰§è¡Œå•ä¸ªæ—¶é—´åºåˆ—åˆ†æ"""
    return {"status": "not_implemented", "message": f"{analysis_option}åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"}


def display_analysis_summary(result):
    """æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦"""
    if 'error' in result:
        st.error(f"é”™è¯¯: {result['error']}")
        return
    
    if result.get('status') == 'not_implemented':
        st.info(result.get('message', 'åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­'))
        return
    
    analysis_type = result.get('type', 'æœªçŸ¥åˆ†æ')
    st.write(f"**åˆ†æç±»å‹:** {analysis_type}")
    
    if analysis_type == 'é¢‘æ•°åˆ†æ':
        st.write(f"**åˆ†æå˜é‡:** {result['variable']}")
        st.dataframe(result['frequency_table'].head(5))
    
    elif analysis_type == 'æè¿°ç»Ÿè®¡':
        st.write(f"**åˆ†æå˜é‡:** {', '.join(result['variables'])}")
        st.dataframe(result['descriptive_stats'].head())
    
    elif analysis_type == 'ä¿¡åº¦åˆ†æ':
        st.write(f"**é‡è¡¨å˜é‡:** {', '.join(result['variables'])}")
        alpha = result['reliability_results']['cronbach_alpha']
        st.metric("å…‹æœ—å·´èµ«Î±ç³»æ•°", f"{alpha:.4f}")
    
    elif analysis_type == 'çº¿æ€§å›å½’':
        st.write(f"**å› å˜é‡:** {result['dependent_var']}")
        st.write(f"**è‡ªå˜é‡:** {', '.join(result['independent_vars'])}")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("RÂ²", f"{result['r2_score']:.4f}")
        with col2:
            st.metric("RMSE", f"{result['rmse']:.4f}")
    
    elif analysis_type == 'èšç±»åˆ†æ':
        st.write(f"**åˆ†æå˜é‡:** {', '.join(result['variables'])}")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("èšç±»æ•°", result['n_clusters'])
        with col2:
            st.metric("è½®å»“ç³»æ•°", f"{result['silhouette_score']:.4f}")
        with col3:
            st.metric("æ ·æœ¬æ•°", result['n_samples'])
    
    elif analysis_type == 'å› å­åˆ†æ':
        st.write(f"**åˆ†æå˜é‡:** {', '.join(result['variables'])}")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("å› å­æ•°", result['n_factors'])
        with col2:
            st.metric("æ€»æ–¹å·®è§£é‡Š", f"{result['total_variance']:.1f}%")
        
        # æ˜¾ç¤ºå„å› å­çš„æ–¹å·®è§£é‡Š
        variance_data = pd.DataFrame({
            'å› å­': [f"å› å­{i+1}" for i in range(result['n_factors'])],
            'æ–¹å·®è§£é‡Š(%)': [f"{v:.1f}" for v in result['variance_explained']]
        })
        st.dataframe(variance_data)
    
    elif analysis_type == 'æ–¹å·®åˆ†æ':
        st.write(f"**å› å˜é‡:** {result['dependent_var']}")
        st.write(f"**è‡ªå˜é‡:** {result['independent_var']}")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Fç»Ÿè®¡é‡", f"{result['f_statistic']:.4f}")
        with col2:
            st.metric("på€¼", f"{result['p_value']:.4f}")
        with col3:
            significance = "æ˜¾è‘—" if result['significant'] else "ä¸æ˜¾è‘—"
            st.metric("ç»“æœ", significance)
    
    elif analysis_type == 'æ•°æ®æ¸…æ´—':
        col1, col2 = st.columns(2)
        with col1:
            st.metric("åŸå§‹æ•°æ®", f"{result['original_shape'][0]} Ã— {result['original_shape'][1]}")
        with col2:
            st.metric("ç¼ºå¤±å€¼", result['missing_count'])


def display_batch_results_summary(batch_results):
    """æ˜¾ç¤ºæ‰¹é‡åˆ†æç»“æœæ±‡æ€»"""
    if not batch_results:
        st.info("æ²¡æœ‰åˆ†æç»“æœ")
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_analyses = len(batch_results)
    successful_analyses = sum(1 for result in batch_results.values() 
                             if result.get('status') == 'completed')
    failed_analyses = sum(1 for result in batch_results.values() 
                         if 'error' in result)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ€»åˆ†ææ•°", total_analyses)
    with col2:
        st.metric("æˆåŠŸå®Œæˆ", successful_analyses)
    with col3:
        st.metric("æ‰§è¡Œå¤±è´¥", failed_analyses)
    
    # æ˜¾ç¤ºæ¯ä¸ªåˆ†æçš„ç»“æœ
    for analysis_name, result in batch_results.items():
        with st.expander(f"ğŸ“Š {analysis_name}"):
            display_analysis_summary(result)


def display_batch_analysis_results():
    """æ˜¾ç¤ºæ‰¹é‡åˆ†æçš„è¯¦ç»†ç»“æœ"""
    if 'batch_analysis_results' not in st.session_state:
        return
    
    st.write("### ğŸ“‹ æ‰¹é‡åˆ†æè¯¦ç»†ç»“æœ")
    batch_results = st.session_state.batch_analysis_results
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    if batch_results:
        tabs = st.tabs(list(batch_results.keys()))
        
        for i, (analysis_name, result) in enumerate(batch_results.items()):
            with tabs[i]:
                display_analysis_summary(result)
                
                # å¦‚æœæœ‰è¯¦ç»†ç»“æœï¼Œæ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
                if result.get('status') == 'completed':
                    if result.get('type') == 'é¢‘æ•°åˆ†æ' and 'frequency_table' in result:
                        st.write("#### å®Œæ•´é¢‘æ•°åˆ†å¸ƒè¡¨")
                        st.dataframe(result['frequency_table'])
                    
                    elif result.get('type') == 'æè¿°ç»Ÿè®¡' and 'descriptive_stats' in result:
                        st.write("#### å®Œæ•´æè¿°ç»Ÿè®¡")
                        st.dataframe(result['descriptive_stats'])


def visualize_section():
    """æ•°æ®å¯è§†åŒ–éƒ¨åˆ†"""
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šåˆ†æç±»å‹çš„ç»“æœå±•ç¤º
    if hasattr(st.session_state, 'analysis_type') and st.session_state.analysis_type in ['contrast_analysis', 'reliability_analysis', 'validity_analysis']:
        analysis_type = st.session_state.analysis_type
        
        if analysis_type == 'contrast_analysis':
            # åå·®åˆ†æç•Œé¢
            st.subheader("ğŸ“Š åå·®åˆ†æ")
            current_data = st.session_state.analysis_data
            
            # é€‰æ‹©åˆ†ç»„åˆ—
            categorical_cols = current_data.select_dtypes(include=['object', 'category']).columns.tolist()
            group_column = st.selectbox("é€‰æ‹©åˆ†ç»„åˆ—", categorical_cols)
            
            # é€‰æ‹©æ•°å€¼åˆ—
            numeric_cols = current_data.select_dtypes(include=['number']).columns.tolist()
            value_columns = st.multiselect("é€‰æ‹©è¦åˆ†æçš„æ•°å€¼åˆ—", numeric_cols, default=numeric_cols[:min(3, len(numeric_cols))])
            
            # é€‰æ‹©èšåˆæ–¹æ³•
            agg_methods = ['mean', 'median', 'sum', 'std']
            agg_method = st.selectbox("é€‰æ‹©èšåˆæ–¹æ³•", agg_methods, format_func=lambda x: {'mean':'å¹³å‡å€¼', 'median':'ä¸­ä½æ•°', 'sum':'æ€»å’Œ', 'std':'æ ‡å‡†å·®'}[x])
            
            if st.button("æ‰§è¡Œåå·®åˆ†æ"):
                if not group_column or not value_columns:
                    st.error("è¯·é€‰æ‹©åˆ†ç»„åˆ—å’Œè‡³å°‘ä¸€ä¸ªæ•°å€¼åˆ—")
                else:
                    with st.spinner("æ­£åœ¨è¿›è¡Œåå·®åˆ†æ..."):
                        # æ‰§è¡Œåå·®åˆ†æ
                        processor = DataProcessor()
                        try:
                            results = processor.contrast_analysis(
                                data=current_data,
                                group_column=group_column,
                                value_columns=value_columns,
                                agg_method=agg_method
                            )
                            
                            # æ˜¾ç¤ºç»“æœ
                            st.session_state.special_analysis_results = results
                            st.session_state.analysis_type = 'contrast_analysis_results'
                            st.rerun()
                        except Exception as e:
                            st.error(f"åˆ†æå¤±è´¥: {str(e)}")
            
        elif analysis_type in ['reliability_analysis', 'validity_analysis', 'contrast_analysis_results']:
            # æ˜¾ç¤ºåˆ†æç»“æœ
            if analysis_type == 'reliability_analysis':
                st.subheader("âœ… ä¿¡åº¦åˆ†æç»“æœ")
            elif analysis_type == 'validity_analysis':
                st.subheader("âœ… æ•ˆåº¦åˆ†æç»“æœ")
            else:
                st.subheader("âœ… åå·®åˆ†æç»“æœ")
                
            # æ˜¾ç¤ºåˆ†æç»“æœ
            if hasattr(st.session_state, 'special_analysis_results'):
                results = st.session_state.special_analysis_results
                
                # æ ¹æ®åˆ†æç±»å‹æ˜¾ç¤ºä¸åŒçš„ç»“æœ
                if analysis_type == 'reliability_analysis':
                    # æ˜¾ç¤ºä¿¡åº¦åˆ†æç»“æœ
                    st.write("### å…‹æœ—å·´èµ«Î±ç³»æ•°")
                    st.info(f"Î±ç³»æ•°: {results['cronbach_alpha']:.4f}")
                    st.write(f"ä¿¡åº¦è¯„ä»·: {results['reliability_interpretation']}")
                    
                    st.write("### é¡¹ç›®é—´ç›¸å…³æ€§")
                    st.dataframe(results['item_correlations'])
                    
                    st.write("### é¡¹ç›®ç»Ÿè®¡")
                    st.dataframe(results['item_statistics'])
                    
                elif analysis_type == 'validity_analysis':
                    # æ˜¾ç¤ºæ•ˆåº¦åˆ†æç»“æœ
                    st.write("### ç»“æ„æ•ˆåº¦ (PCAåˆ†æ)")
                    st.write(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {results['pca_results']['explained_variance_ratio']:.4f}")
                    st.write(f"ç´¯ç§¯è§£é‡Šæ–¹å·®: {results['pca_results']['cumulative_variance_ratio']:.4f}")
                    
                    st.write("### å› å­è½½è·çŸ©é˜µ")
                    st.dataframe(results['pca_results']['factor_loadings'])
                    
                    if 'criterion_validity' in results:
                        st.write("### æ•ˆæ ‡æ•ˆåº¦")
                        st.dataframe(results['criterion_validity'])
                    
                    st.write("### æ•ˆåº¦è§£é‡Š")
                    st.info(results['validity_interpretation'])
                    
                elif analysis_type == 'contrast_analysis_results':
                    # æ˜¾ç¤ºåå·®åˆ†æç»“æœ
                    st.write(f"### åˆ†ç»„ç»Ÿè®¡ ({results['agg_method']})")
                    st.dataframe(results['group_statistics'])
                    
                    st.write("### ç»„é—´å·®å¼‚")
                    st.dataframe(results['group_differences'])
                    
                    st.write("### æ€»ä½“ç»Ÿè®¡")
                    st.dataframe(results['overall_statistics'])
                    
                    st.write("### å˜å¼‚ç³»æ•°")
                    st.dataframe(results['coefficient_of_variation'])
            
            # è¿”å›æŒ‰é’®
            if st.button("è¿”å›åˆ†æé€‰æ‹©"):
                if hasattr(st.session_state, 'analysis_type'):
                    del st.session_state.analysis_type
                if hasattr(st.session_state, 'special_analysis_results'):
                    del st.session_state.special_analysis_results
                st.session_state.current_step = 'analyze'
                st.rerun()
    else:
        # å¸¸è§„å¯è§†åŒ–é€»è¾‘
        st.subheader("ğŸ“ˆ æ•°æ®å¯è§†åŒ–")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if st.session_state.data is None:
            st.error("è¯·å…ˆä¸Šä¼ æ•°æ®!")
            if st.button("è¿”å›ä¸Šä¼ "):
                st.session_state.current_step = 'upload'
                st.rerun()
            return
        
        # è·å–å½“å‰æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨å·²å¤„ç†çš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸ä¸º Noneï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹ä¸Šä¼ çš„æ•°æ®
        if 'processed_data' in st.session_state and st.session_state.processed_data is not None:
            current_data = st.session_state.processed_data
        else:
            current_data = st.session_state.data
        
        # äºŒæ¬¡æ£€æŸ¥ç¡®ä¿current_dataä¸ä¸ºNone
        if current_data is None:
            st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ•°æ®!")
            if st.button("è¿”å›ä¸Šä¼ "):
                st.session_state.current_step = 'upload'
                st.rerun()
            return
        
        # åˆ›å»ºå¯è§†åŒ–ç®¡ç†å™¨
        viz_manager = create_visualization_manager()
        
        # å¯è§†åŒ–ç±»å‹é€‰æ‹©
        st.write("### ğŸ¨ å¯è§†åŒ–é€‰é¡¹")
        viz_option = st.radio(
            "é€‰æ‹©å¯è§†åŒ–æ¨¡å¼",
            ["æ™ºèƒ½æ¨èå¯è§†åŒ–", "è‡ªå®šä¹‰å¯è§†åŒ–", "äº¤äº’å¼å›¾è¡¨"]
        )
    
    # æ™ºèƒ½æ¨èå¯è§†åŒ–
    if viz_option == "æ™ºèƒ½æ¨èå¯è§†åŒ–":
        st.write("#### ğŸ¤– ç³»ç»Ÿæ¨èå›¾è¡¨")
        
        with st.spinner("æ­£åœ¨åˆ†ææ•°æ®å¹¶ç”Ÿæˆæ¨èå›¾è¡¨..."):
            # è·å–æ•°æ®ç‰¹å¾
            data_features = {}
            data_features['numeric_columns'] = list(current_data.select_dtypes(include=['number']).columns)
            data_features['categorical_columns'] = list(current_data.select_dtypes(include=['object', 'category']).columns)
            
            # æ£€æµ‹æ—¥æœŸåˆ—
            date_columns = []
            for col in current_data.columns:
                if pd.api.types.is_datetime64_any_dtype(current_data[col]):
                    date_columns.append(col)
                elif 'date' in col.lower() or 'time' in col.lower():
                    try:
                        current_data[col] = pd.to_datetime(current_data[col])
                        date_columns.append(col)
                    except:
                        pass
            data_features['date_columns'] = date_columns
            
            # ç”Ÿæˆæ¨èå›¾è¡¨
            recommended_charts = viz_manager.get_recommended_charts(current_data, data_features)
            
            # æ˜¾ç¤ºå›¾è¡¨
            if recommended_charts:
                st.success(f"å·²ç”Ÿæˆ {len(recommended_charts)} ä¸ªæ¨èå›¾è¡¨")
                
                # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€ä¾›æŠ¥å‘Šä½¿ç”¨
                st.session_state.recommended_charts = recommended_charts
                
                # å±•ç¤ºå›¾è¡¨
                for chart_name, fig in recommended_charts.items():
                    with st.expander(f"ğŸ“ˆ {chart_name}"):
                        safe_display_figure(fig)
            else:
                st.warning("æ— æ³•ç”Ÿæˆæ¨èå›¾è¡¨ï¼Œè¯·å°è¯•è‡ªå®šä¹‰å¯è§†åŒ–")
    
    # è‡ªå®šä¹‰å¯è§†åŒ–
    elif viz_option == "è‡ªå®šä¹‰å¯è§†åŒ–":
        st.write("#### ğŸ›ï¸ è‡ªå®šä¹‰å›¾è¡¨")
        
        chart_type = st.selectbox(
            "é€‰æ‹©å›¾è¡¨ç±»å‹",
            ["æŸ±çŠ¶å›¾", "æŠ˜çº¿å›¾", "æ•£ç‚¹å›¾", "ç›´æ–¹å›¾", "ç®±çº¿å›¾", "é¥¼å›¾", "çƒ­åŠ›å›¾"]
        )
        
        # è·å–åˆ—ç±»å‹
        numeric_columns = list(current_data.select_dtypes(include=['number']).columns)
        categorical_columns = list(current_data.select_dtypes(include=['object', 'category']).columns)
        all_columns = numeric_columns + categorical_columns
        
        # æ ¹æ®å›¾è¡¨ç±»å‹æ˜¾ç¤ºä¸åŒçš„é€‰é¡¹
        if chart_type == "æŸ±çŠ¶å›¾":
            if categorical_columns and numeric_columns:
                x_col = st.selectbox("é€‰æ‹©Xè½´ï¼ˆåˆ†ç±»å˜é‡ï¼‰", categorical_columns)
                y_col = st.selectbox("é€‰æ‹©Yè½´ï¼ˆæ•°å€¼å˜é‡ï¼‰", numeric_columns)
                
                # èšåˆæ–¹æ³•
                agg_method = st.selectbox("èšåˆæ–¹æ³•", ["å‡å€¼", "æ€»å’Œ", "è®¡æ•°"])
                
                # æ’åºæ–¹å¼
                sort_by = st.selectbox("æ’åºæ–¹å¼", ["é»˜è®¤é¡ºåº", "Yè½´å€¼å‡åº", "Yè½´å€¼é™åº"])
                
                if st.button("ç”ŸæˆæŸ±çŠ¶å›¾"):
                    # æ•°æ®èšåˆ
                    if agg_method == "å‡å€¼":
                        agg_data = current_data.groupby(x_col)[y_col].mean().reset_index()
                    elif agg_method == "æ€»å’Œ":
                        agg_data = current_data.groupby(x_col)[y_col].sum().reset_index()
                    else:
                        agg_data = current_data.groupby(x_col).size().reset_index(name=y_col)
                    
                    # æ’åº
                    if sort_by == "Yè½´å€¼å‡åº":
                        agg_data = agg_data.sort_values(y_col)
                    elif sort_by == "Yè½´å€¼é™åº":
                        agg_data = agg_data.sort_values(y_col, ascending=False)
                    
                    # åˆ›å»ºå›¾è¡¨
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_bar_chart(
                        agg_data, x_col, y_col, 
                        title=f"{y_col} by {x_col} ({agg_method})")
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["æŸ±çŠ¶å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸€ä¸ªåˆ†ç±»åˆ—å’Œä¸€ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºæŸ±çŠ¶å›¾")
        
        elif chart_type == "æŠ˜çº¿å›¾":
            if len(numeric_columns) >= 2:
                x_col = st.selectbox("é€‰æ‹©Xè½´", numeric_columns)
                y_cols = st.multiselect("é€‰æ‹©Yè½´ï¼ˆå¯å¤šé€‰ï¼‰", numeric_columns, default=[numeric_columns[0]])
                
                if st.button("ç”ŸæˆæŠ˜çº¿å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_line_chart(
                        current_data, x_col, y_cols,
                        title=f"æŠ˜çº¿å›¾: {', '.join(y_cols)} vs {x_col}")
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["æŠ˜çº¿å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºæŠ˜çº¿å›¾")
        
        elif chart_type == "æ•£ç‚¹å›¾":
            if len(numeric_columns) >= 2:
                x_col = st.selectbox("é€‰æ‹©Xè½´", numeric_columns)
                y_col = st.selectbox("é€‰æ‹©Yè½´", numeric_columns)
                
                # å¯é€‰å‚æ•°
                show_trendline = st.checkbox("æ˜¾ç¤ºè¶‹åŠ¿çº¿", value=False)
                
                hue_col = None
                if categorical_columns:
                    if st.checkbox("æŒ‰åˆ†ç±»ç€è‰²", value=False):
                        hue_col = st.selectbox("é€‰æ‹©åˆ†ç±»åˆ—", categorical_columns)
                
                if st.button("ç”Ÿæˆæ•£ç‚¹å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_scatter_plot(
                        current_data, x_col, y_col,
                        title=f"æ•£ç‚¹å›¾: {y_col} vs {x_col}",
                        trendline=show_trendline,
                        hue=hue_col)
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["æ•£ç‚¹å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºæ•£ç‚¹å›¾")
        
        elif chart_type == "ç›´æ–¹å›¾":
            if numeric_columns:
                x_col = st.selectbox("é€‰æ‹©è¦åˆ†æçš„åˆ—", numeric_columns)
                bins = st.slider("ç›´æ–¹å›¾æŸ±æ•°", min_value=5, max_value=100, value=30)
                show_kde = st.checkbox("æ˜¾ç¤ºå¯†åº¦æ›²çº¿", value=True)
                
                if st.button("ç”Ÿæˆç›´æ–¹å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_histogram(
                        current_data, x_col, bins=bins, kde=show_kde,
                        title=f"{x_col}çš„åˆ†å¸ƒ")
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["ç›´æ–¹å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºç›´æ–¹å›¾")
        
        elif chart_type == "ç®±çº¿å›¾":
            if numeric_columns:
                y_col = st.selectbox("é€‰æ‹©æ•°å€¼åˆ—", numeric_columns)
                
                x_col = None
                if categorical_columns:
                    if st.checkbox("æŒ‰åˆ†ç±»åˆ†ç»„", value=False):
                        x_col = st.selectbox("é€‰æ‹©åˆ†ç±»åˆ—", categorical_columns)
                
                if st.button("ç”Ÿæˆç®±çº¿å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_box_plot(
                        current_data, x_col, y_col,
                        title=f"{y_col}çš„ç®±çº¿å›¾" + (f" by {x_col}" if x_col else ""))
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["ç®±çº¿å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºç®±çº¿å›¾")
        
        elif chart_type == "é¥¼å›¾":
            if categorical_columns:
                category_col = st.selectbox("é€‰æ‹©åˆ†ç±»åˆ—", categorical_columns)
                
                # å¯é€‰å‚æ•°
                top_n = st.slider("æ˜¾ç¤ºå‰Nä¸ªç±»åˆ«", min_value=1, max_value=20, value=10)
                
                if st.button("ç”Ÿæˆé¥¼å›¾"):
                    # è®¡ç®—é¢‘ç‡
                    freq_data = current_data[category_col].value_counts().reset_index()
                    freq_data.columns = [category_col, 'count']
                    
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_pie_chart(
                        freq_data, 'count', category_col,
                        title=f"{category_col}çš„åˆ†å¸ƒ",
                        top_n=top_n)
                    
                    safe_display_figure(fig)
                    
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    if 'custom_charts' not in st.session_state:
                        st.session_state.custom_charts = {}
                    st.session_state.custom_charts["é¥¼å›¾"] = fig
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸€ä¸ªåˆ†ç±»åˆ—æ¥åˆ›å»ºé¥¼å›¾")
        
        elif chart_type == "çƒ­åŠ›å›¾":
            if len(numeric_columns) >= 2:
                selected_cols = st.multiselect(
                    "é€‰æ‹©è¦åŒ…å«åœ¨çƒ­åŠ›å›¾ä¸­çš„åˆ—",
                    numeric_columns,
                    default=numeric_columns[:min(5, len(numeric_columns))]
                )
                
                if len(selected_cols) >= 2:
                    if st.button("ç”Ÿæˆçƒ­åŠ›å›¾"):
                        subset_data = current_data[selected_cols]
                        
                        visualizer = viz_manager.visualizer
                        fig = visualizer.create_heatmap(
                            subset_data,
                            title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾")
                        
                        safe_display_figure(fig)
                        
                        # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                        if 'custom_charts' not in st.session_state:
                            st.session_state.custom_charts = {}
                        st.session_state.custom_charts["çƒ­åŠ›å›¾"] = fig
                else:
                    st.warning("éœ€è¦è‡³å°‘é€‰æ‹©ä¸¤ä¸ªåˆ—")
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªæ•°å€¼åˆ—æ¥åˆ›å»ºçƒ­åŠ›å›¾")
    
    # äº¤äº’å¼å›¾è¡¨
    elif viz_option == "äº¤äº’å¼å›¾è¡¨":
        st.write("#### ğŸ”„ äº¤äº’å¼å›¾è¡¨")
        st.info("äº¤äº’å¼å›¾è¡¨æ”¯æŒç¼©æ”¾ã€æ‚¬åœæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ç­‰åŠŸèƒ½")
        
        # è·å–åˆ—ç±»å‹
        numeric_columns = list(current_data.select_dtypes(include=['number']).columns)
        categorical_columns = list(current_data.select_dtypes(include=['object', 'category']).columns)
        
        interactive_type = st.selectbox(
            "é€‰æ‹©äº¤äº’å¼å›¾è¡¨ç±»å‹",
            ["äº¤äº’å¼æ•£ç‚¹å›¾", "äº¤äº’å¼ç›´æ–¹å›¾"]
        )
        
        if interactive_type == "äº¤äº’å¼æ•£ç‚¹å›¾":
            if len(numeric_columns) >= 2:
                x_col = st.selectbox("é€‰æ‹©Xè½´", numeric_columns)
                y_col = st.selectbox("é€‰æ‹©Yè½´", numeric_columns)
                
                # å¯é€‰å‚æ•°
                color_col = None
                if categorical_columns:
                    if st.checkbox("æŒ‰åˆ†ç±»ç€è‰²", value=False):
                        color_col = st.selectbox("é€‰æ‹©é¢œè‰²åˆ—", categorical_columns)
                
                if st.button("ç”Ÿæˆäº¤äº’å¼æ•£ç‚¹å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_interactive_scatter(
                        current_data, x_col, y_col,
                        title=f"äº¤äº’å¼æ•£ç‚¹å›¾: {y_col} vs {x_col}",
                        color=color_col
                    )
                    
                    st.plotly_chart(fig)
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸¤ä¸ªæ•°å€¼åˆ—")
        
        elif interactive_type == "äº¤äº’å¼ç›´æ–¹å›¾":
            if numeric_columns:
                x_col = st.selectbox("é€‰æ‹©è¦åˆ†æçš„åˆ—", numeric_columns)
                
                color_col = None
                if categorical_columns:
                    if st.checkbox("æŒ‰åˆ†ç±»ç€è‰²", value=False):
                        color_col = st.selectbox("é€‰æ‹©é¢œè‰²åˆ—", categorical_columns)
                
                if st.button("ç”Ÿæˆäº¤äº’å¼ç›´æ–¹å›¾"):
                    visualizer = viz_manager.visualizer
                    fig = visualizer.create_interactive_histogram(
                        current_data, x_col,
                        title=f"äº¤äº’å¼ç›´æ–¹å›¾: {x_col}çš„åˆ†å¸ƒ",
                        color=color_col
                    )
                    
                    st.plotly_chart(fig)
            else:
                st.warning("éœ€è¦è‡³å°‘ä¸€ä¸ªæ•°å€¼åˆ—")
    
    # å¯¼èˆªæŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("âª è¿”å›æ•°æ®åˆ†æ", use_container_width=True):
            st.session_state.current_step = "analyze"
            st.rerun()
    
    with col2:
        if st.button("â© ç”ŸæˆæŠ¥å‘Š", use_container_width=True):
            st.session_state.current_step = "report"
            st.rerun()


def report_section():
    """æŠ¥å‘Šç”Ÿæˆéƒ¨åˆ†"""
    st.subheader("ğŸ“„ æŠ¥å‘Šç”Ÿæˆ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if st.session_state.data is None:
        st.error("è¯·å…ˆä¸Šä¼ æ•°æ®!")
        if st.button("è¿”å›ä¸Šä¼ "):
            st.session_state.current_step = 'upload'
            st.rerun()
        return
    
    # ä½¿ç”¨å¤„ç†åçš„æ•°æ®æˆ–åŸå§‹æ•°æ®
    current_data = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    
    # æŠ¥å‘Šè®¾ç½®
    st.subheader("æŠ¥å‘Šè®¾ç½®")
    
    # åŸºæœ¬ä¿¡æ¯è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        report_title = st.text_input("æŠ¥å‘Šæ ‡é¢˜", value=f"æ•°æ®åˆ†ææŠ¥å‘Š - {st.session_state.file_name if hasattr(st.session_state, 'file_name') and st.session_state.file_name else 'æ•°æ®é›†'}")
    with col2:
        report_author = st.text_input("æŠ¥å‘Šä½œè€…", value="AIæ•°æ®åˆ†æç³»ç»Ÿ")
    
    # æŠ¥å‘Šæ¨¡æ¿é€‰é¡¹ï¼ˆæ–°å¢ï¼‰
    st.subheader("ğŸ“‹ æŠ¥å‘Šæ¨¡æ¿é€‰é¡¹")
    include_template = st.checkbox("åŒ…å«æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿", value=True, 
                                  help="åœ¨æŠ¥å‘Šå¼€å¤´æ·»åŠ æ ‡å‡†åŒ–çš„æŠ¥å‘Šæ¨¡æ¿ç¤ºä¾‹ï¼Œå±•ç¤ºä¸“ä¸šçš„æŠ¥å‘Šç»“æ„")
    
    # é—®å·æ•°æ®æ™ºèƒ½æ•´åˆï¼ˆæ–°å¢ï¼‰
    st.subheader("ğŸ§  é—®å·æ•°æ®æ™ºèƒ½æ•´åˆ")
    enable_smart_merge = st.checkbox("å¯ç”¨é—®å·æ•°æ®æ™ºèƒ½åˆ†æ", value=True,
                                   help="ç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«é—®å·é¢˜é¡¹ç±»å‹ï¼Œæ£€æµ‹é‡è¡¨ç»“æ„ï¼Œå¹¶ç”Ÿæˆä¸“é—¨çš„é—®å·åˆ†æç« èŠ‚")
    
    # é¢˜é¡¹æ˜ å°„è®¾ç½®
    item_mapping = None
    if enable_smart_merge:
        st.write("**é¢˜é¡¹æ˜ å°„è®¾ç½®**")
        mapping_option = st.radio(
            "é€‰æ‹©é¢˜é¡¹æ˜ å°„æ–¹å¼ï¼š",
            ["è‡ªåŠ¨æ™ºèƒ½è¯†åˆ«", "æ‰‹åŠ¨æŒ‡å®šæ˜ å°„", "ä¸Šä¼ é¢˜é¡¹å¯¹ç…§è¡¨"],
            help="è‡ªåŠ¨è¯†åˆ«ï¼šç³»ç»Ÿæ™ºèƒ½æ¨æ–­åˆ—åå«ä¹‰ï¼›æ‰‹åŠ¨æŒ‡å®šï¼šè‡ªå®šä¹‰é¢˜é¡¹æè¿°ï¼›ä¸Šä¼ å¯¹ç…§è¡¨ï¼šä½¿ç”¨å¤–éƒ¨æ˜ å°„æ–‡ä»¶"
        )
        
        if mapping_option == "æ‰‹åŠ¨æŒ‡å®šæ˜ å°„":
            st.write("ä¸ºæ•°æ®åˆ—æŒ‡å®šé¢˜é¡¹æè¿°ï¼š")
            item_mapping = {}
            
            # åªæ˜¾ç¤ºå‰10ä¸ªåˆ—ä»¥é¿å…ç•Œé¢è¿‡äºå¤æ‚
            columns_to_show = current_data.columns[:10].tolist()
            if len(current_data.columns) > 10:
                st.info(f"æ˜¾ç¤ºå‰10ä¸ªåˆ—çš„æ˜ å°„è®¾ç½®ï¼Œå…±æœ‰ {len(current_data.columns)} åˆ—")
            
            for i, col in enumerate(columns_to_show):
                col_key = f"item_mapping_{i}"
                description = st.text_input(f"åˆ— '{col}' çš„é¢˜é¡¹æè¿°ï¼š", 
                                          value=f"é¢˜é¡¹: {col}", 
                                          key=col_key)
                if description.strip():
                    item_mapping[col] = description.strip()
        
        elif mapping_option == "ä¸Šä¼ é¢˜é¡¹å¯¹ç…§è¡¨":
            st.write("ä¸Šä¼ CSVæ ¼å¼çš„é¢˜é¡¹å¯¹ç…§è¡¨ï¼ŒåŒ…å«'åˆ—å'å’Œ'é¢˜é¡¹æè¿°'ä¸¤åˆ—ï¼š")
            mapping_file = st.file_uploader("é€‰æ‹©é¢˜é¡¹å¯¹ç…§è¡¨æ–‡ä»¶", type=['csv'], key="mapping_file")
            
            if mapping_file is not None:
                try:
                    import io
                    mapping_df = pd.read_csv(io.StringIO(mapping_file.getvalue().decode("utf-8")))
                    
                    if 'åˆ—å' in mapping_df.columns and 'é¢˜é¡¹æè¿°' in mapping_df.columns:
                        item_mapping = dict(zip(mapping_df['åˆ—å'], mapping_df['é¢˜é¡¹æè¿°']))
                        st.success(f"æˆåŠŸåŠ è½½ {len(item_mapping)} ä¸ªé¢˜é¡¹æ˜ å°„")
                        
                        # æ˜¾ç¤ºæ˜ å°„é¢„è§ˆ
                        st.write("**æ˜ å°„é¢„è§ˆï¼š**")
                        preview_df = mapping_df.head(5)
                        st.dataframe(preview_df, use_container_width=True)
                    else:
                        st.error("å¯¹ç…§è¡¨å¿…é¡»åŒ…å«'åˆ—å'å’Œ'é¢˜é¡¹æè¿°'ä¸¤åˆ—")
                except Exception as e:
                    st.error(f"è¯»å–é¢˜é¡¹å¯¹ç…§è¡¨å¤±è´¥ï¼š{str(e)}")
    
    # æŠ¥å‘Šå†…å®¹é€‰æ‹©
    st.subheader("æŠ¥å‘Šå†…å®¹é€‰æ‹©")
    
    # é€‰æ‹©è¦åŒ…å«çš„å†…å®¹éƒ¨åˆ†
    include_executive_summary = st.checkbox("æ‰§è¡Œæ‘˜è¦", value=True)
    include_data_overview = st.checkbox("æ•°æ®æ¦‚è§ˆ", value=True)
    include_preprocessing = st.checkbox("æ•°æ®é¢„å¤„ç†", value=True)
    include_analysis_results = st.checkbox("åˆ†æç»“æœ", value=True)
    include_visualizations = st.checkbox("æ•°æ®å¯è§†åŒ–", value=True)
    include_conclusion = st.checkbox("ç»“è®ºä¸å»ºè®®", value=True)
    
    # é«˜çº§åˆ†æç»“æœé€‰é¡¹ï¼ˆæ–°å¢ï¼‰
    if include_analysis_results:
        st.write("**åŒ…å«çš„åˆ†æç±»å‹ï¼š**")
        col1, col2 = st.columns(2)
        with col1:
            include_descriptive = st.checkbox("æè¿°æ€§ç»Ÿè®¡", value=True)
            include_correlation = st.checkbox("ç›¸å…³æ€§åˆ†æ", value=True)
            include_cluster = st.checkbox("èšç±»åˆ†æ", value=True)
        with col2:
            include_factor = st.checkbox("å› å­åˆ†æ", value=True)
            include_anova = st.checkbox("æ–¹å·®åˆ†æ", value=True)
            include_models = st.checkbox("æ¨¡å‹æ¨è", value=True)
    
    # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
    custom_output_path = st.checkbox("è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„")
    output_path = None
    if custom_output_path:
        default_path = os.path.join(os.path.expanduser("~"), "Desktop")
        output_path = st.text_input("è¾“å‡ºè·¯å¾„", value=default_path)
    
    # ç”ŸæˆæŠ¥å‘ŠæŒ‰é’®
    if st.button("ğŸš€ ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š", use_container_width=True):
        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š..."):
                # åˆ›å»ºAIå¢å¼ºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                ai_enhancer = None
                if (st.session_state.ai_enhancement_enabled and 
                    AI_ENHANCEMENT_AVAILABLE and
                    (st.session_state.ai_api_key or st.session_state.ai_provider == "local")):
                    
                    try:
                        config = AIModelConfig(
                            provider=st.session_state.ai_provider,
                            model_name=st.session_state.ai_model,
                            api_key=st.session_state.ai_api_key if st.session_state.ai_api_key else None,
                            api_base=st.session_state.ai_api_base if st.session_state.ai_api_base else None
                        )
                        ai_enhancer = AIReportEnhancer(config)
                        st.info("ğŸ¤– AIæŠ¥å‘Šå¢å¼ºå·²å¯ç”¨")
                    except Exception as e:
                        st.warning(f"âš ï¸ AIå¢å¼ºå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨æ™®é€šæŠ¥å‘Šç”Ÿæˆ")
                        ai_enhancer = None
                
                # åˆ›å»ºé«˜çº§æŠ¥å‘Šç”Ÿæˆå™¨ï¼ˆå¸¦AIå¢å¼ºï¼‰
                report_gen = create_advanced_report_generator()
                if ai_enhancer:
                    report_gen.set_ai_enhancer(ai_enhancer)
                
                # å‡†å¤‡åˆ†æç»“æœ
                analysis_results = st.session_state.analysis_results.copy() if hasattr(st.session_state, 'analysis_results') else {}
                
                # è¿‡æ»¤åˆ†æç»“æœï¼ˆæ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼‰
                if include_analysis_results:
                    filtered_results = {}
                    if include_descriptive and 'descriptive_stats' in analysis_results:
                        filtered_results['descriptive_stats'] = analysis_results['descriptive_stats']
                    if include_correlation and 'correlation' in analysis_results:
                        filtered_results['correlation'] = analysis_results['correlation']
                    if include_cluster and 'cluster_analysis' in analysis_results:
                        filtered_results['cluster_analysis'] = analysis_results['cluster_analysis']
                    if include_factor and 'factor_analysis' in analysis_results:
                        filtered_results['factor_analysis'] = analysis_results['factor_analysis']
                    if include_anova and 'anova_analysis' in analysis_results:
                        filtered_results['anova_analysis'] = analysis_results['anova_analysis']
                    if include_models and 'model_recommendations' in analysis_results:
                        filtered_results['model_recommendations'] = analysis_results['model_recommendations']
                    
                    analysis_results = filtered_results
                
                # æ·»åŠ é¢„å¤„ç†ä¿¡æ¯
                if hasattr(st.session_state, 'preprocessing_info') and st.session_state.preprocessing_info:
                    analysis_results['preprocessing'] = st.session_state.preprocessing_info
                
                # å‡†å¤‡å¯è§†åŒ–å›¾è¡¨
                charts = {}
                if include_visualizations:
                    # æ”¶é›†æ‰€æœ‰å¯ç”¨å›¾è¡¨
                    if hasattr(st.session_state, 'recommended_charts') and st.session_state.recommended_charts:
                        charts.update(st.session_state.recommended_charts)
                    if hasattr(st.session_state, 'custom_charts') and st.session_state.custom_charts:
                        charts.update(st.session_state.custom_charts)
                
                # å¦‚æœé€‰æ‹©äº†è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
                if custom_output_path and output_path:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(output_path, exist_ok=True)
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f"{report_title.replace(' ', '_')}_{timestamp}.docx"
                    full_output_path = os.path.join(output_path, filename)
                else:
                    full_output_path = None
                
                # ç”ŸæˆæŠ¥å‘Š
                # å®‰å…¨åœ°ä¼ é€’chartså‚æ•°ï¼Œç¡®ä¿å³ä½¿ä¸ºç©ºä¹Ÿä¸ä¼šå¯¼è‡´NoneTypeè¿­ä»£é”™è¯¯
                charts_to_use = charts if include_visualizations and charts else {}
                saved_path = report_gen.generate_full_report(
                    data=current_data,
                    analysis_results=analysis_results,
                    charts=charts_to_use,
                    file_info={
                        'file_name': st.session_state.file_name if hasattr(st.session_state, 'file_name') else 'æœªçŸ¥',
                        'file_format': st.session_state.file_format if hasattr(st.session_state, 'file_format') else 'æœªçŸ¥'
                    },
                    output_path=full_output_path,
                    include_template=include_template,
                    item_mapping=item_mapping if enable_smart_merge else None
                )
                
                # ä¿å­˜æŠ¥å‘Šè·¯å¾„åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.report_path = saved_path
                
                # æ›´æ–°è¿›åº¦
                st.session_state.progress = 100
                
                st.success(f"æ™ºèƒ½åˆ†ææŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³ï¼š{saved_path}")
                
                # æ˜¾ç¤ºæŠ¥å‘Šä¿¡æ¯
                st.info("æŠ¥å‘Šç‰¹è‰²åŠŸèƒ½ï¼š")
                features = []
                if include_template:
                    features.append("âœ… åŒ…å«ä¸“ä¸šæŠ¥å‘Šæ¨¡æ¿ç¤ºä¾‹")
                if enable_smart_merge:
                    features.append("âœ… æ™ºèƒ½é—®å·æ•°æ®åˆ†æ")
                if item_mapping:
                    features.append(f"âœ… è‡ªå®šä¹‰é¢˜é¡¹æ˜ å°„ ({len(item_mapping)} ä¸ªé¢˜é¡¹)")
                if len(analysis_results) > 2:
                    features.append(f"âœ… å®Œæ•´ç»Ÿè®¡åˆ†æ ({len(analysis_results)} ç§åˆ†æç±»å‹)")
                if charts_to_use:
                    features.append(f"âœ… æ•°æ®å¯è§†åŒ–å›¾è¡¨ ({len(charts_to_use)} ä¸ªå›¾è¡¨)")
                
                for feature in features:
                    st.write(feature)
                
        except Exception as e:
            st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}")
            logger.exception("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
    
    # æ˜¾ç¤ºå·²ç”Ÿæˆçš„æŠ¥å‘Šä¿¡æ¯
    if hasattr(st.session_state, 'report_path') and st.session_state.report_path:
        st.subheader("æŠ¥å‘Šå·²ç”Ÿæˆ")
        st.info(f"æŠ¥å‘Šè·¯å¾„: {st.session_state.report_path}")
        
        # æä¾›ä¸‹è½½é“¾æ¥ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            with open(st.session_state.report_path, "rb") as file:
                st.download_button(
                    label="ä¸‹è½½æŠ¥å‘Š",
                    data=file,
                    file_name=os.path.basename(st.session_state.report_path),
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    use_container_width=True
                )
        except:
            st.info("è¯·åœ¨æ–‡ä»¶èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€æŠ¥å‘Šæ–‡ä»¶")
    
    # å¯¼èˆªæŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â¬…ï¸ è¿”å›æ•°æ®å¯è§†åŒ–", use_container_width=True):
            st.session_state.current_step = 'visualize'
            st.rerun()
    with col2:
        if st.button("ğŸ”„ é‡æ–°å¼€å§‹", use_container_width=True):
            # é‡ç½®æ‰€æœ‰ä¼šè¯çŠ¶æ€
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.session_state.current_step = 'upload'
            st.rerun()


def show_progress():
    """
    æ˜¾ç¤ºè¿›åº¦æ¡å’ŒçŠ¶æ€ä¿¡æ¯
    """
    with st.sidebar:
        # æ ‡é¢˜å’Œè¯´æ˜
        st.markdown("### ğŸ“Š å¤„ç†è¿›åº¦è·Ÿè¸ª")
        st.markdown("---")
        
        # æ€»è¿›åº¦æ¡
        st.markdown("**æ€»ä½“è¿›åº¦**")
        col1, col2 = st.columns([3, 1])
        with col1:
            st.progress(st.session_state.progress)
        with col2:
            st.markdown(f"**{st.session_state.progress}%**")
        
        st.markdown("---")
        
        # æ­¥éª¤è¿›åº¦
        st.markdown("**æ­¥éª¤è¿›åº¦**")
        
        # å®šä¹‰æ­¥éª¤ä¿¡æ¯
        steps = [
            {'id': 'upload', 'name': 'æ•°æ®ä¸Šä¼ ', 'icon': 'ğŸ“', 'status': 'completed' if st.session_state.current_step != 'upload' else 'active'},
            {'id': 'analyze', 'name': 'æ•°æ®åˆ†æ', 'icon': 'ğŸ”', 'status': 'completed' if st.session_state.current_step not in ['upload', 'analyze'] else 'active' if st.session_state.current_step == 'analyze' else 'pending'},
            {'id': 'visualize', 'name': 'æ•°æ®å¯è§†åŒ–', 'icon': 'ğŸ“Š', 'status': 'completed' if st.session_state.current_step == 'report' else 'active' if st.session_state.current_step == 'visualize' else 'pending'},
            {'id': 'report', 'name': 'æŠ¥å‘Šç”Ÿæˆ', 'icon': 'ğŸ“‘', 'status': 'active' if st.session_state.current_step == 'report' else 'pending'}
        ]
        
        # æ˜¾ç¤ºæ­¥éª¤è¿›åº¦
        for i, step in enumerate(steps):
            # çŠ¶æ€å›¾æ ‡
            status_icon = {
                'completed': 'âœ…',
                'active': 'ğŸ”„',
                'pending': 'â³'
            }.get(step['status'], 'â³')
            
            # çŠ¶æ€æ–‡æœ¬é¢œè‰²
            status_color = {
                'completed': '#28a745',  # ç»¿è‰²
                'active': '#007bff',    # è“è‰²
                'pending': '#6c757d'    # ç°è‰²
            }.get(step['status'], '#6c757d')
            
            # æ˜¾ç¤ºæ­¥éª¤
            st.markdown(
                f"<div style='display: flex; align-items: center; margin-bottom: 8px;'>"  
                f"  <span style='font-size: 16px; margin-right: 8px;'>{status_icon}</span>"  
                f"  <span style='font-weight: bold; color: {status_color};'>{step['icon']} {step['name']}</span>"  
                f"</div>",
                unsafe_allow_html=True
            )
            
            # æ·»åŠ è¿æ¥çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªæ­¥éª¤ï¼‰
            if i < len(steps) - 1:
                st.markdown("<div style='margin-left: 8px; height: 12px; border-left: 2px dashed #ddd;'></div>", unsafe_allow_html=True)
        
        st.markdown("---")
        
        # çŠ¶æ€ä¿¡æ¯é¢æ¿
        st.markdown("**çŠ¶æ€ä¿¡æ¯**")
        status_panel = st.empty()
        
        # æ ¹æ®å½“å‰æ­¥éª¤æ˜¾ç¤ºä¸åŒçš„çŠ¶æ€ä¿¡æ¯
        status_info = []
        
        # æ•°æ®çŠ¶æ€
        if st.session_state.data is not None:
            data_status = "âœ… å·²åŠ è½½"
            data_details = f"ğŸ“‹ {len(st.session_state.data)} è¡Œ Ã— {len(st.session_state.data.columns)} åˆ—"
        else:
            data_status = "â³ æœªåŠ è½½"
            data_details = "ğŸ“‹ è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶"
        
        status_info.append(f"**æ•°æ®çŠ¶æ€**: {data_status}")
        status_info.append(f"{data_details}")
        
        # å¤„ç†çŠ¶æ€
        if st.session_state.processed_data is not None:
            process_status = "âœ… å·²å¤„ç†"
        elif st.session_state.data is not None:
            process_status = "â³ å¾…å¤„ç†"
        else:
            process_status = "ğŸ”’ æœªå¼€å§‹"
        
        status_info.append(f"**å¤„ç†çŠ¶æ€**: {process_status}")
        
        # å¯è§†åŒ–çŠ¶æ€
        viz_count = 0
        if hasattr(st.session_state, 'recommended_charts') and st.session_state.recommended_charts:
            viz_count += len(st.session_state.recommended_charts)
        if hasattr(st.session_state, 'custom_charts') and st.session_state.custom_charts:
            viz_count += len(st.session_state.custom_charts)
            
        if viz_count > 0:
            viz_status = f"âœ… å·²ç”Ÿæˆ ({viz_count} ä¸ªå›¾è¡¨)"
        elif st.session_state.data is not None:
            viz_status = "â³ å¾…ç”Ÿæˆ"
        else:
            viz_status = "ğŸ”’ æœªå¼€å§‹"
        
        status_info.append(f"**å¯è§†åŒ–çŠ¶æ€**: {viz_status}")
        
        # æŠ¥å‘ŠçŠ¶æ€
        if hasattr(st.session_state, 'report_path') and st.session_state.report_path:
            report_status = "âœ… å·²ç”Ÿæˆ"
        elif st.session_state.data is not None:
            report_status = "â³ å¾…ç”Ÿæˆ"
        else:
            report_status = "ğŸ”’ æœªå¼€å§‹"
        
        status_info.append(f"**æŠ¥å‘ŠçŠ¶æ€**: {report_status}")
        
        # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
        status_panel.markdown("\n".join(status_info))
        
        st.markdown("---")
        
        # ç³»ç»Ÿä¿¡æ¯
        st.markdown("**ç³»ç»Ÿä¿¡æ¯**")
        st.markdown("ğŸ¤– AIæ™ºèƒ½æ•°æ®åˆ†æç³»ç»Ÿ v1.0")
        st.markdown("ğŸ“… å®æ—¶å¤„ç†ä¸åˆ†æ")


def ai_assistant_section():
    """
    AIæ™ºèƒ½åŠ©æ‰‹éƒ¨åˆ†
    """
    if not st.session_state.show_ai_assistant:
        return
    
    with st.sidebar:
        st.markdown("## ğŸ’¬ AIæ™ºèƒ½åŠ©æ‰‹")
        st.markdown("æœ‰ä»»ä½•æ•°æ®åˆ†æé—®é¢˜ï¼Œéšæ—¶å‘æˆ‘æé—®ï¼")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        chat_container = st.container(height=300)
        
        # æ˜¾ç¤ºå†å²å¯¹è¯
        with chat_container:
            if st.session_state.conversation_history:
                for message in st.session_state.conversation_history:
                    if message['role'] == 'user':
                        st.markdown(f"**ğŸ‘¤ æ‚¨:** {message['content']}")
                    else:
                        st.markdown(f"**ğŸ¤– AI:** {message['content']}")
                        st.markdown("---")
            else:
                st.markdown("**ğŸ¤– AI:** æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ")
        
        # ç”¨æˆ·è¾“å…¥
        user_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤:", placeholder="ä¾‹å¦‚ï¼šè§£é‡Šä»€ä¹ˆæ˜¯ç›¸å…³æ€§åˆ†æï¼Ÿ")
        
        if st.button("å‘é€", use_container_width=True):
            if user_query.strip():
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
                st.session_state.conversation_history.append({
                    'role': 'user',
                    'content': user_query.strip()
                })
                
                # ç”ŸæˆAIå“åº”
                with st.spinner("AIæ­£åœ¨æ€è€ƒ..."):
                    # è·å–ç›¸å…³æ•°æ®å’Œåˆ†æç»“æœ
                    data = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
                    analysis_results = st.session_state.analysis_results
                    
                    # ç”Ÿæˆå“åº”
                    response = st.session_state.ai_assistant.generate_response(
                        user_query.strip(),
                        data=data,
                        analysis_results=analysis_results
                    )
                    
                    # æ·»åŠ AIå›å¤åˆ°å†å²
                    st.session_state.conversation_history.append({
                        'role': 'assistant',
                        'content': response
                    })
                
                # é™åˆ¶å¯¹è¯å†å²é•¿åº¦
                if len(st.session_state.conversation_history) > 10:
                    st.session_state.conversation_history = st.session_state.conversation_history[-10:]
                
                # é‡æ–°è¿è¡Œåº”ç”¨ä»¥æ›´æ–°UI
                st.rerun()
        
        # æ¸…é™¤å¯¹è¯å†å²
        if st.button("æ¸…é™¤å†å²", use_container_width=True, type="secondary"):
            st.session_state.conversation_history = []
            st.rerun()

def ai_enhancement_sidebar():
    """æ˜¾ç¤ºAIå¢å¼ºé…ç½®ä¾§è¾¹æ """
    if not AI_ENHANCEMENT_AVAILABLE:
        return
        
    with st.sidebar:
        st.markdown("### ğŸ¤– AIæŠ¥å‘Šå¢å¼º")
        
        # å¯ç”¨/ç¦ç”¨AIå¢å¼º
        ai_enabled = st.checkbox(
            "å¯ç”¨AIæŠ¥å‘Šå¢å¼º",
            value=st.session_state.ai_enhancement_enabled,
            help="ä½¿ç”¨AIå¤§æ¨¡å‹å¯¹åˆ†æç»“æœè¿›è¡Œæ·±åº¦è§£è¯»å’Œæ´å¯Ÿ"
        )
        st.session_state.ai_enhancement_enabled = ai_enabled
        
        if ai_enabled:
            # AIæä¾›å•†é€‰æ‹©
            ai_provider = st.selectbox(
                "AIæä¾›å•†",
                options=["openai", "qwen", "chatglm", "local"],
                index=["openai", "qwen", "chatglm", "local"].index(st.session_state.ai_provider),
                help="é€‰æ‹©AIæ¨¡å‹æä¾›å•†"
            )
            st.session_state.ai_provider = ai_provider
            
            # æ¨¡å‹é€‰æ‹©
            if ai_provider == "openai":
                model_options = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
                default_model = "gpt-3.5-turbo"
            elif ai_provider == "qwen":
                model_options = ["qwen-turbo", "qwen-plus", "qwen-max"]
                default_model = "qwen-turbo"
            elif ai_provider == "chatglm":
                model_options = ["chatglm3-6b", "chatglm4"]
                default_model = "chatglm3-6b"
            else:  # local
                model_options = ["llama2", "llama3", "qwen2", "chatglm3"]
                default_model = "llama2"
            
            ai_model = st.selectbox(
                "AIæ¨¡å‹",
                options=model_options,
                index=model_options.index(st.session_state.ai_model) if st.session_state.ai_model in model_options else 0
            )
            st.session_state.ai_model = ai_model
            
            # APIé…ç½®
            if ai_provider != "local":
                api_key = st.text_input(
                    "APIå¯†é’¥",
                    value=st.session_state.ai_api_key,
                    type="password",
                    help="è¾“å…¥AIæœåŠ¡çš„APIå¯†é’¥"
                )
                st.session_state.ai_api_key = api_key
                
                api_base = st.text_input(
                    "APIåŸºç¡€URL (å¯é€‰)",
                    value=st.session_state.ai_api_base,
                    help="è‡ªå®šä¹‰APIåŸºç¡€URLï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤"
                )
                st.session_state.ai_api_base = api_base
            else:
                api_base = st.text_input(
                    "æœ¬åœ°æ¨¡å‹APIåœ°å€",
                    value=st.session_state.ai_api_base or "http://localhost:11434/api/generate",
                    help="æœ¬åœ°æ¨¡å‹APIåœ°å€ï¼Œå¦‚Ollama"
                )
                st.session_state.ai_api_base = api_base
            
            # å¢å¼ºç±»å‹é€‰æ‹©
            enhancement_type = st.selectbox(
                "å¢å¼ºç±»å‹",
                options=["comprehensive", "insights", "recommendations", "interpretation"],
                index=["comprehensive", "insights", "recommendations", "interpretation"].index(st.session_state.ai_enhancement_type),
                help="é€‰æ‹©AIå¢å¼ºçš„é‡ç‚¹æ–¹å‘"
            )
            st.session_state.ai_enhancement_type = enhancement_type
            
            # å¢å¼ºç±»å‹è¯´æ˜
            enhancement_descriptions = {
                "comprehensive": "ğŸ¯ **ç»¼åˆåˆ†æ**: å…¨é¢çš„æ´å¯Ÿã€å»ºè®®å’Œè§£é‡Š",
                "insights": "ğŸ” **æ·±åº¦æ´å¯Ÿ**: ä¸“æ³¨äºæ•°æ®æ¨¡å¼å’Œè¶‹åŠ¿å‘ç°",
                "recommendations": "ğŸ’¡ **è¡ŒåŠ¨å»ºè®®**: åŸºäºåˆ†æç»“æœæä¾›å…·ä½“å»ºè®®",
                "interpretation": "ğŸ“– **ç»“æœè§£è¯»**: ç”¨é€šä¿—è¯­è¨€è§£é‡Šç»Ÿè®¡ç»“æœ"
            }
            st.markdown(enhancement_descriptions[enhancement_type])
            
            # æµ‹è¯•è¿æ¥æŒ‰é’®
            if st.button("ğŸ”§ æµ‹è¯•AIè¿æ¥", use_container_width=True):
                test_ai_connection()

def test_ai_connection():
    """æµ‹è¯•AIè¿æ¥"""
    try:
        if not st.session_state.ai_api_key and st.session_state.ai_provider != "local":
            st.error("è¯·å…ˆè¾“å…¥APIå¯†é’¥")
            return
            
        with st.spinner("æ­£åœ¨æµ‹è¯•AIè¿æ¥..."):
            # åˆ›å»ºAIå¢å¼ºå™¨
            config = AIModelConfig(
                provider=st.session_state.ai_provider,
                model_name=st.session_state.ai_model,
                api_key=st.session_state.ai_api_key,
                api_base=st.session_state.ai_api_base if st.session_state.ai_api_base else None
            )
            
            enhancer = AIReportEnhancer(config)
            
            # è¿›è¡Œç®€å•æµ‹è¯•
            test_prompt = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'"
            response = enhancer._call_ai_model(test_prompt)
            
            if response:
                st.success(f"âœ… AIè¿æ¥æµ‹è¯•æˆåŠŸï¼\nå›å¤: {response[:100]}...")
            else:
                st.error("âŒ AIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
                
    except Exception as e:
        # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        st.error(f"âŒ AIè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # åœ¨expanderä¸­æ˜¾ç¤ºæ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        with st.expander("ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯ (è°ƒè¯•ç”¨)", expanded=False):
            st.code(f"""
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}
é…ç½®ä¿¡æ¯:
  - æä¾›å•†: {st.session_state.get('ai_provider', 'æœªè®¾ç½®')}
  - æ¨¡å‹: {st.session_state.get('ai_model', 'æœªè®¾ç½®')}
  - APIå¯†é’¥: {'å·²è®¾ç½®' if st.session_state.get('ai_api_key') else 'æœªè®¾ç½®'}
  - APIåœ°å€: {st.session_state.get('ai_api_base', 'é»˜è®¤åœ°å€')}

è°ƒè¯•å»ºè®®:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. ç¡®è®¤APIå¯†é’¥æ˜¯å¦æ­£ç¡®
3. éªŒè¯APIåœ°å€æ˜¯å¦å¯è®¿é—®
4. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
            """, language="text")
            
        # æä¾›å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ
        st.info("""
        **å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ:**
        
        ğŸ”— **è¿æ¥é”™è¯¯ (Connection error)**
        - æ£€æŸ¥ç½‘ç»œè¿æ¥
        - ç¡®è®¤APIåœ°å€æ˜¯å¦æ­£ç¡®
        - æ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç†è®¾ç½®
        
        ğŸ”‘ **è®¤è¯é”™è¯¯ (Authentication failed)**
        - éªŒè¯APIå¯†é’¥æ˜¯å¦æ­£ç¡®
        - æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆæœŸå†…
        - ç¡®è®¤å¯†é’¥æƒé™æ˜¯å¦è¶³å¤Ÿ
        
        â±ï¸ **è¶…æ—¶é”™è¯¯ (Timeout)**
        - æ£€æŸ¥ç½‘ç»œé€Ÿåº¦
        - è€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´
        - å°è¯•æ›´æ¢ç½‘ç»œç¯å¢ƒ
        
        ğŸ“Š **é…é¢é”™è¯¯ (Rate limit/Quota exceeded)**
        - æ£€æŸ¥APIä½¿ç”¨é…é¢
        - ç­‰å¾…é…é¢é‡ç½®
        - å‡çº§APIå¥—é¤
        """)

def run_app():
    """è¿è¡ŒStreamlitåº”ç”¨"""
    try:
        # è®¾ç½®é¡µé¢é…ç½®
        set_page_config()
        
        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        AppState.initialize_session_state()
        
        # æ˜¾ç¤ºè¿›åº¦æ¡å’ŒçŠ¶æ€ä¿¡æ¯
        show_progress()
        
        # æ˜¾ç¤ºAIåŠ©æ‰‹
        ai_assistant_section()
        
        # æ˜¾ç¤ºAIå¢å¼ºé…ç½®
        ai_enhancement_sidebar()
        
        # æ˜¾ç¤ºæ ‡é¢˜
        display_header()
        
        # æ ¹æ®å½“å‰æ­¥éª¤æ˜¾ç¤ºç›¸åº”å†…å®¹
        if st.session_state.current_step == 'upload':
            file_upload_section()
        elif st.session_state.current_step == 'analyze':
            analyze_section()
        elif st.session_state.current_step == 'visualize':
            visualize_section()
        elif st.session_state.current_step == 'report':
            report_section()
        
        # æ˜¾ç¤ºé¡µè„š
        st.markdown("""
        <hr>
        <div style='text-align: center; color: #7f8c8d;'>
            <p>Â© 2025 AIæ•°æ®åˆ†æå¤§æ¨¡å‹ç³»ç»Ÿ | ç‰ˆæœ¬ 1.0.0</p>
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"åº”ç”¨è¿è¡Œå‡ºé”™: {str(e)}")
        logger.error(f"åº”ç”¨è¿è¡Œå‡ºé”™: {str(e)}", exc_info=True)


if __name__ == "__main__":
    run_app()