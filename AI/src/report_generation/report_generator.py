from typing import Optional
import os
import io
import datetime
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_BREAK
from docx.enum.table import WD_TABLE_ALIGNMENT, WD_ALIGN_VERTICAL
from matplotlib.figure import Figure
import tempfile
import matplotlib.pyplot as plt
import seaborn as sns
import logging


# æ¨¡å—çº§æ—¥å¿—å™¨
logger = logging.getLogger (__name__)
logger.addHandler(logging.NullHandler())

# å¯¼å…¥AIå¢å¼ºæ¨¡å—
try:
    from ..ai_agent.ai_report_enhancer import AIReportEnhancer, create_ai_enhancer, DEFAULT_CONFIGS
    AI_ENHANCEMENT_AVAILABLE = True
except ImportError:
    AI_ENHANCEMENT_AVAILABLE = False
    logger.warning("AIæŠ¥å‘Šå¢å¼ºæ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡AIå¢å¼ºåŠŸèƒ½")


class ReportGenerator:
    """
    WordæŠ¥å‘Šç”Ÿæˆå™¨ï¼Œç”¨äºåˆ›å»ºä¸“ä¸šçš„æ•°æ®æŠ¥å‘Šæ–‡æ¡£
    """
    
    def __init__(self):
        self.document = None
        self.temp_dir = tempfile.mkdtemp()
    
    def create_report(self, title: str = "æ•°æ®åˆ†ææŠ¥å‘Š", 
                     author: str = "AIæ•°æ®åˆ†æç³»ç»Ÿ",
                     subtitle: Optional[str] = None) -> None:
        """
        åˆ›å»ºä¸€ä¸ªæ–°çš„æŠ¥å‘Šæ–‡æ¡£
        
        Args:
            title: æŠ¥å‘Šæ ‡é¢˜
            author: æŠ¥å‘Šä½œè€…
            subtitle: æŠ¥å‘Šå‰¯æ ‡é¢˜
        """
        # åˆ›å»ºæ–°æ–‡æ¡£
        self.document = Document()
        
        # è®¾ç½®æ ‡é¢˜
        self._add_title_section(title, subtitle, author)
        
        # æ·»åŠ ç›®å½•
        self._add_table_of_contents()


    def _add_title_section(self, title: str, subtitle: Optional[str], author: str) -> None:
        """
        æ·»åŠ æ ‡é¢˜éƒ¨åˆ†
        """
        # ä¸»æ ‡é¢˜
        title_para = self.document.add_heading(title, 0)
        title_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # å‰¯æ ‡é¢˜
        if subtitle:
            subtitle_para = self.document.add_heading(subtitle, level=1)
            subtitle_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
        
        # æ·»åŠ ä½œè€…å’Œæ—¥æœŸä¿¡æ¯
        info_para = self.document.add_paragraph()
        info_run = info_para.add_run(f"ä½œè€…: {author}")
        info_run.font.size = Pt(12)
        info_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        
        # æ·»åŠ æ—¥æœŸ
        date_para = self.document.add_paragraph()
        date_run = date_para.add_run(f"ç”Ÿæˆæ—¥æœŸ: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}")
        date_run.font.size = Pt(12)
        date_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        
        # åˆ†é¡µç¬¦
        self.document.add_page_break()
    
    def _add_table_of_contents(self) -> None:
        """
        æ·»åŠ ç›®å½•
        """
        # ç›®å½•æ ‡é¢˜
        toc_title = self.document.add_heading("ç›®å½•", level=1)
        toc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ·»åŠ ç›®å½•ï¼ˆå®é™…å†…å®¹ä¼šåœ¨ä¿å­˜å‰æ›´æ–°ï¼‰
        self.document.add_paragraph("[ç›®å½•å°†åœ¨ä¿å­˜æ—¶è‡ªåŠ¨ç”Ÿæˆ]")
        
        # åˆ†é¡µç¬¦
        self.document.add_page_break()
    
    def add_executive_summary(self, summary: str) -> None:
        """
        æ·»åŠ æ‰§è¡Œæ‘˜è¦
        
        Args:
            summary: æ‘˜è¦å†…å®¹
        """
        self.document.add_heading("æ‰§è¡Œæ‘˜è¦", level=1)
        
        # æ·»åŠ æ‘˜è¦æ®µè½
        self.document.add_paragraph(summary)
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
    
    def add_data_overview(self, data_info: Dict) -> None:
        """
        æ·»åŠ æ•°æ®æ¦‚è§ˆéƒ¨åˆ†
        
        Args:
            data_info: æ•°æ®ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«è¡Œæ•°ã€åˆ—æ•°ã€æ–‡ä»¶åç§°ç­‰
        """
        self.document.add_heading("1. æ•°æ®æ¦‚è§ˆ", level=1)
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯
        self.document.add_heading("1.1 æ•°æ®é›†åŸºæœ¬ä¿¡æ¯", level=2)
        
        # åˆ›å»ºä¿¡æ¯è¡¨æ ¼
        info_table = self.document.add_table(rows=1, cols=2)
        info_table.style = 'Table Grid'
        info_table.alignment = WD_TABLE_ALIGNMENT.CENTER
        
        # è¡¨å¤´
        hdr_cells = info_table.rows[0].cells
        hdr_cells[0].text = 'å±æ€§'
        hdr_cells[1].text = 'å€¼'
        
        # è®¾ç½®è¡¨å¤´æ ·å¼
        for cell in hdr_cells:
            cell.paragraphs[0].runs[0].bold = True
            cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
        
        # æ·»åŠ æ•°æ®
        rows = [
            ('æ–‡ä»¶åç§°', str(data_info.get('file_name', 'æœªçŸ¥')) if data_info.get('file_name') is not None else 'æœªçŸ¥'),
            ('æ•°æ®æ ¼å¼', str(data_info.get('file_format', 'æœªçŸ¥')) if data_info.get('file_format') is not None else 'æœªçŸ¥'),
            ('æ•°æ®è¡Œæ•°', str(data_info.get('num_rows', 0))),
            ('æ•°æ®åˆ—æ•°', str(data_info.get('num_columns', 0))),
            ('æ•°å€¼å‹ç‰¹å¾', str(data_info.get('num_numeric_cols', 0))),
            ('åˆ†ç±»å‹ç‰¹å¾', str(data_info.get('num_categorical_cols', 0))),
            ('æ—¥æœŸå‹ç‰¹å¾', str(data_info.get('num_date_cols', 0))),
            ('æ•°æ®å¤§å°', str(data_info.get('data_size', 'æœªçŸ¥')) if data_info.get('data_size') is not None else 'æœªçŸ¥')
        ]
        
        for row_data in rows:
            cells = info_table.add_row().cells
            # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½ä¸ä¸ºNoneï¼Œè¿›è¡ŒåŒé‡ä¿æŠ¤
            key_text = row_data[0]
            value_text = row_data[1]
            
            # ä¿è¯é”®åæ°¸è¿œä¸ä¸ºNone
            if key_text is None:
                key_text = 'æœªçŸ¥é¡¹ç›®'
            elif not isinstance(key_text, str):
                key_text = str(key_text)
            
            # ä¿è¯å€¼æ°¸è¿œä¸ä¸ºNone
            if value_text is None:
                value_text = 'æœªçŸ¥'
            elif not isinstance(value_text, str):
                value_text = str(value_text)
            
            cells[0].text = key_text
            cells[1].text = value_text
            for cell in cells:
                cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
        
        # æ·»åŠ åˆ—ä¿¡æ¯
        if 'columns_info' in data_info:
            self.document.add_heading("1.2 åˆ—ä¿¡æ¯", level=2)
            
            col_table = self.document.add_table(rows=1, cols=4)
            col_table.style = 'Table Grid'
            col_table.alignment = WD_TABLE_ALIGNMENT.CENTER
            
            # è¡¨å¤´
            hdr_cells = col_table.rows[0].cells
            hdr_cells[0].text = 'åˆ—å'
            hdr_cells[1].text = 'æ•°æ®ç±»å‹'
            hdr_cells[2].text = 'éç©ºå€¼æ•°é‡'
            hdr_cells[3].text = 'æè¿°'
            
            for cell in hdr_cells:
                cell.paragraphs[0].runs[0].bold = True
                cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
            
            # æ·»åŠ åˆ—æ•°æ®
            for col_info in data_info['columns_info']:
                cells = col_table.add_row().cells
                # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½ä¸ä¸ºNone
                cells[0].text = str(col_info.get('name', '')) if col_info.get('name') is not None else ''
                cells[1].text = str(col_info.get('dtype', '')) if col_info.get('dtype') is not None else ''
                cells[2].text = str(col_info.get('non_null_count', 0))
                cells[3].text = str(col_info.get('description', 'N/A')) if col_info.get('description') is not None else 'N/A'
                for cell in cells:
                    cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
    
    def add_data_preprocessing_section(self, preprocessing_info: Dict) -> None:
        """
        æ·»åŠ æ•°æ®é¢„å¤„ç†éƒ¨åˆ†
        
        Args:
            preprocessing_info: é¢„å¤„ç†ä¿¡æ¯å­—å…¸
        """
        self.document.add_heading("2. æ•°æ®é¢„å¤„ç†", level=1)
        
        # ç¼ºå¤±å€¼å¤„ç†ï¼ˆå¯¹å¯èƒ½ä¸º None çš„å­—æ®µè¿›è¡Œä¿æŠ¤ï¼‰
        mv_info = preprocessing_info.get('missing_values') or {}
        if mv_info:
            self.document.add_heading("2.1 ç¼ºå¤±å€¼å¤„ç†", level=2)

            mv_para = self.document.add_paragraph()
            mv_para.add_run(f"å‘ç° {mv_info.get('total_missing', 0)} ä¸ªç¼ºå¤±å€¼ï¼Œåˆ†å¸ƒåœ¨ {mv_info.get('columns_with_missing', 0)} åˆ—ä¸­ã€‚").bold = True

            # æ·»åŠ å¤„ç†æ–¹æ³•
            handling_method = mv_info.get('handling_method')
            if handling_method:
                self.document.add_paragraph(f"å¤„ç†æ–¹æ³•: {handling_method}")

            # å¦‚æœæœ‰å¤„ç†è¯¦æƒ…
            for detail in mv_info.get('details') or []:
                # ç¡®ä¿ details å¯è¿­ä»£
                if detail is None:
                    continue
                self.document.add_paragraph(f"- {detail}", style='List Bullet')
        
        # å¼‚å¸¸å€¼å¤„ç†ï¼ˆä¿æŠ¤ Noneï¼‰
        outliers_info = preprocessing_info.get('outliers') or {}
        if outliers_info:
            self.document.add_heading("2.2 å¼‚å¸¸å€¼å¤„ç†", level=2)

            outliers_para = self.document.add_paragraph()
            outliers_para.add_run(f"å‘ç° {outliers_info.get('total_outliers', 0)} ä¸ªå¼‚å¸¸å€¼ï¼Œåˆ†å¸ƒåœ¨ {outliers_info.get('columns_with_outliers', 0)} åˆ—ä¸­ã€‚").bold = True

            # æ·»åŠ å¤„ç†æ–¹æ³•
            if outliers_info.get('handling_method'):
                self.document.add_paragraph(f"å¤„ç†æ–¹æ³•: {outliers_info.get('handling_method')}")
        
        # ç‰¹å¾å·¥ç¨‹ï¼ˆä¿æŠ¤ Noneï¼‰
        fe_info = preprocessing_info.get('feature_engineering') or {}
        if fe_info:
            self.document.add_heading("2.3 ç‰¹å¾å·¥ç¨‹", level=2)

            # ç¼–ç å¤„ç†
            if fe_info.get('encoding'):
                self.document.add_paragraph(f"åˆ†ç±»ç‰¹å¾ç¼–ç : {fe_info.get('encoding')}")

            # æ ‡å‡†åŒ–/å½’ä¸€åŒ–
            if fe_info.get('scaling'):
                self.document.add_paragraph(f"æ•°å€¼ç‰¹å¾ç¼©æ”¾: {fe_info.get('scaling')}")

            # ç‰¹å¾é€‰æ‹©
            feature_selection = fe_info.get('feature_selection') or {}
            selected_features = feature_selection.get('selected_features', []) if isinstance(feature_selection, dict) else []
            self.document.add_paragraph(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}")

            # æ·»åŠ é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨
            if len(selected_features) <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
                if selected_features:
                    features_para = self.document.add_paragraph("é€‰æ‹©çš„ç‰¹å¾:")
                    for feature in selected_features:
                        if feature is None:
                            continue
                        self.document.add_paragraph(f"- {feature}", style='List Bullet')
            else:
                self.document.add_paragraph(f"(æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾)")
                for feature in selected_features[:10]:
                    if feature is None:
                        continue
                    self.document.add_paragraph(f"- {feature}", style='List Bullet')
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
    
    def add_analysis_results(self, analysis_results: Dict) -> None:
        """
        æ·»åŠ åˆ†æç»“æœéƒ¨åˆ†
        
        Args:
            analysis_results: åˆ†æç»“æœå­—å…¸
        """
        try:
            logger.info("å¼€å§‹æ·»åŠ åˆ†æç»“æœ")
            self.document.add_heading("3. æ•°æ®åˆ†æç»“æœ", level=1)

            # ä¿æŠ¤ analysis_results ä¸º None çš„æƒ…å†µ
            if not analysis_results:
                logger.warning("analysis_resultsä¸ºç©ºï¼Œæ·»åŠ é»˜è®¤æ¶ˆæ¯")
                self.document.add_paragraph("æš‚æ— åˆ†æç»“æœå¯å±•ç¤ºã€‚")
                self.document.add_paragraph()
                return

            logger.debug(f"analysis_results keys: {list(analysis_results.keys())}")

            # ğŸ¤– é¦–å…ˆæ·»åŠ AIå¢å¼ºå†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            self._add_ai_enhanced_content(analysis_results)

            # æè¿°æ€§ç»Ÿè®¡
            try:
                if 'descriptive_stats' in analysis_results:
                    logger.info("æ·»åŠ æè¿°æ€§ç»Ÿè®¡")
                    self.document.add_heading("3.1 æè¿°æ€§ç»Ÿè®¡", level=2)
                    
                    stats_df = analysis_results.get('descriptive_stats')
                    logger.debug(f"stats_df type: {type(stats_df)}")
                    
                    if isinstance(stats_df, pd.DataFrame) and not stats_df.empty:
                        # åˆ›å»ºè¡¨æ ¼
                        stats_table = self.document.add_table(rows=1, cols=len(stats_df.columns) + 1)
                        stats_table.style = 'Table Grid'
                        stats_table.alignment = WD_TABLE_ALIGNMENT.CENTER
                        
                        # è¡¨å¤´
                        hdr_cells = stats_table.rows[0].cells
                        hdr_cells[0].text = 'ç»Ÿè®¡é‡'
                        for i, col in enumerate(stats_df.columns):
                            hdr_cells[i+1].text = str(col)
                        
                        for cell in hdr_cells:
                            cell.paragraphs[0].runs[0].bold = True
                            cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
                        
                        # æ·»åŠ æ•°æ®ï¼ˆåªæ˜¾ç¤ºä¸»è¦ç»Ÿè®¡é‡ï¼‰
                        stats_to_show = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']
                        for stat in stats_to_show:
                            if stat in stats_df.index:
                                cells = stats_table.add_row().cells
                                cells[0].text = self._get_statistic_name(stat)
                                for i, col in enumerate(stats_df.columns):
                                    try:
                                        value = stats_df.loc[stat, col]
                                        cells[i+1].text = f"{value:.4f}" if pd.notnull(value) else "N/A"
                                    except Exception as e:
                                        logger.error(f"å¤„ç†ç»Ÿè®¡å€¼æ—¶å‡ºé”™: {str(e)}")
                                        cells[i+1].text = "N/A"
                                for cell in cells:
                                    cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
                        logger.info("æè¿°æ€§ç»Ÿè®¡è¡¨æ ¼æ·»åŠ æˆåŠŸ")
                    else:
                        logger.warning("æè¿°æ€§ç»Ÿè®¡æ•°æ®æ— æ•ˆ")
                        self.document.add_paragraph("æè¿°æ€§ç»Ÿè®¡æ•°æ®æš‚ä¸å¯ç”¨ã€‚")
            except Exception as e:
                logger.error(f"æ·»åŠ æè¿°æ€§ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("æè¿°æ€§ç»Ÿè®¡ç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")
        
            # ç›¸å…³æ€§åˆ†æ
            try:
                if 'correlation' in analysis_results:
                    logger.info("æ·»åŠ ç›¸å…³æ€§åˆ†æ")
                    self.document.add_heading("3.2 ç›¸å…³æ€§åˆ†æ", level=2)
                    
                    corr_results = analysis_results.get('correlation') or {}
                    logger.debug(f"corr_results type: {type(corr_results)}, keys: {list(corr_results.keys()) if isinstance(corr_results, dict) else 'not dict'}")
                    
                    if isinstance(corr_results, dict):
                        method = corr_results.get('method', 'Pearson')
                        self.document.add_paragraph(f"ç›¸å…³æ€§åˆ†æé‡‡ç”¨ {method} æ–¹æ³•ã€‚")
                        
                        # æ˜¾ç¤ºå¼ºç›¸å…³çš„ç‰¹å¾å¯¹
                        strong_corr = corr_results.get('strong_correlations') or []
                        logger.debug(f"strong_correlations type: {type(strong_corr)}, length: {len(strong_corr) if hasattr(strong_corr, '__len__') else 'no len'}")
                        
                        if strong_corr and isinstance(strong_corr, (list, tuple)) and len(strong_corr) > 0:
                            self.document.add_paragraph("å‘ç°ä»¥ä¸‹å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆç›¸å…³ç³»æ•°ç»å¯¹å€¼ > 0.7ï¼‰:")
                            for i, pair in enumerate(strong_corr):
                                try:
                                    # ä¿æŠ¤æ¯ä¸ª pair æ˜¯å­—å…¸ä¸”åŒ…å«éœ€è¦çš„å­—æ®µ
                                    if not isinstance(pair, dict):
                                        logger.warning(f"å¼ºç›¸å…³å¯¹ {i} ä¸æ˜¯å­—å…¸ç±»å‹: {type(pair)}")
                                        continue
                                    f1 = pair.get('feature1', 'æœªçŸ¥')
                                    f2 = pair.get('feature2', 'æœªçŸ¥')
                                    corr_v = pair.get('correlation', 0.0)
                                    self.document.add_paragraph(f"- {f1} å’Œ {f2}: {corr_v:.4f}", style='List Bullet')
                                    logger.debug(f"æ·»åŠ å¼ºç›¸å…³å¯¹: {f1} - {f2}: {corr_v}")
                                except Exception as e:
                                    logger.error(f"å¤„ç†å¼ºç›¸å…³å¯¹ {i} æ—¶å‡ºé”™: {str(e)}")
                        else:
                            self.document.add_paragraph("æœªå‘ç°å¼ºç›¸å…³çš„ç‰¹å¾å¯¹ã€‚")
                    else:
                        logger.warning("ç›¸å…³æ€§åˆ†æç»“æœæ ¼å¼ä¸æ­£ç¡®")
                        self.document.add_paragraph("ç›¸å…³æ€§åˆ†æç»“æœæš‚ä¸å¯ç”¨ã€‚")
                        
                    logger.info("ç›¸å…³æ€§åˆ†ææ·»åŠ æˆåŠŸ")
            except Exception as e:
                logger.error(f"æ·»åŠ ç›¸å…³æ€§åˆ†ææ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("ç›¸å…³æ€§åˆ†æç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")
                
            # æ·»åŠ æ–°çš„ç»Ÿè®¡åˆ†æç»“æœï¼šèšç±»åˆ†æ
            try:
                if 'cluster_analysis' in analysis_results:
                    logger.info("æ·»åŠ èšç±»åˆ†æç»“æœ")
                    self.document.add_heading("3.3 èšç±»åˆ†æ", level=2)
                    
                    cluster_results = analysis_results.get('cluster_analysis') or {}
                    if isinstance(cluster_results, dict):
                        method = cluster_results.get('method', 'æœªçŸ¥')
                        n_clusters = cluster_results.get('n_clusters', 0)
                        silhouette_score = cluster_results.get('silhouette_score')
                        
                        self.document.add_paragraph(f"é‡‡ç”¨ {method} èšç±»æ–¹æ³•ï¼Œå°†æ•°æ®åˆ†ä¸º {n_clusters} ä¸ªç±»åˆ«ã€‚")
                        
                        if silhouette_score is not None:
                            self.document.add_paragraph(f"è½®å»“ç³»æ•°ï¼ˆSilhouette Scoreï¼‰: {silhouette_score:.4f}")
                            self.document.add_paragraph("è½®å»“ç³»æ•°è¶Šæ¥è¿‘1è¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½ï¼Œæ¥è¿‘0è¡¨ç¤ºèšç±»æ•ˆæœä¸€èˆ¬ï¼Œæ¥è¿‘-1è¡¨ç¤ºèšç±»æ•ˆæœè¾ƒå·®ã€‚")
                        
                        # æ˜¾ç¤ºèšç±»ä¸­å¿ƒ
                        if 'cluster_centers' in cluster_results:
                            centers = cluster_results.get('cluster_centers')
                            if centers is not None and hasattr(centers, 'shape'):
                                self.document.add_paragraph(f"èšç±»ä¸­å¿ƒç‰¹å¾ç»´åº¦: {centers.shape}")
                        
                        # æ˜¾ç¤ºå„ç±»åˆ«æ ·æœ¬æ•°é‡
                        if 'cluster_counts' in cluster_results:
                            counts = cluster_results.get('cluster_counts', {})
                            if counts:
                                self.document.add_paragraph("å„ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ:")
                                for cluster_id, count in counts.items():
                                    self.document.add_paragraph(f"- ç±»åˆ« {cluster_id}: {count} ä¸ªæ ·æœ¬", style='List Bullet')
            except Exception as e:
                logger.error(f"æ·»åŠ èšç±»åˆ†ææ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("èšç±»åˆ†æç»“æœç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")
            
            # æ·»åŠ å› å­åˆ†æç»“æœ
            try:
                if 'factor_analysis' in analysis_results:
                    logger.info("æ·»åŠ å› å­åˆ†æç»“æœ")
                    self.document.add_heading("3.4 å› å­åˆ†æ", level=2)
                    
                    factor_results = analysis_results.get('factor_analysis') or {}
                    if isinstance(factor_results, dict):
                        n_factors = factor_results.get('n_factors', 0)
                        kmo_score = factor_results.get('kmo_score')
                        explained_variance = factor_results.get('explained_variance_ratio')
                        
                        self.document.add_paragraph(f"æå– {n_factors} ä¸ªä¸»è¦å› å­ã€‚")
                        
                        if kmo_score is not None:
                            self.document.add_paragraph(f"KMOæ£€éªŒå€¼: {kmo_score:.4f}")
                            if kmo_score > 0.7:
                                self.document.add_paragraph("KMOæ£€éªŒå€¼å¤§äº0.7ï¼Œè¡¨æ˜æ•°æ®é€‚åˆè¿›è¡Œå› å­åˆ†æã€‚")
                            elif kmo_score > 0.5:
                                self.document.add_paragraph("KMOæ£€éªŒå€¼åœ¨0.5-0.7ä¹‹é—´ï¼Œæ•°æ®å‹‰å¼ºé€‚åˆå› å­åˆ†æã€‚")
                            else:
                                self.document.add_paragraph("KMOæ£€éªŒå€¼å°äº0.5ï¼Œæ•°æ®ä¸å¤ªé€‚åˆå› å­åˆ†æã€‚")
                        
                        if explained_variance is not None and hasattr(explained_variance, '__len__'):
                            total_variance = sum(explained_variance) * 100
                            self.document.add_paragraph(f"æå–çš„å› å­ç´¯è®¡è§£é‡Šæ–¹å·®: {total_variance:.2f}%")
                            
                            self.document.add_paragraph("å„å› å­è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
                            for i, var_ratio in enumerate(explained_variance, 1):
                                self.document.add_paragraph(f"- å› å­ {i}: {var_ratio*100:.2f}%", style='List Bullet')
                        
                        # æ˜¾ç¤ºå› å­è½½è·
                        if 'factor_loadings' in factor_results:
                            loadings = factor_results.get('factor_loadings')
                            if loadings is not None and hasattr(loadings, 'shape'):
                                self.document.add_paragraph(f"å› å­è½½è·çŸ©é˜µç»´åº¦: {loadings.shape[0]} ä¸ªå˜é‡ Ã— {loadings.shape[1]} ä¸ªå› å­")
            except Exception as e:
                logger.error(f"æ·»åŠ å› å­åˆ†ææ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("å› å­åˆ†æç»“æœç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")
            
            # æ·»åŠ æ–¹å·®åˆ†æç»“æœ
            try:
                if 'anova_analysis' in analysis_results:
                    logger.info("æ·»åŠ æ–¹å·®åˆ†æç»“æœ")
                    self.document.add_heading("3.5 æ–¹å·®åˆ†æï¼ˆANOVAï¼‰", level=2)
                    
                    anova_results = analysis_results.get('anova_analysis') or {}
                    if isinstance(anova_results, dict):
                        dependent_var = anova_results.get('dependent_variable', 'æœªçŸ¥')
                        independent_var = anova_results.get('independent_variable', 'æœªçŸ¥')
                        f_statistic = anova_results.get('f_statistic')
                        p_value = anova_results.get('p_value')
                        effect_size = anova_results.get('effect_size')
                        
                        self.document.add_paragraph(f"åˆ†æå› å˜é‡ '{dependent_var}' åœ¨è‡ªå˜é‡ '{independent_var}' ä¸åŒæ°´å¹³é—´çš„å·®å¼‚ã€‚")
                        
                        if f_statistic is not None and p_value is not None:
                            self.document.add_paragraph(f"Fç»Ÿè®¡é‡: {f_statistic:.4f}")
                            self.document.add_paragraph(f"på€¼: {p_value:.4f}")
                            
                            if p_value < 0.05:
                                self.document.add_paragraph("p < 0.05ï¼Œæ‹’ç»åŸå‡è®¾ï¼Œä¸åŒç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚")
                            else:
                                self.document.add_paragraph("p â‰¥ 0.05ï¼Œæ¥å—åŸå‡è®¾ï¼Œä¸åŒç»„é—´æ— æ˜¾è‘—å·®å¼‚ã€‚")
                        
                        if effect_size is not None:
                            self.document.add_paragraph(f"æ•ˆåº”é‡ï¼ˆÎ·Â²ï¼‰: {effect_size:.4f}")
                            if effect_size > 0.14:
                                self.document.add_paragraph("æ•ˆåº”é‡å¤§ï¼ˆÎ·Â² > 0.14ï¼‰ï¼Œè¡¨æ˜ç»„é—´å·®å¼‚å¾ˆå¤§ã€‚")
                            elif effect_size > 0.06:
                                self.document.add_paragraph("æ•ˆåº”é‡ä¸­ç­‰ï¼ˆ0.06 < Î·Â² â‰¤ 0.14ï¼‰ï¼Œè¡¨æ˜ç»„é—´å·®å¼‚é€‚ä¸­ã€‚")
                            else:
                                self.document.add_paragraph("æ•ˆåº”é‡å°ï¼ˆÎ·Â² â‰¤ 0.06ï¼‰ï¼Œè¡¨æ˜ç»„é—´å·®å¼‚è¾ƒå°ã€‚")
                        
                        # æ˜¾ç¤ºLeveneæ£€éªŒç»“æœ
                        if 'levene_test' in anova_results:
                            levene_results = anova_results.get('levene_test', {})
                            levene_p = levene_results.get('p_value')
                            if levene_p is not None:
                                self.document.add_paragraph(f"Leveneæ–¹å·®é½æ€§æ£€éªŒ på€¼: {levene_p:.4f}")
                                if levene_p > 0.05:
                                    self.document.add_paragraph("æ–¹å·®é½æ€§å‡è®¾æˆç«‹ï¼ŒANOVAç»“æœå¯ä¿¡ã€‚")
                                else:
                                    self.document.add_paragraph("æ–¹å·®é½æ€§å‡è®¾ä¸æˆç«‹ï¼Œå»ºè®®ä½¿ç”¨Welch ANOVAã€‚")
                        
                        # æ˜¾ç¤ºäº‹åæ£€éªŒç»“æœ
                        if 'post_hoc' in anova_results:
                            post_hoc_results = anova_results.get('post_hoc', {})
                            if 'significant_pairs' in post_hoc_results:
                                sig_pairs = post_hoc_results.get('significant_pairs', [])
                                if sig_pairs:
                                    self.document.add_paragraph("äº‹åæ£€éªŒå‘ç°ä»¥ä¸‹ç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚:")
                                    for pair in sig_pairs:
                                        if isinstance(pair, dict):
                                            group1 = pair.get('group1', 'æœªçŸ¥')
                                            group2 = pair.get('group2', 'æœªçŸ¥')
                                            p_val = pair.get('p_value', 0)
                                            self.document.add_paragraph(f"- {group1} vs {group2}: p = {p_val:.4f}", style='List Bullet')
            except Exception as e:
                logger.error(f"æ·»åŠ æ–¹å·®åˆ†ææ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("æ–¹å·®åˆ†æç»“æœç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")

            # æ¨¡å‹æ¨è
            try:
                if 'model_recommendations' in analysis_results:
                    logger.info("æ·»åŠ æ¨¡å‹æ¨è")
                    recommendations = analysis_results.get('model_recommendations') or []
                    logger.debug(f"recommendations type: {type(recommendations)}, length: {len(recommendations) if hasattr(recommendations, '__len__') else 'no len'}")
                    
                    # æ£€æŸ¥recommendationsæ˜¯å¦ä¸ºå¯è¿­ä»£åˆ—è¡¨
                    if isinstance(recommendations, (list, tuple)) and len(recommendations) > 0:
                        self.document.add_heading("3.6 æ¨¡å‹æ¨è", level=2)
                        self.document.add_paragraph("æ ¹æ®æ•°æ®ç‰¹å¾ï¼Œæ¨èä»¥ä¸‹åˆ†ææ¨¡å‹:")
                        
                        for i, model in enumerate(recommendations, 1):
                            try:
                                if not isinstance(model, dict):
                                    logger.warning(f"æ¨¡å‹æ¨è {i} ä¸æ˜¯å­—å…¸ç±»å‹: {type(model)}")
                                    continue
                                model_para = self.document.add_paragraph()
                                model_name = model.get('name', 'æœªçŸ¥æ¨¡å‹')
                                model_score = model.get('score', 0)
                                model_para.add_run(f"{i}. {model_name}").bold = True
                                model_para.add_run(f" (æ¨èæŒ‡æ•°: {model_score:.2f}/10)")
                                
                                if model.get('description'):
                                    self.document.add_paragraph(str(model.get('description')), style='List Bullet')
                                
                                if model.get('reason'):
                                    self.document.add_paragraph(f"æ¨èåŸå› : {model.get('reason')}", style='List Bullet')
                                    
                                logger.debug(f"æ·»åŠ æ¨¡å‹æ¨è: {model_name}")
                            except Exception as e:
                                logger.error(f"å¤„ç†æ¨¡å‹æ¨è {i} æ—¶å‡ºé”™: {str(e)}")
                    else:
                        self.document.add_heading("3.6 æ¨¡å‹æ¨è", level=2)
                        self.document.add_paragraph("æš‚æ— å¯ç”¨çš„æ¨¡å‹æ¨èã€‚")
                        
                    logger.info("æ¨¡å‹æ¨èæ·»åŠ æˆåŠŸ")
            except Exception as e:
                logger.error(f"æ·»åŠ æ¨¡å‹æ¨èæ—¶å‡ºé”™: {str(e)}")
                self.document.add_paragraph("æ¨¡å‹æ¨èç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")

            # æ·»åŠ ç©ºè¡Œ
            self.document.add_paragraph()
            logger.info("åˆ†æç»“æœæ·»åŠ å®Œæˆ")
            
        except Exception as e:
            logger.exception(f"æ·»åŠ åˆ†æç»“æœæ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
            raise

    def add_report_template_example(self) -> None:
        """
        æ·»åŠ æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿åˆ°æŠ¥å‘Šå¼€å¤´
        """
        try:
            logger.info("æ·»åŠ æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿")
            
            # æ’å…¥ä¸€ä¸ªé¡µé¢åˆ†éš”ç¬¦ï¼Œå°†æ ·ä¾‹æ¨¡æ¿ä¸æ­£å¼æŠ¥å‘Šåˆ†å¼€
            self.document.add_page_break()
            
            # æ·»åŠ æ ·ä¾‹æ¨¡æ¿æ ‡é¢˜
            template_title = self.document.add_heading("æ•°æ®åˆ†ææŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿", level=1)
            template_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # æ·»åŠ æ¨¡æ¿è¯´æ˜
            self.document.add_paragraph("æœ¬æ¨¡æ¿æä¾›æ ‡å‡†åŒ–çš„æ•°æ®åˆ†ææŠ¥å‘Šç»“æ„ï¼Œç¡®ä¿æŠ¥å‘Šå†…å®¹å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€‚")
            
            # 1. æ‰§è¡Œæ‘˜è¦æ¨¡æ¿
            self.document.add_heading("1. æ‰§è¡Œæ‘˜è¦", level=2)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘æœ¬æŠ¥å‘Šå¯¹XXXæ•°æ®é›†è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæ•°æ®é›†åŒ…å«Xä¸ªæ ·æœ¬å’ŒYä¸ªç‰¹å¾ã€‚ä¸»è¦å‘ç°åŒ…æ‹¬ï¼š")
            self.document.add_paragraph("â€¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç¼ºå¤±å€¼æ¯”ä¾‹ä¸ºX%ï¼Œå·²é‡‡ç”¨åˆé€‚æ–¹æ³•å¤„ç†", style='List Bullet')
            self.document.add_paragraph("â€¢ å‘ç°äº†Xä¸ªå¼ºç›¸å…³ç‰¹å¾å¯¹ï¼Œç›¸å…³ç³»æ•°å‡è¶…è¿‡0.7", style='List Bullet')
            self.document.add_paragraph("â€¢ èšç±»åˆ†æå°†æ ·æœ¬åˆ†ä¸ºXä¸ªç±»åˆ«ï¼Œè½®å»“ç³»æ•°è¾¾åˆ°X", style='List Bullet')
            self.document.add_paragraph("â€¢ å› å­åˆ†ææå–äº†Xä¸ªä¸»è¦å› å­ï¼Œç´¯è®¡è§£é‡Šæ–¹å·®X%", style='List Bullet')
            self.document.add_paragraph("æ¨èæ¨¡å‹ï¼šåŸºäºæ•°æ®ç‰¹å¾ï¼Œæ¨èä½¿ç”¨XXXæ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚")
            
            # 2. æ•°æ®æ¦‚è§ˆæ¨¡æ¿
            self.document.add_heading("2. æ•°æ®æ¦‚è§ˆ", level=2)
            self.document.add_heading("2.1 æ•°æ®é›†åŸºæœ¬ä¿¡æ¯", level=3)
            
            # åˆ›å»ºæ ·ä¾‹ä¿¡æ¯è¡¨æ ¼
            example_table = self.document.add_table(rows=1, cols=2)
            example_table.style = 'Table Grid'
            hdr_cells = example_table.rows[0].cells
            hdr_cells[0].text = 'å±æ€§'
            hdr_cells[1].text = 'å€¼'
            for cell in hdr_cells:
                cell.paragraphs[0].runs[0].bold = True
            
            example_data = [
                ('æ–‡ä»¶åç§°', 'sample_data.csv'),
                ('æ•°æ®æ ¼å¼', 'CSV'),
                ('æ•°æ®è¡Œæ•°', '1,000'),
                ('æ•°æ®åˆ—æ•°', '15'),
                ('æ•°å€¼å‹ç‰¹å¾', '10'),
                ('åˆ†ç±»å‹ç‰¹å¾', '5'),
                ('æ•°æ®å¤§å°', '125.6 KB')
            ]
            
            for attr, value in example_data:
                cells = example_table.add_row().cells
                cells[0].text = attr
                cells[1].text = value
            
            self.document.add_heading("2.2 å­—æ®µä¿¡æ¯", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘æ•°æ®é›†åŒ…å«ä»¥ä¸‹ä¸»è¦å­—æ®µï¼š")
            self.document.add_paragraph("â€¢ ç”¨æˆ·IDï¼šå”¯ä¸€æ ‡è¯†ç¬¦", style='List Bullet')
            self.document.add_paragraph("â€¢ å¹´é¾„ï¼šæ•°å€¼å‹ï¼ŒèŒƒå›´18-65å²", style='List Bullet')
            self.document.add_paragraph("â€¢ æ€§åˆ«ï¼šåˆ†ç±»å‹ï¼Œç”·/å¥³", style='List Bullet')
            self.document.add_paragraph("â€¢ æ”¶å…¥ï¼šæ•°å€¼å‹ï¼Œå•ä½ï¼šä¸‡å…ƒ", style='List Bullet')
            
            # 3. æ•°æ®é¢„å¤„ç†æ¨¡æ¿
            self.document.add_heading("3. æ•°æ®é¢„å¤„ç†", level=2)
            self.document.add_heading("3.1 ç¼ºå¤±å€¼å¤„ç†", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘å‘ç° 50 ä¸ªç¼ºå¤±å€¼ï¼Œåˆ†å¸ƒåœ¨ 3 åˆ—ä¸­ã€‚")
            self.document.add_paragraph("å¤„ç†æ–¹æ³•: æ•°å€¼å‹ç‰¹å¾é‡‡ç”¨å‡å€¼å¡«å……ï¼Œåˆ†ç±»å‹ç‰¹å¾é‡‡ç”¨ä¼—æ•°å¡«å……")
            
            self.document.add_heading("3.2 å¼‚å¸¸å€¼å¤„ç†", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘é‡‡ç”¨3ÏƒåŸåˆ™æ£€æµ‹å¼‚å¸¸å€¼ï¼Œå‘ç° 15 ä¸ªå¼‚å¸¸å€¼ã€‚")
            self.document.add_paragraph("å¤„ç†æ–¹æ³•: å¯¹è¶…å‡º3ÏƒèŒƒå›´çš„å€¼è¿›è¡Œæˆªæ–­å¤„ç†")
            
            # 4. æ•°æ®åˆ†æç»“æœæ¨¡æ¿
            self.document.add_heading("4. æ•°æ®åˆ†æç»“æœ", level=2)
            
            self.document.add_heading("4.1 æè¿°æ€§ç»Ÿè®¡", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘ä¸»è¦æ•°å€¼å‹ç‰¹å¾çš„æè¿°æ€§ç»Ÿè®¡å¦‚ä¸‹ï¼š")
            # åˆ›å»ºæ ·ä¾‹ç»Ÿè®¡è¡¨æ ¼
            stats_example_table = self.document.add_table(rows=1, cols=4)
            stats_example_table.style = 'Table Grid'
            stats_hdr = stats_example_table.rows[0].cells
            stats_hdr[0].text = 'ç»Ÿè®¡é‡'
            stats_hdr[1].text = 'å¹´é¾„'
            stats_hdr[2].text = 'æ”¶å…¥'
            stats_hdr[3].text = 'æ¶ˆè´¹é‡‘é¢'
            for cell in stats_hdr:
                cell.paragraphs[0].runs[0].bold = True
            
            stats_data = [
                ('å‡å€¼', '32.5', '8.7', '2.3'),
                ('æ ‡å‡†å·®', '8.2', '3.1', '1.2'),
                ('æœ€å°å€¼', '18.0', '2.0', '0.1'),
                ('æœ€å¤§å€¼', '65.0', '20.0', '8.9')
            ]
            
            for stat_row in stats_data:
                cells = stats_example_table.add_row().cells
                for i, value in enumerate(stat_row):
                    cells[i].text = value
            
            self.document.add_heading("4.2 ç›¸å…³æ€§åˆ†æ", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘ç›¸å…³æ€§åˆ†æé‡‡ç”¨ Pearson æ–¹æ³•ã€‚")
            self.document.add_paragraph("å‘ç°ä»¥ä¸‹å¼ºç›¸å…³ç‰¹å¾å¯¹ï¼ˆç›¸å…³ç³»æ•°ç»å¯¹å€¼ > 0.7ï¼‰:")
            self.document.add_paragraph("â€¢ æ”¶å…¥ å’Œ æ¶ˆè´¹é‡‘é¢: 0.8523", style='List Bullet')
            self.document.add_paragraph("â€¢ å¹´é¾„ å’Œ å‚¨è“„é‡‘é¢: 0.7341", style='List Bullet')
            
            self.document.add_heading("4.3 èšç±»åˆ†æ", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘é‡‡ç”¨ K-means èšç±»æ–¹æ³•ï¼Œå°†æ•°æ®åˆ†ä¸º 3 ä¸ªç±»åˆ«ã€‚")
            self.document.add_paragraph("è½®å»“ç³»æ•°ï¼ˆSilhouette Scoreï¼‰: 0.7234")
            self.document.add_paragraph("å„ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ:")
            self.document.add_paragraph("â€¢ ç±»åˆ« 0: 350 ä¸ªæ ·æœ¬", style='List Bullet')
            self.document.add_paragraph("â€¢ ç±»åˆ« 1: 420 ä¸ªæ ·æœ¬", style='List Bullet')
            self.document.add_paragraph("â€¢ ç±»åˆ« 2: 230 ä¸ªæ ·æœ¬", style='List Bullet')
            
            self.document.add_heading("4.4 å› å­åˆ†æ", level=3)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘æå– 4 ä¸ªä¸»è¦å› å­ã€‚")
            self.document.add_paragraph("KMOæ£€éªŒå€¼: 0.8456 - æ•°æ®é€‚åˆè¿›è¡Œå› å­åˆ†æ")
            self.document.add_paragraph("æå–çš„å› å­ç´¯è®¡è§£é‡Šæ–¹å·®: 78.92%")
            self.document.add_paragraph("å„å› å­è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
            self.document.add_paragraph("â€¢ å› å­ 1: 34.12%", style='List Bullet')
            self.document.add_paragraph("â€¢ å› å­ 2: 22.45%", style='List Bullet')
            self.document.add_paragraph("â€¢ å› å­ 3: 14.78%", style='List Bullet')
            self.document.add_paragraph("â€¢ å› å­ 4: 7.57%", style='List Bullet')
            
            # 5. ç»“è®ºä¸å»ºè®®æ¨¡æ¿
            self.document.add_heading("5. ç»“è®ºä¸å»ºè®®", level=2)
            self.document.add_paragraph("ã€æ ·ä¾‹ã€‘é€šè¿‡å¯¹æ•°æ®é›†çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š")
            self.document.add_paragraph("1. æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»è¿‡é€‚å½“çš„æ¸…æ´—å’Œé¢„å¤„ç†åå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æã€‚")
            self.document.add_paragraph("2. æ”¶å…¥ä¸æ¶ˆè´¹é‡‘é¢å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³»ï¼Œå¯ä½œä¸ºé¢„æµ‹æ¨¡å‹çš„é‡è¦ç‰¹å¾ã€‚")
            self.document.add_paragraph("3. ç”¨æˆ·å¯æ˜ç¡®åˆ†ä¸ºä¸‰ä¸ªç¾¤ä½“ï¼Œå…·æœ‰ä¸åŒçš„æ¶ˆè´¹ç‰¹å¾å’Œè¡Œä¸ºæ¨¡å¼ã€‚")
            self.document.add_paragraph("4. æå–çš„ä¸»è¦å› å­èƒ½å¤Ÿæœ‰æ•ˆè§£é‡Šæ•°æ®çš„ä¸»è¦å˜å¼‚æ¥æºã€‚")
            
            self.document.add_paragraph("å»ºè®®ï¼š")
            self.document.add_paragraph("â€¢ åŸºäºèšç±»ç»“æœåˆ¶å®šå·®å¼‚åŒ–çš„è¥é”€ç­–ç•¥", style='List Bullet')
            self.document.add_paragraph("â€¢ åˆ©ç”¨å¼ºç›¸å…³ç‰¹å¾æ„å»ºæ¶ˆè´¹é¢„æµ‹æ¨¡å‹", style='List Bullet')
            self.document.add_paragraph("â€¢ å®šæœŸæ›´æ–°åˆ†ææŠ¥å‘Šï¼Œç›‘æ§ç”¨æˆ·è¡Œä¸ºå˜åŒ–", style='List Bullet')
            self.document.add_paragraph("â€¢ è€ƒè™‘å¼•å…¥æ›´å¤šè¡Œä¸ºç‰¹å¾ä»¥æé«˜åˆ†æç²¾åº¦", style='List Bullet')
            
            # æ·»åŠ åˆ†éš”çº¿
            self.document.add_paragraph("=" * 60, style='Heading 1')
            self.document.add_paragraph("ä»¥ä¸‹ä¸ºå®é™…åˆ†ææŠ¥å‘Šå†…å®¹", style='Heading 1')
            self.document.add_paragraph("=" * 60, style='Heading 1')
            
            logger.info("æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            logger.exception(f"æ·»åŠ æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            # å¦‚æœå‡ºé”™ï¼Œæ·»åŠ ç®€å•çš„è¯´æ˜
            self.document.add_paragraph("æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿ç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ï¼Œå°†ç›´æ¥æ˜¾ç¤ºåˆ†æç»“æœã€‚")
    
    def smart_merge_questionnaire_data(self, data: pd.DataFrame, 
                                     item_mapping: Optional[Dict[str, str]] = None) -> Dict:
        """
        æ™ºèƒ½æ•´åˆé—®å·æ•°æ®ä¸é¢˜é¡¹è¡¨
        
        Args:
            data: åŸå§‹æ•°æ®DataFrame
            item_mapping: é¢˜é¡¹æ˜ å°„å­—å…¸ï¼Œæ ¼å¼ä¸º {åˆ—å: é¢˜ç›®æè¿°}
        
        Returns:
            åŒ…å«æ•´åˆåä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info("å¼€å§‹æ™ºèƒ½æ•´åˆé—®å·æ•°æ®ä¸é¢˜é¡¹è¡¨")
            
            merged_info = {
                'original_columns': list(data.columns),
                'item_mapping': {},
                'scale_detection': {},
                'data_types': {},
                'missing_analysis': {},
                'recommendations': []
            }
            
            # å¦‚æœæä¾›äº†é¢˜é¡¹æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨
            if item_mapping and isinstance(item_mapping, dict):
                merged_info['item_mapping'] = item_mapping.copy()
                logger.info(f"ä½¿ç”¨æä¾›çš„é¢˜é¡¹æ˜ å°„ï¼ŒåŒ…å« {len(item_mapping)} ä¸ªé¢˜é¡¹")
            else:
                # æ™ºèƒ½æ¨æ–­é¢˜é¡¹æ˜ å°„
                logger.info("å¼€å§‹æ™ºèƒ½æ¨æ–­é¢˜é¡¹æ˜ å°„")
                for col in data.columns:
                    col_str = str(col).lower()
                    
                    # åŸºäºåˆ—åæ¨¡å¼æ¨æ–­é¢˜ç›®ç±»å‹
                    if any(pattern in col_str for pattern in ['q', 'question', 'é¢˜', 'é—®é¢˜']):
                        # é—®å·é¢˜ç›®
                        if 'age' in col_str or 'å¹´é¾„' in col_str:
                            merged_info['item_mapping'][col] = f"å¹´é¾„ç›¸å…³é¢˜é¡¹: {col}"
                        elif 'gender' in col_str or 'æ€§åˆ«' in col_str:
                            merged_info['item_mapping'][col] = f"æ€§åˆ«ç›¸å…³é¢˜é¡¹: {col}"
                        elif 'income' in col_str or 'æ”¶å…¥' in col_str or 'salary' in col_str:
                            merged_info['item_mapping'][col] = f"æ”¶å…¥ç›¸å…³é¢˜é¡¹: {col}"
                        elif 'satisfaction' in col_str or 'æ»¡æ„' in col_str:
                            merged_info['item_mapping'][col] = f"æ»¡æ„åº¦é¢˜é¡¹: {col}"
                        elif 'attitude' in col_str or 'æ€åº¦' in col_str:
                            merged_info['item_mapping'][col] = f"æ€åº¦é¢˜é¡¹: {col}"
                        else:
                            merged_info['item_mapping'][col] = f"é—®å·é¢˜é¡¹: {col}"
                    elif any(pattern in col_str for pattern in ['scale', 'é‡è¡¨', 'score', 'å¾—åˆ†']):
                        merged_info['item_mapping'][col] = f"é‡è¡¨é¢˜é¡¹: {col}"
                    elif col_str in ['id', 'userid', 'user_id', 'ç”¨æˆ·id', 'ç¼–å·']:
                        merged_info['item_mapping'][col] = f"æ ‡è¯†ç¬¦: {col}"
                    else:
                        # åŸºäºæ•°æ®ç±»å‹æ¨æ–­
                        if data[col].dtype in ['int64', 'float64']:
                            unique_values = data[col].nunique()
                            if unique_values <= 10:
                                merged_info['item_mapping'][col] = f"åˆ†ç±»å˜é‡/ç­‰çº§è¯„åˆ†: {col}"
                            else:
                                merged_info['item_mapping'][col] = f"è¿ç»­å˜é‡: {col}"
                        else:
                            merged_info['item_mapping'][col] = f"æ–‡æœ¬/åˆ†ç±»å˜é‡: {col}"
            
            # æ£€æµ‹é‡è¡¨ç±»å‹
            logger.info("æ£€æµ‹é‡è¡¨ç±»å‹")
            for col in data.columns:
                if data[col].dtype in ['int64', 'float64']:
                    unique_values = sorted(data[col].dropna().unique())
                    
                    # Likerté‡è¡¨æ£€æµ‹
                    if len(unique_values) <= 7 and all(isinstance(v, (int, float)) for v in unique_values):
                        if set(unique_values).issubset(set(range(1, 6))):
                            merged_info['scale_detection'][col] = "5ç‚¹Likerté‡è¡¨ (1-5)"
                        elif set(unique_values).issubset(set(range(1, 8))):
                            merged_info['scale_detection'][col] = "7ç‚¹Likerté‡è¡¨ (1-7)"
                        elif set(unique_values).issubset(set([0, 1])):
                            merged_info['scale_detection'][col] = "äºŒåˆ†å˜é‡ (0-1)"
                        else:
                            merged_info['scale_detection'][col] = f"ç­‰çº§å˜é‡ ({min(unique_values)}-{max(unique_values)})"
                    else:
                        merged_info['scale_detection'][col] = "è¿ç»­å˜é‡"
                else:
                    merged_info['scale_detection'][col] = "åˆ†ç±»å˜é‡"
            
            # æ•°æ®ç±»å‹åˆ†æ
            for col in data.columns:
                merged_info['data_types'][col] = {
                    'dtype': str(data[col].dtype),
                    'unique_count': int(data[col].nunique()),
                    'null_count': int(data[col].isnull().sum()),
                    'null_percentage': float(data[col].isnull().sum() / len(data) * 100)
                }
            
            # ç¼ºå¤±å€¼åˆ†æ
            missing_cols = data.columns[data.isnull().any()].tolist()
            if missing_cols:
                merged_info['missing_analysis']['columns_with_missing'] = missing_cols
                merged_info['missing_analysis']['total_missing'] = int(data.isnull().sum().sum())
                merged_info['missing_analysis']['missing_percentage'] = float(data.isnull().sum().sum() / (len(data) * len(data.columns)) * 100)
                
                # å¯¹æ¯ä¸ªæœ‰ç¼ºå¤±å€¼çš„åˆ—åˆ†æç¼ºå¤±æ¨¡å¼
                for col in missing_cols[:5]:  # åªåˆ†æå‰5ä¸ªåˆ—
                    missing_count = int(data[col].isnull().sum())
                    missing_pct = float(missing_count / len(data) * 100)
                    merged_info['missing_analysis'][col] = {
                        'count': missing_count,
                        'percentage': missing_pct
                    }
            
            # ç”Ÿæˆå»ºè®®
            recommendations = []
            
            # åŸºäºç¼ºå¤±å€¼æƒ…å†µç»™å»ºè®®
            if merged_info['missing_analysis'].get('missing_percentage', 0) > 10:
                recommendations.append("æ•°æ®ç¼ºå¤±å€¼æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®è¿›è¡Œä¸“é—¨çš„ç¼ºå¤±å€¼åˆ†æå’Œå¤„ç†")
            elif merged_info['missing_analysis'].get('missing_percentage', 0) > 5:
                recommendations.append("å­˜åœ¨ä¸€å®šæ¯”ä¾‹çš„ç¼ºå¤±å€¼ï¼Œå»ºè®®é‡‡ç”¨é€‚å½“çš„æ’è¡¥æ–¹æ³•")
            
            # åŸºäºé‡è¡¨ç±»å‹ç»™å»ºè®®
            likert_scales = [col for col, scale_type in merged_info['scale_detection'].items() 
                           if 'Likert' in scale_type]
            if likert_scales:
                recommendations.append(f"æ£€æµ‹åˆ° {len(likert_scales)} ä¸ªLikerté‡è¡¨å˜é‡ï¼Œå»ºè®®è¿›è¡Œä¿¡åº¦åˆ†æ")
            
            # åŸºäºå˜é‡æ•°é‡ç»™å»ºè®®
            if len(data.columns) > 20:
                recommendations.append("å˜é‡æ•°é‡è¾ƒå¤šï¼Œå»ºè®®è¿›è¡Œé™ç»´åˆ†ææˆ–å› å­åˆ†æ")
            
            # åŸºäºæ•°æ®é‡ç»™å»ºè®®
            if len(data) < 100:
                recommendations.append("æ ·æœ¬é‡è¾ƒå°ï¼Œç»Ÿè®¡åˆ†æç»“æœå¯èƒ½ä¸å¤Ÿç¨³å®š")
            elif len(data) > 10000:
                recommendations.append("æ ·æœ¬é‡è¾ƒå¤§ï¼Œå¯ä»¥è¿›è¡Œå¤æ‚çš„ç»Ÿè®¡å»ºæ¨¡åˆ†æ")
            
            merged_info['recommendations'] = recommendations
            
            logger.info(f"æ™ºèƒ½æ•´åˆå®Œæˆï¼Œè¯†åˆ« {len(merged_info['item_mapping'])} ä¸ªé¢˜é¡¹ï¼Œ{len(likert_scales)} ä¸ªLikerté‡è¡¨")
            return merged_info
            
        except Exception as e:
            logger.exception(f"æ™ºèƒ½æ•´åˆé—®å·æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            # è¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'original_columns': list(data.columns) if data is not None else [],
                'item_mapping': {col: f"å˜é‡: {col}" for col in data.columns} if data is not None else {},
                'scale_detection': {},
                'data_types': {},
                'missing_analysis': {},
                'recommendations': ["æ•°æ®æ•´åˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æ ¼å¼"]
            }
    
    def add_questionnaire_analysis_section(self, merged_info: Dict) -> None:
        """
        æ·»åŠ é—®å·æ•°æ®åˆ†æä¸“èŠ‚
        
        Args:
            merged_info: æ™ºèƒ½æ•´åˆåçš„é—®å·ä¿¡æ¯
        """
        try:
            logger.info("æ·»åŠ é—®å·æ•°æ®åˆ†æä¸“èŠ‚")
            
            self.document.add_heading("é—®å·æ•°æ®æ™ºèƒ½åˆ†æ", level=1)
            
            # 1. é¢˜é¡¹æ˜ å°„è¡¨
            if merged_info.get('item_mapping'):
                self.document.add_heading("1.1 é¢˜é¡¹æ˜ å°„è¡¨", level=2)
                self.document.add_paragraph("ä»¥ä¸‹æ˜¯æ™ºèƒ½è¯†åˆ«çš„é¢˜é¡¹ä¸æè¿°å¯¹åº”å…³ç³»ï¼š")
                
                # åˆ›å»ºé¢˜é¡¹è¡¨æ ¼
                item_table = self.document.add_table(rows=1, cols=3)
                item_table.style = 'Table Grid'
                item_table.alignment = WD_TABLE_ALIGNMENT.CENTER
                
                hdr_cells = item_table.rows[0].cells
                hdr_cells[0].text = 'å˜é‡å'
                hdr_cells[1].text = 'é¢˜é¡¹æè¿°'
                hdr_cells[2].text = 'æ•°æ®ç±»å‹'
                
                for cell in hdr_cells:
                    cell.paragraphs[0].runs[0].bold = True
                    cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
                
                # æ·»åŠ é¢˜é¡¹æ•°æ®
                for col, description in merged_info['item_mapping'].items():
                    cells = item_table.add_row().cells
                    cells[0].text = str(col)
                    cells[1].text = str(description)
                    
                    # è·å–æ•°æ®ç±»å‹ä¿¡æ¯
                    data_type_info = merged_info.get('data_types', {}).get(col, {})
                    scale_type = merged_info.get('scale_detection', {}).get(col, 'æœªçŸ¥')
                    cells[2].text = scale_type
                    
                    for cell in cells:
                        cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
            
            # 2. é‡è¡¨æ£€æµ‹ç»“æœ
            if merged_info.get('scale_detection'):
                self.document.add_heading("1.2 é‡è¡¨ç±»å‹æ£€æµ‹", level=2)
                
                # ç»Ÿè®¡ä¸åŒé‡è¡¨ç±»å‹
                scale_stats = {}
                for col, scale_type in merged_info['scale_detection'].items():
                    if scale_type not in scale_stats:
                        scale_stats[scale_type] = []
                    scale_stats[scale_type].append(col)
                
                self.document.add_paragraph("æ£€æµ‹åˆ°ä»¥ä¸‹é‡è¡¨ç±»å‹ï¼š")
                for scale_type, columns in scale_stats.items():
                    self.document.add_paragraph(f"â€¢ {scale_type}: {len(columns)} ä¸ªå˜é‡", style='List Bullet')
                    if len(columns) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªå˜é‡
                        var_list = ", ".join(columns)
                        self.document.add_paragraph(f"  å˜é‡: {var_list}", style='List Bullet')
                    else:
                        var_list = ", ".join(columns[:5])
                        self.document.add_paragraph(f"  å˜é‡: {var_list} ç­‰{len(columns)}ä¸ª", style='List Bullet')
            
            # 3. ç¼ºå¤±å€¼åˆ†æ
            if merged_info.get('missing_analysis'):
                missing_info = merged_info['missing_analysis']
                if missing_info.get('total_missing', 0) > 0:
                    self.document.add_heading("1.3 æ•°æ®å®Œæ•´æ€§åˆ†æ", level=2)
                    
                    total_missing = missing_info.get('total_missing', 0)
                    missing_pct = missing_info.get('missing_percentage', 0)
                    
                    self.document.add_paragraph(f"æ•°æ®é›†å…±æœ‰ {total_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå æ€»æ•°æ®ç‚¹çš„ {missing_pct:.2f}%")
                    
                    # å„åˆ—ç¼ºå¤±æƒ…å†µ
                    if missing_info.get('columns_with_missing'):
                        self.document.add_paragraph("å„å˜é‡ç¼ºå¤±æƒ…å†µï¼š")
                        for col in missing_info['columns_with_missing'][:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                            if col in missing_info:
                                col_missing = missing_info[col]
                                count = col_missing.get('count', 0)
                                pct = col_missing.get('percentage', 0)
                                self.document.add_paragraph(f"â€¢ {col}: {count} ä¸ªç¼ºå¤±å€¼ ({pct:.1f}%)", style='List Bullet')
            
            # 4. åˆ†æå»ºè®®
            if merged_info.get('recommendations'):
                self.document.add_heading("1.4 åˆ†æå»ºè®®", level=2)
                self.document.add_paragraph("åŸºäºæ•°æ®ç‰¹å¾ï¼Œç³»ç»Ÿæä¾›ä»¥ä¸‹åˆ†æå»ºè®®ï¼š")
                
                for i, recommendation in enumerate(merged_info['recommendations'], 1):
                    self.document.add_paragraph(f"{i}. {recommendation}")
            
            self.document.add_paragraph()
            logger.info("é—®å·æ•°æ®åˆ†æä¸“èŠ‚æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            logger.exception(f"æ·»åŠ é—®å·æ•°æ®åˆ†æä¸“èŠ‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            self.document.add_paragraph("é—®å·æ•°æ®åˆ†æä¸“èŠ‚ç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ã€‚")
    
    def add_chart(self, chart: Union[Figure, str], title: str, 
                 description: Optional[str] = None) -> None:
        """
        æ·»åŠ å›¾è¡¨åˆ°æŠ¥å‘Š
        
        Args:
            chart: matplotlib Figureå¯¹è±¡æˆ–å›¾ç‰‡è·¯å¾„
            title: å›¾è¡¨æ ‡é¢˜
            description: å›¾è¡¨æè¿°
        """
        # æ·»åŠ å›¾è¡¨æ ‡é¢˜
        chart_heading = self.document.add_heading(title, level=2)
        
        # ä¿å­˜å›¾è¡¨åˆ°ä¸´æ—¶æ–‡ä»¶
        if isinstance(chart, Figure):
            img_path = os.path.join(self.temp_dir, f"chart_{len(os.listdir(self.temp_dir)) + 1}.png")
            chart.savefig(img_path, dpi=300, bbox_inches='tight')
            plt.close(chart)  # å…³é—­å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜
        else:
            img_path = chart
        
        # æ·»åŠ å›¾ç‰‡
        if os.path.exists(img_path):
            para = self.document.add_paragraph()
            run = para.add_run()
            run.add_picture(img_path, width=Inches(6.0))
            para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ·»åŠ æè¿°
        if description:
            desc_para = self.document.add_paragraph(description)
            desc_para.style = 'Quote'
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
    
    def add_multiple_charts(self, charts: Optional[Dict[str, Figure]], section_title: str) -> None:
        """
        æ·»åŠ å¤šä¸ªå›¾è¡¨åˆ°æŠ¥å‘Š
        
        Args:
            charts: å›¾è¡¨åç§°åˆ°å›¾è¡¨å¯¹è±¡çš„æ˜ å°„
            section_title: ç« èŠ‚æ ‡é¢˜
        """
        self.document.add_heading(section_title, level=1)
        
        # æ£€æŸ¥chartsæ˜¯å¦å­˜åœ¨ã€ä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if charts and isinstance(charts, dict):
            # æ·»åŠ æ¯ä¸ªå›¾è¡¨
            for chart_name, chart in charts.items():
                try:
                    self.add_chart(chart, chart_name, f"å›¾è¡¨: {chart_name}")
                except Exception as e:
                    logger.exception(f"æ·»åŠ å›¾è¡¨ {chart_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    # ç»§ç»­æ·»åŠ å…¶ä»–å›¾è¡¨
                    continue
        else:
            # å¦‚æœæ²¡æœ‰å¯ç”¨å›¾è¡¨ï¼Œæ·»åŠ è¯´æ˜æ–‡æœ¬
            self.document.add_paragraph("æš‚æ— å¯ç”¨çš„å›¾è¡¨ã€‚")
    
    def add_conclusion(self, conclusion: str) -> None:
        """
        æ·»åŠ ç»“è®ºéƒ¨åˆ†
        
        Args:
            conclusion: ç»“è®ºå†…å®¹
        """
        self.document.add_heading("4. ç»“è®ºä¸å»ºè®®", level=1)
        
        # æ·»åŠ ç»“è®ºæ®µè½
        self.document.add_paragraph(conclusion)
        
        # æ·»åŠ ç©ºè¡Œ
        self.document.add_paragraph()
    
    def add_recommendations(self, recommendations: List[str]) -> None:
        """
        æ·»åŠ å»ºè®®éƒ¨åˆ†
        
        Args:
            recommendations: å»ºè®®åˆ—è¡¨
        """
        if recommendations:
            self.document.add_heading("4.1 ä¸šåŠ¡å»ºè®®", level=2)
            
            for rec in recommendations:
                self.document.add_paragraph(f"- {rec}", style='List Bullet')
    
    def save_report(self, output_path: Optional[str] = None) -> str:
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if self.document is None:
            raise ValueError("è¯·å…ˆåˆ›å»ºæŠ¥å‘Š")
        
        # å¦‚æœæœªæä¾›è·¯å¾„ï¼Œç”Ÿæˆé»˜è®¤è·¯å¾„ï¼ˆæ¡Œé¢ï¼‰
        if output_path is None:
            # è·å–æ¡Œé¢è·¯å¾„
            desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(desktop_path, f"æ•°æ®åˆ†ææŠ¥å‘Š_{timestamp}.docx")
        
        # æ›´æ–°ç›®å½•
        # æ³¨æ„ï¼špython-docx ä¸æ”¯æŒè‡ªåŠ¨ç”Ÿæˆç›®å½•ã€‚ä¸ºäº†é¿å…ç›´æ¥æ“ä½œåº•å±‚ XMLï¼ˆå¯èƒ½å¯¼è‡´ AttributeErrorï¼‰ï¼Œ
        # æˆ‘ä»¬ä»…ç”¨ä¸€ä¸ªå ä½æ–‡æœ¬æ›¿æ¢åŸæœ‰æ ‡è®°ï¼Œç”¨æˆ·å¯ä»¥åœ¨ Word ä¸­æ‰‹åŠ¨æ›´æ–°ç›®å½•å­—æ®µã€‚
        for para in self.document.paragraphs:
            if "[ç›®å½•å°†åœ¨ä¿å­˜æ—¶è‡ªåŠ¨ç”Ÿæˆ]" in para.text:
                para.clear()
                para.add_run("ç›®å½•ï¼ˆè¯·åœ¨ Word ä¸­æ›´æ–°ç›®å½•å­—æ®µä»¥ç”Ÿæˆæœ€æ–°ç›®å½•ï¼‰")
        
        # ä¿å­˜æ–‡æ¡£
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        try:
            out_dir = os.path.dirname(output_path)
            if out_dir:
                os.makedirs(out_dir, exist_ok=True)
        except Exception as e:
            logger.exception(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
            raise

        try:
            self.document.save(output_path)
        except Exception as e:
            logger.exception(f"ä¿å­˜æ–‡æ¡£åˆ° {output_path} å¤±è´¥: {e}")
            # æŠ›å‡ºæ›´å‹å¥½çš„é”™è¯¯
            raise IOError(f"æ— æ³•ä¿å­˜æŠ¥å‘Šåˆ° {output_path}: {e}")

        return output_path
    
    def _get_statistic_name(self, stat: str) -> str:
        """
        è·å–ç»Ÿè®¡é‡çš„ä¸­æ–‡åç§°
        
        Args:
            stat: ç»Ÿè®¡é‡è‹±æ–‡åç§°
            
        Returns:
            ä¸­æ–‡åç§°
        """
        stat_names = {
            'mean': 'å‡å€¼',
            'std': 'æ ‡å‡†å·®',
            'min': 'æœ€å°å€¼',
            '25%': 'ç¬¬ä¸€å››åˆ†ä½æ•°',
            '50%': 'ä¸­ä½æ•°',
            '75%': 'ç¬¬ä¸‰å››åˆ†ä½æ•°',
            'max': 'æœ€å¤§å€¼',
            'count': 'è®¡æ•°',
            'unique': 'å”¯ä¸€å€¼æ•°é‡',
            'top': 'æœ€å¸¸è§å€¼',
            'freq': 'æœ€å¸¸è§å€¼é¢‘ç‡'
        }
        return stat_names.get(stat, stat)
    
    def _add_ai_enhanced_content(self, analysis_results: Dict) -> None:
        """
        æ·»åŠ AIå¢å¼ºçš„åˆ†æå†…å®¹
        
        Args:
            analysis_results: åŒ…å«AIå¢å¼ºå†…å®¹çš„åˆ†æç»“æœå­—å…¸
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰AIå¢å¼ºå†…å®¹
            ai_sections = []
            
            if 'ai_comprehensive_analysis' in analysis_results:
                ai_sections.append(('ç»¼åˆæ™ºèƒ½åˆ†æ', analysis_results['ai_comprehensive_analysis']))
            
            if 'ai_insights' in analysis_results:
                ai_sections.append(('AIæ´å¯Ÿå‘ç°', analysis_results['ai_insights']))
            
            if 'ai_recommendations' in analysis_results:
                ai_sections.append(('æ™ºèƒ½å»ºè®®', analysis_results['ai_recommendations']))
            
            if 'ai_interpretation' in analysis_results:
                ai_sections.append(('ç»“æœæ™ºèƒ½è§£è¯»', analysis_results['ai_interpretation']))
            
            if not ai_sections:
                return
                
            # æ·»åŠ AIå¢å¼ºåˆ†æç« èŠ‚
            self.document.add_heading("3.0 ğŸ¤– AIæ™ºèƒ½åˆ†æ", level=2)
            
            # æ·»åŠ è¯´æ˜æ®µè½
            intro_para = self.document.add_paragraph()
            intro_run = intro_para.add_run("ä»¥ä¸‹å†…å®¹ç”±AIå¤§æ¨¡å‹åŸºäºæ•°æ®åˆ†æç»“æœç”Ÿæˆï¼Œæä¾›æ·±åº¦æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®ï¼š")
            intro_run.font.size = Pt(10)
            intro_run.italic = True
            
            for section_title, ai_content in ai_sections:
                if not isinstance(ai_content, dict):
                    continue
                    
                # æ·»åŠ å­æ ‡é¢˜
                self.document.add_heading(f"ğŸ” {section_title}", level=3)
                
                # æ·»åŠ AIæ¨¡å‹ä¿¡æ¯
                model_info = f"AIæ¨¡å‹: {ai_content.get('ai_model', 'Unknown')} | " \
                           f"æä¾›å•†: {ai_content.get('ai_provider', 'Unknown')} | " \
                           f"ç”Ÿæˆæ—¶é—´: {ai_content.get('enhancement_timestamp', 'Unknown')}"
                
                info_para = self.document.add_paragraph()
                info_run = info_para.add_run(model_info)
                info_run.font.size = Pt(9)
                info_run.font.color.rgb = RGBColor(128, 128, 128)
                info_run.italic = True
                
                # æ·»åŠ AIç”Ÿæˆçš„å†…å®¹
                enhanced_content = ai_content.get('enhanced_content', '')
                if enhanced_content:
                    # å°†AIå†…å®¹æŒ‰æ®µè½åˆ†å‰²å¹¶æ·»åŠ 
                    paragraphs = enhanced_content.split('\n\n')
                    for para_text in paragraphs:
                        if para_text.strip():
                            para = self.document.add_paragraph()
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œï¼ˆåŒ…å«##æˆ–**ï¼‰
                            if para_text.strip().startswith('##') or '**' in para_text:
                                # å¤„ç†Markdownæ ¼å¼çš„æ ‡é¢˜å’ŒåŠ ç²—æ–‡æœ¬
                                if para_text.strip().startswith('##'):
                                    title_text = para_text.strip().replace('##', '').strip()
                                    run = para.add_run(title_text)
                                    run.bold = True
                                    run.font.size = Pt(12)
                                else:
                                    # å¤„ç†åŠ ç²—æ–‡æœ¬
                                    parts = para_text.split('**')
                                    for i, part in enumerate(parts):
                                        if part:
                                            run = para.add_run(part)
                                            if i % 2 == 1:  # å¥‡æ•°ç´¢å¼•çš„éƒ¨åˆ†åº”è¯¥åŠ ç²—
                                                run.bold = True
                            else:
                                # æ™®é€šæ®µè½
                                para.add_run(para_text.strip())
                
                # æ·»åŠ åˆ†éš”çº¿
                self.document.add_paragraph()
                
            logger.info(f"å·²æ·»åŠ  {len(ai_sections)} ä¸ªAIå¢å¼ºåˆ†æç« èŠ‚")
            
        except Exception as e:
            logger.error(f"æ·»åŠ AIå¢å¼ºå†…å®¹å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰æµç¨‹

# è¾…åŠ©å‡½æ•°
def qn(tag):
    """
    ç”Ÿæˆå‘½åç©ºé—´æ ‡ç­¾
    """
    return '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}' + tag

# é«˜çº§æŠ¥å‘Šç”Ÿæˆå™¨ç±»
class AdvancedReportGenerator:
    """
    é«˜çº§æŠ¥å‘Šç”Ÿæˆå™¨ï¼Œé›†æˆæ•°æ®åˆ†æç»“æœè‡ªåŠ¨ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    æ”¯æŒAIå¤§æ¨¡å‹å¢å¼ºåŠŸèƒ½
    """
    
    def __init__(self, ai_enhancer: Optional[AIReportEnhancer] = None):
        self.generator = ReportGenerator()
        self.ai_enhancer = ai_enhancer
        
        # å¦‚æœæ²¡æœ‰æä¾›AIå¢å¼ºå™¨ä½†AIæ¨¡å—å¯ç”¨ï¼Œå°è¯•åˆ›å»ºé»˜è®¤å¢å¼ºå™¨
        if self.ai_enhancer is None and AI_ENHANCEMENT_AVAILABLE:
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºAIå¢å¼ºå™¨
                self.ai_enhancer = create_ai_enhancer()
                logger.info("å·²åˆ›å»ºé»˜è®¤AIæŠ¥å‘Šå¢å¼ºå™¨")
            except Exception as e:
                logger.warning(f"æ— æ³•åˆ›å»ºé»˜è®¤AIå¢å¼ºå™¨: {str(e)}")
                self.ai_enhancer = None
    
    def set_ai_enhancer(self, ai_enhancer: AIReportEnhancer):
        """è®¾ç½®AIå¢å¼ºå™¨"""
        self.ai_enhancer = ai_enhancer
        logger.info("AIæŠ¥å‘Šå¢å¼ºå™¨å·²è®¾ç½®")
    
    def generate_full_report(self, data: pd.DataFrame, 
                           analysis_results: Dict, 
                           charts: Optional[Dict[str, Figure]] = None,
                           file_info: Optional[Dict] = None,
                           output_path: Optional[str] = None,
                           include_template: bool = True,
                           item_mapping: Optional[Dict[str, str]] = None) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š
        
        Args:
            data: æ•°æ®æ¡†
            analysis_results: åˆ†æç»“æœ
            charts: å›¾è¡¨å­—å…¸
            file_info: æ–‡ä»¶ä¿¡æ¯
            output_path: è¾“å‡ºè·¯å¾„
            include_template: æ˜¯å¦åŒ…å«æŠ¥å‘Šæ¨¡æ¿ç¤ºä¾‹
            item_mapping: é¢˜é¡¹æ˜ å°„å­—å…¸ï¼Œæ ¼å¼ä¸º {åˆ—å: é¢˜ç›®æè¿°}
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆå®Œæ•´çš„æ•°æ®åˆ†ææŠ¥å‘Š")
            
            # ä¿æŠ¤ analysis_results ä¸º None çš„æƒ…å†µ
            analysis_results = analysis_results or {}
            if not isinstance(analysis_results, dict):
                raise ValueError("analysis_results å¿…é¡»æ˜¯å­—å…¸ç±»å‹")

            # è®°å½•è¾“å…¥æ‘˜è¦ä»¥ä¾¿è°ƒè¯•
            logger.info(f"è¾“å…¥æ•°æ®ç±»å‹: {type(data)}, è¡Œæ•°: {len(data) if hasattr(data, '__len__') else 'unknown'}")
            logger.info(f"åˆ†æç»“æœé”®: {list(analysis_results.keys())}")
            logger.info(f"å›¾è¡¨ç±»å‹: {type(charts)}, å›¾è¡¨æ•°é‡: {len(charts) if charts else 0}")
            logger.info(f"æ–‡ä»¶ä¿¡æ¯: {file_info}")
            logger.info(f"åŒ…å«æ¨¡æ¿: {include_template}, é¢˜é¡¹æ˜ å°„: {bool(item_mapping)}")

            # åˆ›å»ºæŠ¥å‘Š
            try:
                title = f"æ•°æ®åˆ†ææŠ¥å‘Š - {file_info.get('file_name', 'æ•°æ®é›†')}" if file_info and isinstance(file_info, dict) else "æ•°æ®åˆ†ææŠ¥å‘Š"
                logger.info(f"åˆ›å»ºæŠ¥å‘Šï¼Œæ ‡é¢˜: {title}")
                self.generator.create_report(title=title)
                logger.info("æŠ¥å‘Šæ–‡æ¡£åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"åˆ›å»ºæŠ¥å‘Šæ–‡æ¡£å¤±è´¥: {str(e)}")
                raise

            # 1. æ·»åŠ æŠ¥å‘Šæ¨¡æ¿ç¤ºä¾‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if include_template:
                try:
                    logger.info("æ·»åŠ æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿")
                    self.generator.add_report_template_example()
                    logger.info("æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿æ·»åŠ æˆåŠŸ")
                except Exception as e:
                    logger.error(f"æ·»åŠ æŠ¥å‘Šæ ·ä¾‹æ¨¡æ¿å¤±è´¥: {str(e)}")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ

            # 2. æ™ºèƒ½é—®å·æ•°æ®æ•´åˆä¸åˆ†æ
            merged_questionnaire_info = None
            if data is not None:
                try:
                    logger.info("å¼€å§‹æ™ºèƒ½é—®å·æ•°æ®æ•´åˆ")
                    merged_questionnaire_info = self.generator.smart_merge_questionnaire_data(data, item_mapping)
                    
                    # æ·»åŠ é—®å·æ•°æ®åˆ†æä¸“èŠ‚
                    self.generator.add_questionnaire_analysis_section(merged_questionnaire_info)
                    logger.info("é—®å·æ•°æ®æ•´åˆä¸åˆ†æå®Œæˆ")
                except Exception as e:
                    logger.error(f"æ™ºèƒ½é—®å·æ•°æ®æ•´åˆå¤±è´¥: {str(e)}")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ

        except Exception as e:
            logger.exception(f"ç”Ÿæˆå®Œæ•´æŠ¥å‘Šæ—¶è¾“å…¥å‚æ•°æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
        
        # ğŸ¤– AIå¢å¼ºåˆ†æç»“æœ - åœ¨ç”ŸæˆæŠ¥å‘Šå‰è¿›è¡ŒAIä¼˜åŒ–
        if self.ai_enhancer and data is not None and analysis_results:
            try:
                logger.info("å¼€å§‹AIå¢å¼ºåˆ†æç»“æœ")
                # è¿›è¡Œç»¼åˆæ€§AIå¢å¼º
                analysis_results = self.ai_enhancer.enhance_analysis_results(
                    data=data,
                    analysis_results=analysis_results,
                    enhancement_type="comprehensive"
                )
                logger.info("AIå¢å¼ºåˆ†æç»“æœå®Œæˆ")
            except Exception as e:
                logger.error(f"AIå¢å¼ºåˆ†æç»“æœå¤±è´¥: {str(e)}")
                # AIå¢å¼ºå¤±è´¥ä¸å½±å“æŠ¥å‘Šç”Ÿæˆï¼Œç»§ç»­ä½¿ç”¨åŸå§‹ç»“æœ
        
        # 3. æ·»åŠ æ‰§è¡Œæ‘˜è¦
        try:
            logger.info("æ·»åŠ æ‰§è¡Œæ‘˜è¦")
            self._generate_executive_summary(data, analysis_results, merged_questionnaire_info)
            logger.info("æ‰§è¡Œæ‘˜è¦æ·»åŠ æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ·»åŠ æ‰§è¡Œæ‘˜è¦å¤±è´¥: {str(e)}")
            logger.exception("æ‰§è¡Œæ‘˜è¦ç”Ÿæˆè¯¦ç»†é”™è¯¯")
            raise
        
        # 4. æ·»åŠ æ•°æ®æ¦‚è§ˆ
        try:
            logger.info("æ·»åŠ æ•°æ®æ¦‚è§ˆ")
            self._generate_data_overview(data, file_info)
            logger.info("æ•°æ®æ¦‚è§ˆæ·»åŠ æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ·»åŠ æ•°æ®æ¦‚è§ˆå¤±è´¥: {str(e)}")
            logger.exception("æ•°æ®æ¦‚è§ˆç”Ÿæˆè¯¦ç»†é”™è¯¯")
            raise
        
        # 5. æ·»åŠ æ•°æ®é¢„å¤„ç†ä¿¡æ¯
        try:
            if 'preprocessing' in analysis_results:
                logger.info("æ·»åŠ æ•°æ®é¢„å¤„ç†ä¿¡æ¯")
                self.generator.add_data_preprocessing_section(analysis_results['preprocessing'])
                logger.info("æ•°æ®é¢„å¤„ç†ä¿¡æ¯æ·»åŠ æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ·»åŠ æ•°æ®é¢„å¤„ç†ä¿¡æ¯å¤±è´¥: {str(e)}")
            logger.exception("æ•°æ®é¢„å¤„ç†ä¿¡æ¯ç”Ÿæˆè¯¦ç»†é”™è¯¯")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ
        
        # 6. æ·»åŠ å®Œæ•´çš„åˆ†æç»“æœï¼ˆåŒ…å«æ–°å¢çš„ç»Ÿè®¡åˆ†æï¼‰
        try:
            logger.info("æ·»åŠ å®Œæ•´åˆ†æç»“æœ")
            self.generator.add_analysis_results(analysis_results)
            logger.info("å®Œæ•´åˆ†æç»“æœæ·»åŠ æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ·»åŠ å®Œæ•´åˆ†æç»“æœå¤±è´¥: {str(e)}")
            logger.exception("å®Œæ•´åˆ†æç»“æœç”Ÿæˆè¯¦ç»†é”™è¯¯")
            raise
        
        # 7. æ·»åŠ å›¾è¡¨
        try:
            if charts and isinstance(charts, dict):
                logger.info(f"æ·»åŠ å›¾è¡¨ï¼Œå›¾è¡¨æ•°é‡: {len(charts)}")
                self.generator.add_multiple_charts(charts, "5. æ•°æ®å¯è§†åŒ–")
                logger.info("å›¾è¡¨æ·»åŠ æˆåŠŸ")
            else:
                logger.info("æ— å›¾è¡¨éœ€è¦æ·»åŠ ")
        except Exception as e:
            logger.error(f"æ·»åŠ å›¾è¡¨å¤±è´¥: {str(e)}")
            logger.exception("å›¾è¡¨ç”Ÿæˆè¯¦ç»†é”™è¯¯")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ
        
        # 8. æ·»åŠ ç»“è®ºå’Œå»ºè®®
        try:
            logger.info("æ·»åŠ ç»“è®ºå’Œå»ºè®®")
            self._generate_conclusion(analysis_results, merged_questionnaire_info)
            logger.info("ç»“è®ºå’Œå»ºè®®æ·»åŠ æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ·»åŠ ç»“è®ºå’Œå»ºè®®å¤±è´¥: {str(e)}")
            logger.exception("ç»“è®ºå’Œå»ºè®®ç”Ÿæˆè¯¦ç»†é”™è¯¯")
            raise
        
        # 9. ä¿å­˜æŠ¥å‘Š
        try:
            logger.info("å¼€å§‹ä¿å­˜æŠ¥å‘Š")
            saved_path = self.generator.save_report(output_path)
            logger.info(f"æŠ¥å‘Šä¿å­˜æˆåŠŸï¼Œè·¯å¾„: {saved_path}")
            return saved_path
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
            logger.exception("ä¿å­˜æŠ¥å‘Šè¯¦ç»†é”™è¯¯")
            raise
    
    def _generate_executive_summary(self, data: pd.DataFrame, 
                                   analysis_results: Dict,
                                   merged_questionnaire_info: Optional[Dict] = None) -> None:
        """
        è‡ªåŠ¨ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆæ‰§è¡Œæ‘˜è¦")
            summary = []
            
            # æ•°æ®åŸºæœ¬ä¿¡æ¯
            try:
                data_rows = len(data) if data is not None and hasattr(data, '__len__') else 0
                data_cols = len(data.columns) if data is not None and hasattr(data, 'columns') else 0
                summary.append(f"æœ¬æŠ¥å‘Šåˆ†æäº†åŒ…å« {data_rows} è¡Œå’Œ {data_cols} åˆ—çš„æ•°æ®é›†ã€‚")
                logger.debug(f"æ•°æ®åŸºæœ¬ä¿¡æ¯: {data_rows} è¡Œ, {data_cols} åˆ—")
            except Exception as e:
                logger.error(f"è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯å¤±è´¥: {str(e)}")
                summary.append("æœ¬æŠ¥å‘Šå¯¹æä¾›çš„æ•°æ®é›†è¿›è¡Œäº†åˆ†æã€‚")
            
            # é—®å·æ•°æ®ç‰¹å¾ï¼ˆæ–°å¢ï¼‰
            try:
                if merged_questionnaire_info:
                    likert_scales = []
                    binary_vars = []
                    continuous_vars = []
                    
                    for col, scale_type in merged_questionnaire_info.get('scale_detection', {}).items():
                        if 'Likert' in scale_type:
                            likert_scales.append(col)
                        elif 'äºŒåˆ†å˜é‡' in scale_type:
                            binary_vars.append(col)
                        elif 'è¿ç»­å˜é‡' in scale_type:
                            continuous_vars.append(col)
                    
                    if likert_scales:
                        summary.append(f"è¯†åˆ«å‡º {len(likert_scales)} ä¸ªLikerté‡è¡¨å˜é‡ï¼Œé€‚åˆè¿›è¡Œæ€åº¦å’Œæ»¡æ„åº¦åˆ†æã€‚")
                    
                    if binary_vars:
                        summary.append(f"åŒ…å« {len(binary_vars)} ä¸ªäºŒåˆ†å˜é‡ï¼Œå¯ç”¨äºåˆ†ç±»åˆ†æã€‚")
                    
                    missing_pct = merged_questionnaire_info.get('missing_analysis', {}).get('missing_percentage', 0)
                    if missing_pct > 0:
                        summary.append(f"æ•°æ®å®Œæ•´æ€§è‰¯å¥½ï¼Œç¼ºå¤±å€¼æ¯”ä¾‹ä¸º {missing_pct:.1f}%ã€‚")
                    else:
                        summary.append("æ•°æ®å®Œæ•´æ€§ä¼˜ç§€ï¼Œæ— ç¼ºå¤±å€¼ã€‚")
                        
                    logger.debug(f"é—®å·ç‰¹å¾: {len(likert_scales)} ä¸ªLikerté‡è¡¨, {len(binary_vars)} ä¸ªäºŒåˆ†å˜é‡")
            except Exception as e:
                logger.error(f"è·å–é—®å·æ•°æ®ç‰¹å¾å¤±è´¥: {str(e)}")
            
            # åˆ†æç±»å‹
            try:
                if analysis_results and 'analysis_type' in analysis_results and analysis_results['analysis_type']:
                    summary.append(f"æ ¹æ®æ•°æ®ç‰¹å¾ï¼Œç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«å¹¶æ‰§è¡Œäº†{analysis_results['analysis_type']}åˆ†æã€‚")
                    logger.debug(f"åˆ†æç±»å‹: {analysis_results['analysis_type']}")
            except Exception as e:
                logger.error(f"è·å–åˆ†æç±»å‹å¤±è´¥: {str(e)}")
            # ä¸»è¦å‘ç°
            try:
                # ç›¸å…³æ€§åˆ†æå‘ç°
                try:
                    if (analysis_results and 'correlation' in analysis_results and
                        analysis_results['correlation'] is not None):
                        corr_results = analysis_results['correlation']
                        if isinstance(corr_results, dict):
                            strong_corr = corr_results.get('strong_correlations', [])
                            if strong_corr and len(strong_corr) > 0:
                                summary.append(f"â€¢ å‘ç° {len(strong_corr)} å¯¹å¼ºç›¸å…³ç‰¹å¾ï¼Œç›¸å…³ç³»æ•°å‡è¶…è¿‡0.7ã€‚")
                                logger.debug(f"å¼ºç›¸å…³ç‰¹å¾å¯¹æ•°é‡: {len(strong_corr)}")
                except Exception as e:
                    logger.error(f"æå–ç›¸å…³æ€§åˆ†æå‘ç°å¤±è´¥: {str(e)}")
                
                # èšç±»åˆ†æå‘ç°ï¼ˆæ–°å¢ï¼‰
                try:
                    if (analysis_results and 'cluster_analysis' in analysis_results and
                        analysis_results['cluster_analysis'] is not None):
                        cluster_results = analysis_results['cluster_analysis']
                        if isinstance(cluster_results, dict):
                            n_clusters = cluster_results.get('n_clusters', 0)
                            silhouette_score = cluster_results.get('silhouette_score')
                            if n_clusters > 0:
                                if silhouette_score is not None:
                                    summary.append(f"â€¢ èšç±»åˆ†æå°†æ•°æ®åˆ†ä¸º {n_clusters} ä¸ªç±»åˆ«ï¼Œè½®å»“ç³»æ•°ä¸º {silhouette_score:.3f}ã€‚")
                                else:
                                    summary.append(f"â€¢ èšç±»åˆ†æå°†æ•°æ®åˆ†ä¸º {n_clusters} ä¸ªä¸åŒçš„ç±»åˆ«ã€‚")
                                logger.debug(f"èšç±»ç»“æœ: {n_clusters} ä¸ªç±»åˆ«, è½®å»“ç³»æ•°: {silhouette_score}")
                except Exception as e:
                    logger.error(f"æå–èšç±»åˆ†æå‘ç°å¤±è´¥: {str(e)}")
                
                # å› å­åˆ†æå‘ç°ï¼ˆæ–°å¢ï¼‰
                try:
                    if (analysis_results and 'factor_analysis' in analysis_results and
                        analysis_results['factor_analysis'] is not None):
                        factor_results = analysis_results['factor_analysis']
                        if isinstance(factor_results, dict):
                            n_factors = factor_results.get('n_factors', 0)
                            explained_variance = factor_results.get('explained_variance_ratio')
                            if n_factors > 0 and explained_variance is not None:
                                total_variance = sum(explained_variance) * 100
                                summary.append(f"â€¢ å› å­åˆ†ææå– {n_factors} ä¸ªä¸»è¦å› å­ï¼Œç´¯è®¡è§£é‡Šæ–¹å·® {total_variance:.1f}%ã€‚")
                                logger.debug(f"å› å­åˆ†æ: {n_factors} ä¸ªå› å­, è§£é‡Šæ–¹å·®: {total_variance:.1f}%")
                except Exception as e:
                    logger.error(f"æå–å› å­åˆ†æå‘ç°å¤±è´¥: {str(e)}")
                
                # æ–¹å·®åˆ†æå‘ç°ï¼ˆæ–°å¢ï¼‰
                try:
                    if (analysis_results and 'anova_analysis' in analysis_results and
                        analysis_results['anova_analysis'] is not None):
                        anova_results = analysis_results['anova_analysis']
                        if isinstance(anova_results, dict):
                            p_value = anova_results.get('p_value')
                            dependent_var = anova_results.get('dependent_variable', 'ç›®æ ‡å˜é‡')
                            if p_value is not None:
                                if p_value < 0.05:
                                    summary.append(f"â€¢ æ–¹å·®åˆ†ææ˜¾ç¤º {dependent_var} åœ¨ä¸åŒç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ (p < 0.05)ã€‚")
                                else:
                                    summary.append(f"â€¢ æ–¹å·®åˆ†ææ˜¾ç¤º {dependent_var} åœ¨ä¸åŒç»„é—´æ— æ˜¾è‘—å·®å¼‚ (p â‰¥ 0.05)ã€‚")
                                logger.debug(f"æ–¹å·®åˆ†æ: på€¼ = {p_value}, å˜é‡: {dependent_var}")
                except Exception as e:
                    logger.error(f"æå–æ–¹å·®åˆ†æå‘ç°å¤±è´¥: {str(e)}")
                
                # ä»ç»Ÿè®¡åˆ†æä¸­æå–å‘ç°
                try:
                    if (analysis_results and 'descriptive_stats' in analysis_results and 
                        analysis_results['descriptive_stats'] is not None):
                        stats_df = analysis_results['descriptive_stats']
                        if data is not None and hasattr(data, 'select_dtypes'):
                            numeric_cols = data.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                # ä¼˜å…ˆç›´æ¥ä»åŸå§‹æ•°æ®è®¡ç®—æ ‡å‡†å·®å¹¶æ‰¾å‡ºå˜å¼‚æœ€å¤§çš„åˆ—ï¼Œè¿™æ¯”ä¾èµ–ç»Ÿè®¡è¡¨æ›´ç¨³å¥
                                try:
                                    std_series = data[numeric_cols].std()
                                    if std_series is not None and not std_series.empty:
                                        max_var_col = std_series.idxmax()
                                        summary.append(f"â€¢ {max_var_col} æ˜¯å˜å¼‚ç¨‹åº¦æœ€å¤§çš„ç‰¹å¾ã€‚")
                                        logger.debug(f"å˜å¼‚æœ€å¤§çš„ç‰¹å¾: {max_var_col}")
                                except Exception as e:
                                    logger.error(f"è®¡ç®—å˜å¼‚ç¨‹åº¦å¤±è´¥: {str(e)}")
                                    # é€€å›åˆ°å°è¯•ä» stats_df ä¸­è¯»å– 'std' è¡Œï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                    try:
                                        if stats_df is not None and hasattr(stats_df, 'loc') and 'std' in stats_df.index:
                                            max_var_col = stats_df.loc['std'].idxmax()
                                            summary.append(f"â€¢ {max_var_col} æ˜¯å˜å¼‚ç¨‹åº¦æœ€å¤§çš„ç‰¹å¾ã€‚")
                                            logger.debug(f"ä»ç»Ÿè®¡è¡¨è·å–å˜å¼‚æœ€å¤§çš„ç‰¹å¾: {max_var_col}")
                                    except Exception as e2:
                                        logger.error(f"ä»ç»Ÿè®¡è¡¨è·å–å˜å¼‚ç¨‹åº¦å¤±è´¥: {str(e2)}")
                except Exception as e:
                    logger.error(f"æå–ç»Ÿè®¡åˆ†æå‘ç°å¤±è´¥: {str(e)}")
                        
            except Exception as e:
                logger.error(f"æå–ä¸»è¦å‘ç°å¤±è´¥: {str(e)}")
                summary.append("â€¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œé€‚åˆè¿›è¡Œåˆ†æã€‚")
            
            # æ¨¡å‹æ¨è
            try:
                if (analysis_results and 'model_recommendations' in analysis_results and 
                    analysis_results['model_recommendations'] and 
                    isinstance(analysis_results['model_recommendations'], (list, tuple)) and
                    len(analysis_results['model_recommendations']) > 0):
                    top_model = analysis_results['model_recommendations'][0]
                    if top_model and isinstance(top_model, dict):
                        model_name = top_model.get('name', 'æœªçŸ¥æ¨¡å‹')
                        summary.append(f"\næ¨èæ¨¡å‹ï¼š{model_name}")
                        logger.debug(f"æ¨èæ¨¡å‹: {model_name}")
            except Exception as e:
                logger.error(f"è·å–æ¨¡å‹æ¨èå¤±è´¥: {str(e)}")
            
            # åˆå¹¶æ‘˜è¦
            try:
                summary_text = " ".join(str(s) for s in summary if s)  # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²ä¸”éç©º
                logger.debug(f"ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬é•¿åº¦: {len(summary_text)}")
                self.generator.add_executive_summary(summary_text)
                logger.info("æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                logger.error(f"åˆå¹¶å’Œæ·»åŠ æ‘˜è¦å¤±è´¥: {str(e)}")
                # ä½¿ç”¨ç®€å•çš„é»˜è®¤æ‘˜è¦
                default_summary = "æœ¬æŠ¥å‘Šå¯¹æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬æ•°æ®æ¦‚è§ˆã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–å±•ç¤ºã€‚"
                self.generator.add_executive_summary(default_summary)
                logger.info("ä½¿ç”¨é»˜è®¤æ‘˜è¦")
                
        except Exception as e:
            logger.exception(f"ç”Ÿæˆæ‰§è¡Œæ‘˜è¦æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
            raise
    
    def _generate_data_overview(self, data: pd.DataFrame, 
                              file_info: Optional[Dict] = None) -> None:
        """
        è‡ªåŠ¨ç”Ÿæˆæ•°æ®æ¦‚è§ˆ
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆæ•°æ®æ¦‚è§ˆ")
            
            # å®‰å…¨åœ°å¤„ç†file_info
            safe_file_info = file_info or {}
            
            data_info = {
                'file_name': str(safe_file_info.get('file_name', 'æœªçŸ¥')) if safe_file_info.get('file_name') is not None else 'æœªçŸ¥',
                'file_format': str(safe_file_info.get('file_format', 'æœªçŸ¥')) if safe_file_info.get('file_format') is not None else 'æœªçŸ¥',
                'num_rows': len(data) if data is not None else 0,
                'num_columns': len(data.columns) if data is not None and hasattr(data, 'columns') else 0,
                'num_numeric_cols': len(data.select_dtypes(include=[np.number]).columns) if data is not None else 0,
                'num_categorical_cols': len(data.select_dtypes(include=['object', 'category']).columns) if data is not None else 0,
                'num_date_cols': len(data.select_dtypes(include=['datetime64']).columns) if data is not None else 0,
                'data_size': f"{data.memory_usage(deep=True).sum() / 1024:.2f} KB" if data is not None and hasattr(data, 'memory_usage') else 'æœªçŸ¥'
            }
            
            logger.debug(f"æ•°æ®æ¦‚è§ˆä¿¡æ¯: {data_info}")
            
            # åˆ—ä¿¡æ¯
            columns_info = []
            if data is not None and hasattr(data, 'columns'):
                for col in data.columns[:10]:  # åªæ˜¾ç¤ºå‰10åˆ—
                    try:
                        col_info = {
                            'name': str(col) if col is not None else 'æœªçŸ¥åˆ—',
                            'dtype': str(data[col].dtype) if hasattr(data[col], 'dtype') else 'æœªçŸ¥',
                            'non_null_count': int(data[col].count()) if hasattr(data[col], 'count') else 0,
                            'description': f"ç¼ºå¤±å€¼: {data[col].isnull().sum()}" if hasattr(data[col], 'isnull') else 'N/A'
                        }
                        columns_info.append(col_info)
                        logger.debug(f"å¤„ç†åˆ—: {col}, ä¿¡æ¯: {col_info}")
                    except Exception as e:
                        logger.error(f"å¤„ç†åˆ— {col} æ—¶å‡ºé”™: {str(e)}")
                        # æ·»åŠ é»˜è®¤ä¿¡æ¯
                        col_info = {
                            'name': str(col) if col is not None else 'æœªçŸ¥åˆ—',
                            'dtype': 'æœªçŸ¥',
                            'non_null_count': 0,
                            'description': 'N/A'
                        }
                        columns_info.append(col_info)
            
            data_info['columns_info'] = columns_info
            logger.info(f"ç”Ÿæˆäº† {len(columns_info)} ä¸ªåˆ—çš„ä¿¡æ¯")
            
            # æ·»åŠ åˆ°æŠ¥å‘Š
            self.generator.add_data_overview(data_info)
            logger.info("æ•°æ®æ¦‚è§ˆæ·»åŠ æˆåŠŸ")
            
        except Exception as e:
            logger.exception(f"ç”Ÿæˆæ•°æ®æ¦‚è§ˆæ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
            raise
    
    def _generate_conclusion(self, analysis_results: Dict,
                            merged_questionnaire_info: Optional[Dict] = None) -> None:
        """
        è‡ªåŠ¨ç”Ÿæˆç»“è®ºå’Œå»ºè®®
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆç»“è®ºå’Œå»ºè®®")
            conclusion_parts = []
            
            # æ€»ç»“æ•°æ®åˆ†æç»“æœ
            conclusion_parts.append("é€šè¿‡å¯¹æ•°æ®é›†çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š")
            
            # æ·»åŠ ä¸»è¦ç»“è®ºï¼ˆä¿æŠ¤å¯èƒ½ä¸º None çš„å­—æ®µï¼‰
            try:
                conclusions = analysis_results.get('conclusions') if analysis_results else None
                logger.debug(f"ç»“è®ºç±»å‹: {type(conclusions)}, å†…å®¹: {conclusions}")
                
                if conclusions and isinstance(conclusions, (list, tuple)) and len(conclusions) > 0:
                    for i, conclusion in enumerate(conclusions, 1):
                        if conclusion:  # ç¡®ä¿conclusionä¸ä¸ºNoneæˆ–ç©ºå­—ç¬¦ä¸²
                            conclusion_parts.append(f"{i}. {conclusion}")
                            logger.debug(f"æ·»åŠ ç»“è®º {i}: {conclusion}")
                else:
                    logger.info("æ²¡æœ‰æ‰¾åˆ°é¢„è®¾ç»“è®ºï¼Œç”Ÿæˆè‡ªåŠ¨ç»“è®º")
                    conclusion_count = 1
                    
                    # åŸºç¡€æ•°æ®è´¨é‡ç»“è®º
                    conclusion_parts.append(f"{conclusion_count}. æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»è¿‡é€‚å½“çš„æ¸…æ´—å’Œé¢„å¤„ç†åå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æã€‚")
                    conclusion_count += 1
                    
                    # é—®å·æ•°æ®ç‰¹å®šç»“è®ºï¼ˆæ–°å¢ï¼‰
                    if merged_questionnaire_info:
                        try:
                            likert_count = sum(1 for scale_type in merged_questionnaire_info.get('scale_detection', {}).values() 
                                             if 'Likert' in scale_type)
                            if likert_count > 0:
                                conclusion_parts.append(f"{conclusion_count}. é—®å·åŒ…å« {likert_count} ä¸ªLikerté‡è¡¨å˜é‡ï¼Œé€‚åˆè¿›è¡Œæ€åº¦å’Œè®¤çŸ¥åˆ†æã€‚")
                                conclusion_count += 1
                            
                            missing_pct = merged_questionnaire_info.get('missing_analysis', {}).get('missing_percentage', 0)
                            if missing_pct < 5:
                                conclusion_parts.append(f"{conclusion_count}. æ•°æ®å®Œæ•´æ€§ä¼˜ç§€ï¼Œç¼ºå¤±å€¼æ¯”ä¾‹ä»…ä¸º {missing_pct:.1f}%ï¼Œæ•°æ®å¯ä¿¡åº¦é«˜ã€‚")
                                conclusion_count += 1
                        except Exception as e:
                            logger.error(f"ç”Ÿæˆé—®å·ç»“è®ºå¤±è´¥: {str(e)}")
                    
                    # ç›¸å…³æ€§åˆ†æç»“è®º
                    if analysis_results and 'correlation' in analysis_results:
                        conclusion_parts.append(f"{conclusion_count}. é€šè¿‡ç›¸å…³æ€§åˆ†æå‘ç°äº†ç‰¹å¾é—´çš„é‡è¦å…³è”å…³ç³»ã€‚")
                        conclusion_count += 1
                    
                    # èšç±»åˆ†æç»“è®ºï¼ˆæ–°å¢ï¼‰
                    if (analysis_results and 'cluster_analysis' in analysis_results and
                        analysis_results['cluster_analysis'] is not None):
                        cluster_results = analysis_results['cluster_analysis']
                        if isinstance(cluster_results, dict):
                            n_clusters = cluster_results.get('n_clusters', 0)
                            if n_clusters > 0:
                                conclusion_parts.append(f"{conclusion_count}. èšç±»åˆ†ææˆåŠŸå°†æ ·æœ¬åˆ†ä¸º {n_clusters} ä¸ªå…·æœ‰ä¸åŒç‰¹å¾çš„ç¾¤ä½“ã€‚")
                                conclusion_count += 1
                    
                    # å› å­åˆ†æç»“è®ºï¼ˆæ–°å¢ï¼‰
                    if (analysis_results and 'factor_analysis' in analysis_results and
                        analysis_results['factor_analysis'] is not None):
                        factor_results = analysis_results['factor_analysis']
                        if isinstance(factor_results, dict):
                            n_factors = factor_results.get('n_factors', 0)
                            if n_factors > 0:
                                conclusion_parts.append(f"{conclusion_count}. å› å­åˆ†ææˆåŠŸæå– {n_factors} ä¸ªä¸»è¦ç»´åº¦ï¼Œæœ‰æ•ˆç®€åŒ–äº†æ•°æ®ç»“æ„ã€‚")
                                conclusion_count += 1
                    
                    # æ–¹å·®åˆ†æç»“è®ºï¼ˆæ–°å¢ï¼‰
                    if (analysis_results and 'anova_analysis' in analysis_results and
                        analysis_results['anova_analysis'] is not None):
                        anova_results = analysis_results['anova_analysis']
                        if isinstance(anova_results, dict):
                            p_value = anova_results.get('p_value')
                            if p_value is not None:
                                if p_value < 0.05:
                                    conclusion_parts.append(f"{conclusion_count}. æ–¹å·®åˆ†æè¯å®äº†ä¸åŒç»„åˆ«é—´å­˜åœ¨ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚")
                                else:
                                    conclusion_parts.append(f"{conclusion_count}. æ–¹å·®åˆ†ææ˜¾ç¤ºä¸åŒç»„åˆ«é—´å·®å¼‚ä¸æ˜¾è‘—ï¼Œç¾¤ä½“ç‰¹å¾ç›¸å¯¹ä¸€è‡´ã€‚")
                                conclusion_count += 1
                    
                    # æ¨¡å‹æ¨èç»“è®º
                    if analysis_results and 'model_recommendations' in analysis_results:
                        conclusion_parts.append(f"{conclusion_count}. åŸºäºæ•°æ®ç‰¹å¾ï¼Œç³»ç»Ÿæ¨èäº†é€‚åˆçš„åˆ†ææ¨¡å‹ã€‚")
                        conclusion_count += 1
                        
            except Exception as e:
                logger.error(f"ç”Ÿæˆç»“è®ºå¤±è´¥: {str(e)}")
                conclusion_parts.append("1. æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»è¿‡é€‚å½“çš„æ¸…æ´—å’Œé¢„å¤„ç†åå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æã€‚")
            
            # åˆå¹¶ç»“è®º
            try:
                conclusion_text = " ".join(str(part) for part in conclusion_parts if part)
                logger.debug(f"ç”Ÿæˆçš„ç»“è®ºæ–‡æœ¬é•¿åº¦: {len(conclusion_text)}")
                self.generator.add_conclusion(conclusion_text)
                logger.info("ç»“è®ºæ·»åŠ æˆåŠŸ")
            except Exception as e:
                logger.error(f"æ·»åŠ ç»“è®ºå¤±è´¥: {str(e)}")
                default_conclusion = "é€šè¿‡å¯¹æ•°æ®é›†çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š1. æ•°æ®è´¨é‡è‰¯å¥½ï¼Œé€‚åˆè¿›è¡Œåˆ†æã€‚"
                self.generator.add_conclusion(default_conclusion)
                logger.info("ä½¿ç”¨é»˜è®¤ç»“è®º")
            
            # æ·»åŠ å»ºè®®
            try:
                recommendations = []
                if (analysis_results and 'recommendations' in analysis_results and 
                    analysis_results['recommendations'] is not None):
                    # ç¡®ä¿recommendationsæ˜¯å¯è¿­ä»£çš„åˆ—è¡¨
                    if isinstance(analysis_results['recommendations'], (list, tuple)):
                        recommendations = [rec for rec in analysis_results['recommendations'] if rec]  # è¿‡æ»¤æ‰ç©ºå€¼
                        logger.debug(f"æ‰¾åˆ° {len(recommendations)} ä¸ªé¢„è®¾å»ºè®®")
                
                if not recommendations:
                    logger.info("æ²¡æœ‰æ‰¾åˆ°é¢„è®¾å»ºè®®ï¼Œç”Ÿæˆè‡ªåŠ¨å»ºè®®")
                    # è‡ªåŠ¨ç”Ÿæˆå»ºè®®
                    recommendations.append("æŒç»­æ”¶é›†æ•°æ®ï¼Œå»ºç«‹æ—¶é—´åºåˆ—åˆ†ææ¨¡å‹ä»¥é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚")
                    recommendations.append("è€ƒè™‘å¼•å…¥æ›´å¤šç›¸å…³ç‰¹å¾ä»¥æé«˜åˆ†æç²¾åº¦ã€‚")
                    recommendations.append("åŸºäºæ¨èçš„æ¨¡å‹è¿›è¡Œæ·±å…¥çš„é¢„æµ‹åˆ†æã€‚")
                    recommendations.append("å®šæœŸæ›´æ–°åˆ†ææŠ¥å‘Šï¼Œç›‘æ§å…³é”®æŒ‡æ ‡çš„å˜åŒ–ã€‚")
                    
                    # åŸºäºé—®å·æ•°æ®çš„ä¸“é—¨å»ºè®®ï¼ˆæ–°å¢ï¼‰
                    if merged_questionnaire_info:
                        try:
                            likert_count = sum(1 for scale_type in merged_questionnaire_info.get('scale_detection', {}).values() 
                                             if 'Likert' in scale_type)
                            if likert_count > 0:
                                recommendations.append("é’ˆå¯¹Likerté‡è¡¨æ•°æ®ï¼Œå»ºè®®è¿›è¡Œä¿¡åº¦å’Œæ•ˆåº¦åˆ†æä»¥ç¡®ä¿æµ‹é‡è´¨é‡ã€‚")
                            
                            if merged_questionnaire_info.get('missing_analysis', {}).get('missing_percentage', 0) > 0:
                                recommendations.append("å¯¹äºå­˜åœ¨ç¼ºå¤±å€¼çš„é—®å·é¢˜é¡¹ï¼Œå»ºè®®åˆ†æç¼ºå¤±æ¨¡å¼å¹¶é‡‡ç”¨é€‚å½“çš„å¤„ç†æ–¹æ³•ã€‚")
                            
                            recommendations.append("å»ºè®®å¯¹é—®å·æ•°æ®è¿›è¡Œå› å­åˆ†æï¼Œæ¢ç´¢æ½œåœ¨çš„ç»´åº¦ç»“æ„ã€‚")
                        except Exception as e:
                            logger.error(f"ç”Ÿæˆé—®å·å»ºè®®å¤±è´¥: {str(e)}")
                    
                    # åŸºäºåˆ†æç»“æœçš„ä¸“é—¨å»ºè®®ï¼ˆæ–°å¢ï¼‰
                    if (analysis_results and 'cluster_analysis' in analysis_results and
                        analysis_results['cluster_analysis'] is not None):
                        recommendations.append("åŸºäºèšç±»åˆ†æç»“æœï¼Œå»ºè®®é’ˆå¯¹ä¸åŒç¾¤ä½“åˆ¶å®šå·®å¼‚åŒ–çš„ç­–ç•¥ã€‚")
                    
                    if (analysis_results and 'factor_analysis' in analysis_results and
                        analysis_results['factor_analysis'] is not None):
                        recommendations.append("åˆ©ç”¨å› å­åˆ†æç»“æœæ„å»ºé™ç»´æ¨¡å‹ï¼Œæé«˜åç»­åˆ†ææ•ˆç‡ã€‚")
                
                logger.debug(f"æœ€ç»ˆå»ºè®®æ•°é‡: {len(recommendations)}")
                self.generator.add_recommendations(recommendations)
                logger.info("å»ºè®®æ·»åŠ æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"æ·»åŠ å»ºè®®å¤±è´¥: {str(e)}")
                default_recommendations = [
                    "æŒç»­æ”¶é›†æ•°æ®ï¼Œå»ºç«‹æ—¶é—´åºåˆ—åˆ†ææ¨¡å‹ä»¥é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚",
                    "è€ƒè™‘å¼•å…¥æ›´å¤šç›¸å…³ç‰¹å¾ä»¥æé«˜åˆ†æç²¾åº¦ã€‚"
                ]
                self.generator.add_recommendations(default_recommendations)
                logger.info("ä½¿ç”¨é»˜è®¤å»ºè®®")
                
        except Exception as e:
            logger.exception(f"ç”Ÿæˆç»“è®ºå’Œå»ºè®®æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
            raise

# åˆ›å»ºå·¥å‚å‡½æ•°
def create_report_generator() -> ReportGenerator:
    """
    åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
    """
    return ReportGenerator()

def create_advanced_report_generator() -> AdvancedReportGenerator:
    """
    åˆ›å»ºé«˜çº§æŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
    """
    return AdvancedReportGenerator()
