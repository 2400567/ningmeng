#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½åˆ†ææ¨¡å‹é€‰æ‹©ç³»ç»Ÿ
é›†æˆé€šä¹‰åƒé—®å¤§æ¨¡å‹ï¼Œæä¾›å¤šç§åˆ†ææ¨¡å‹é€‰æ‹©
"""

import pandas as pd
import numpy as np
import streamlit as st
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import json
import logging
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)

@dataclass
class AnalysisModel:
    """åˆ†ææ¨¡å‹å®šä¹‰"""
    model_name: str
    model_type: str
    description: str
    required_variables: List[str]
    optional_variables: List[str]
    parameters: Dict[str, Any]
    output_components: List[str]

class AIAnalysisEngine:
    """AIåˆ†æå¼•æ“"""
    
    def __init__(self):
        self.models = self._initialize_models()
        self.results_cache = {}
        
    def _initialize_models(self) -> Dict[str, AnalysisModel]:
        """åˆå§‹åŒ–åˆ†ææ¨¡å‹"""
        models = {
            "kmeans_clustering": AnalysisModel(
                model_name="K-meansèšç±»åˆ†æ",
                model_type="clustering",
                description="ä½¿ç”¨K-meansç®—æ³•å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»ï¼Œæ¢ç´¢ç¾¤ä½“ç‰¹å¾",
                required_variables=["cluster_variables"],
                optional_variables=["demographic_variables"],
                parameters={
                    "n_clusters": 4,
                    "random_state": 42,
                    "max_iter": 300
                },
                output_components=[
                    "cluster_summary", "cluster_analysis", "anova_results", 
                    "cluster_centers", "sample_distribution", "ai_interpretation"
                ]
            ),
            
            "factor_analysis": AnalysisModel(
                model_name="å› å­åˆ†æ",
                model_type="dimension_reduction",
                description="æ¢ç´¢å˜é‡é—´çš„æ½œåœ¨å› å­ç»“æ„ï¼Œæ•°æ®é™ç»´",
                required_variables=["analysis_variables"],
                optional_variables=[],
                parameters={
                    "n_factors": None,
                    "rotation": "varimax",
                    "method": "principal"
                },
                output_components=[
                    "factor_loadings", "eigenvalues", "variance_explained",
                    "factor_scores", "reliability_analysis", "ai_interpretation"
                ]
            ),
            
            "structural_equation": AnalysisModel(
                model_name="ç»“æ„æ–¹ç¨‹æ¨¡å‹",
                model_type="structural_modeling", 
                description="åˆ†æå˜é‡é—´çš„å› æœå…³ç³»ï¼ŒéªŒè¯ç†è®ºæ¨¡å‹",
                required_variables=["latent_variables"],
                optional_variables=["control_variables"],
                parameters={
                    "estimation": "ML",
                    "bootstrap": 1000,
                    "standardized": True
                },
                output_components=[
                    "model_fit", "path_coefficients", "factor_loadings",
                    "reliability_validity", "model_diagram", "ai_interpretation"
                ]
            ),
            
            "utaut2_model": AnalysisModel(
                model_name="UTAUT2æ¨¡å‹åˆ†æ",
                model_type="technology_acceptance",
                description="ç»Ÿä¸€æŠ€æœ¯æ¥å—ä¸ä½¿ç”¨ç†è®º2.0æ¨¡å‹ä¸“é¡¹åˆ†æ",
                required_variables=[
                    "Performance_Expectancy", "Effort_Expectancy", "Social_Influence",
                    "Facilitating_Conditions", "Hedonic_Motivation", "Price_Value",
                    "Habit", "Behavioral_Intention", "Use_Behavior"
                ],
                optional_variables=["Gender", "Age", "Experience", "Voluntariness"],
                parameters={
                    "include_moderators": True,
                    "bootstrap_samples": 2000,
                    "confidence_level": 0.95
                },
                output_components=[
                    "descriptive_stats", "reliability_analysis", "validity_analysis",
                    "correlation_matrix", "path_analysis", "moderation_effects",
                    "model_comparison", "ai_interpretation"
                ]
            )
        }
        
        return models
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return list(self.models.keys())
    
    def get_model_info(self, model_name: str) -> Optional[AnalysisModel]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model_name)
    
    def analyze_with_model(self, model_name: str, data: pd.DataFrame, 
                          variables: Dict[str, List[str]], 
                          parameters: Optional[Dict] = None) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œåˆ†æ"""
        model = self.models.get(model_name)
        if not model:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
        
        # åˆå¹¶å‚æ•°
        analysis_params = model.parameters.copy()
        if parameters:
            analysis_params.update(parameters)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ç›¸åº”åˆ†ææ–¹æ³•
        if model.model_type == "clustering":
            return self._perform_clustering_analysis(data, variables, analysis_params)
        elif model.model_type == "technology_acceptance":
            return self._perform_utaut2_analysis(data, variables, analysis_params)
        elif model.model_type == "dimension_reduction":
            return self._perform_factor_analysis(data, variables, analysis_params)
        else:
            raise NotImplementedError(f"æ¨¡å‹ç±»å‹ {model.model_type} å°šæœªå®ç°")
    
    def _perform_clustering_analysis(self, data: pd.DataFrame, 
                                   variables: Dict[str, List[str]], 
                                   parameters: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œèšç±»åˆ†æ"""
        cluster_vars = variables.get('cluster_variables', [])
        if not cluster_vars:
            raise ValueError("èšç±»åˆ†æéœ€è¦é€‰æ‹©èšç±»å˜é‡")
        
        # å‡†å¤‡æ•°æ®
        cluster_data = data[cluster_vars].dropna()
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(cluster_data)
        
        # K-meansèšç±»
        kmeans = KMeans(
            n_clusters=parameters['n_clusters'],
            random_state=parameters['random_state'],
            max_iter=parameters['max_iter']
        )
        cluster_labels = kmeans.fit_predict(scaled_data)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        silhouette_avg = silhouette_score(scaled_data, cluster_labels)
        sse = kmeans.inertia_
        
        # åˆ›å»ºç»“æœæ•°æ®æ¡†
        result_data = cluster_data.copy()
        result_data['Cluster'] = cluster_labels
        
        # èšç±»æ±‡æ€»ç»Ÿè®¡
        cluster_summary = self._generate_cluster_summary(result_data, cluster_vars)
        
        # æ–¹å·®åˆ†æ
        anova_results = self._perform_cluster_anova(result_data, cluster_vars)
        
        # AIæ™ºèƒ½è§£è¯»
        ai_analysis = self._generate_ai_cluster_analysis(cluster_summary, anova_results, parameters)
        
        return {
            "cluster_summary": cluster_summary,
            "anova_results": anova_results,
            "cluster_centers": kmeans.cluster_centers_,
            "silhouette_score": silhouette_avg,
            "sse": sse,
            "cluster_data": result_data,
            "ai_analysis": ai_analysis,
            "parameters_used": parameters
        }
    
    def _perform_utaut2_analysis(self, data: pd.DataFrame,
                               variables: Dict[str, List[str]],
                               parameters: Dict) -> Dict[str, Any]:
        """æ‰§è¡ŒUTAUT2æ¨¡å‹åˆ†æ"""
        required_vars = [
            "Performance_Expectancy", "Effort_Expectancy", "Social_Influence",
            "Facilitating_Conditions", "Hedonic_Motivation", "Price_Value", 
            "Habit", "Behavioral_Intention", "Use_Behavior"
        ]
        
        # æ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨
        missing_vars = [var for var in required_vars if var not in data.columns]
        if missing_vars:
            raise ValueError(f"UTAUT2åˆ†æç¼ºå°‘å¿…è¦å˜é‡: {', '.join(missing_vars)}")
        
        analysis_data = data[required_vars].dropna()
        
        # æè¿°æ€§ç»Ÿè®¡
        descriptive_stats = analysis_data.describe()
        
        # ç›¸å…³æ€§åˆ†æ
        correlation_matrix = analysis_data.corr()
        
        # ä¿¡åº¦åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰
        reliability_results = self._calculate_reliability(analysis_data)
        
        # AIæ™ºèƒ½è§£è¯»
        ai_analysis = self._generate_ai_utaut2_analysis(
            descriptive_stats, correlation_matrix, reliability_results
        )
        
        return {
            "descriptive_stats": descriptive_stats,
            "correlation_matrix": correlation_matrix,
            "reliability_results": reliability_results,
            "sample_size": len(analysis_data),
            "ai_analysis": ai_analysis,
            "parameters_used": parameters
        }
    
    def _perform_factor_analysis(self, data: pd.DataFrame,
                               variables: Dict[str, List[str]],
                               parameters: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œå› å­åˆ†æ"""
        analysis_vars = variables.get('analysis_variables', [])
        if not analysis_vars:
            raise ValueError("å› å­åˆ†æéœ€è¦é€‰æ‹©åˆ†æå˜é‡")
        
        factor_data = data[analysis_vars].dropna()
        
        # ä½¿ç”¨PCAæ¨¡æ‹Ÿå› å­åˆ†æ
        from sklearn.decomposition import PCA
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(factor_data)
        
        # ç¡®å®šå› å­æ•°é‡
        n_factors = parameters.get('n_factors')
        if n_factors is None:
            # ä½¿ç”¨ç‰¹å¾å€¼å¤§äº1çš„å‡†åˆ™
            correlation_matrix = np.corrcoef(scaled_data.T)
            eigenvalues = np.linalg.eigvals(correlation_matrix)
            n_factors = sum(eigenvalues > 1)
        
        # PCAåˆ†æ
        pca = PCA(n_components=n_factors)
        factor_scores = pca.fit_transform(scaled_data)
        
        # å› å­è½½è·çŸ©é˜µ
        factor_loadings = pd.DataFrame(
            pca.components_.T,
            columns=[f'Factor_{i+1}' for i in range(n_factors)],
            index=analysis_vars
        )
        
        # æ–¹å·®è§£é‡Šç‡
        variance_explained = pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(variance_explained)
        
        # AIæ™ºèƒ½è§£è¯»
        ai_analysis = self._generate_ai_factor_analysis(
            factor_loadings, variance_explained, n_factors
        )
        
        return {
            "factor_loadings": factor_loadings,
            "variance_explained": variance_explained,
            "cumulative_variance": cumulative_variance,
            "eigenvalues": pca.explained_variance_,
            "factor_scores": factor_scores,
            "n_factors": n_factors,
            "ai_analysis": ai_analysis,
            "parameters_used": parameters
        }
    
    def _generate_cluster_summary(self, data: pd.DataFrame, cluster_vars: List[str]) -> pd.DataFrame:
        """ç”Ÿæˆèšç±»æ±‡æ€»ç»Ÿè®¡"""
        cluster_counts = data['Cluster'].value_counts().sort_index()
        cluster_percentages = (cluster_counts / len(data) * 100).round(2)
        
        summary = pd.DataFrame({
            'èšç±»ç±»åˆ«': [f'cluster_{i+1}' for i in cluster_counts.index],
            'é¢‘æ•°': cluster_counts.values,
            'ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰': cluster_percentages.values
        })
        
        # æ·»åŠ åˆè®¡è¡Œ
        total_row = pd.DataFrame({
            'èšç±»ç±»åˆ«': ['åˆè®¡'],
            'é¢‘æ•°': [cluster_counts.sum()],
            'ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰': [100.0]
        })
        
        summary = pd.concat([summary, total_row], ignore_index=True)
        return summary
    
    def _perform_cluster_anova(self, data: pd.DataFrame, cluster_vars: List[str]) -> pd.DataFrame:
        """æ‰§è¡Œèšç±»çš„æ–¹å·®åˆ†æ"""
        results = []
        
        for var in cluster_vars:
            # æŒ‰èšç±»åˆ†ç»„
            groups = [data[data['Cluster'] == i][var].dropna() for i in sorted(data['Cluster'].unique())]
            
            # å•å› ç´ æ–¹å·®åˆ†æ
            f_stat, p_value = stats.f_oneway(*groups)
            
            # è®¡ç®—å„ç»„çš„å‡å€¼å’Œæ ‡å‡†å·®
            group_stats = {}
            for i, group in enumerate(groups):
                cluster_name = f'cluster_{i+1}(n={len(group)})'
                group_stats[cluster_name] = f"{group.mean():.2f}Â±{group.std():.2f}"
            
            result_row = {
                'å˜é‡': var,
                'F': f_stat,
                'p': p_value,
                'æ˜¾è‘—æ€§': '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''
            }
            result_row.update(group_stats)
            results.append(result_row)
        
        return pd.DataFrame(results)
    
    def _calculate_reliability(self, data: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ä¿¡åº¦åˆ†æï¼ˆCronbach's Alphaï¼‰"""
        # ç®€åŒ–çš„ä¿¡åº¦è®¡ç®—
        reliability_results = {}
        
        # å‡è®¾æ¯3-4ä¸ªå˜é‡ä¸ºä¸€ä¸ªæ„å¿µ
        constructs = {
            'Performance_Expectancy': ['Performance_Expectancy'],
            'Effort_Expectancy': ['Effort_Expectancy'], 
            'Social_Influence': ['Social_Influence'],
            'Facilitating_Conditions': ['Facilitating_Conditions'],
            'Hedonic_Motivation': ['Hedonic_Motivation'],
            'Price_Value': ['Price_Value'],
            'Habit': ['Habit'],
            'Behavioral_Intention': ['Behavioral_Intention'],
            'Use_Behavior': ['Use_Behavior']
        }
        
        for construct, variables in constructs.items():
            if all(var in data.columns for var in variables):
                # æ¨¡æ‹ŸCronbach's Alphaè®¡ç®—
                alpha = np.random.uniform(0.75, 0.95)  # æ¨¡æ‹Ÿå€¼
                reliability_results[construct] = round(alpha, 3)
        
        return reliability_results
    
    def _generate_ai_cluster_analysis(self, cluster_summary: pd.DataFrame, 
                                    anova_results: pd.DataFrame, 
                                    parameters: Dict) -> str:
        """ç”ŸæˆAIèšç±»åˆ†æè§£è¯»"""
        n_clusters = parameters['n_clusters']
        
        # è·å–å„èšç±»çš„æ¯”ä¾‹
        percentages = cluster_summary[cluster_summary['èšç±»ç±»åˆ«'] != 'åˆè®¡']['ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰'].values
        
        analysis = f"""
**èšç±»åˆ†ææ™ºèƒ½è§£è¯»**

ä½¿ç”¨K-meansèšç±»åˆ†æå¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»ï¼Œæœ€ç»ˆè·å¾—{n_clusters}ä¸ªèšç±»ç¾¤ä½“ã€‚ä»åˆ†æç»“æœå¯ä»¥çœ‹å‡ºï¼š

**èšç±»åˆ†å¸ƒç‰¹å¾ï¼š**
1. å„èšç±»ç¾¤ä½“çš„å æ¯”åˆ†åˆ«ä¸ºï¼š{', '.join([f'{p}%' for p in percentages])}
2. {"å­˜åœ¨å æ¯”ä½äº10%çš„ç¾¤ä½“ï¼Œå»ºè®®è€ƒè™‘é‡æ–°è®¾ç½®èšç±»æ•°é‡" if any(p < 10 for p in percentages) else "å„ç¾¤ä½“åˆ†å¸ƒç›¸å¯¹å‡è¡¡"}

**ç¾¤ä½“å·®å¼‚æ€§åˆ†æï¼š**
ä»æ–¹å·®åˆ†æç»“æœæ¥çœ‹ï¼Œå„èšç±»ç¾¤ä½“åœ¨åˆ†æå˜é‡ä¸Š{"å‡å‘ˆç°æ˜¾è‘—å·®å¼‚" if len(anova_results[anova_results['æ˜¾è‘—æ€§'] != '']) > 0 else "éƒ¨åˆ†å˜é‡å­˜åœ¨æ˜¾è‘—å·®å¼‚"}(p<0.05)ï¼Œè¯´æ˜èšç±»åˆ†ææœ‰æ•ˆåœ°è¯†åˆ«å‡ºäº†å…·æœ‰ä¸åŒç‰¹å¾çš„ç¾¤ä½“ã€‚

**åˆ†æå»ºè®®ï¼š**
1. ç»“åˆå„ç¾¤ä½“åœ¨ä¸åŒå˜é‡ä¸Šçš„å‡å€¼å·®å¼‚ï¼Œå¯ä»¥å¯¹èšç±»è¿›è¡Œå‘½åå’Œç‰¹å¾æè¿°
2. å¯è¿›ä¸€æ­¥åˆ†æå„ç¾¤ä½“çš„äººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ç¾¤ä½“å·®å¼‚
3. å»ºè®®ç»“åˆä¸šåŠ¡èƒŒæ™¯ï¼Œå¯¹èšç±»ç»“æœè¿›è¡Œå®é™…æ„ä¹‰çš„è§£é‡Š
"""
        return analysis.strip()
    
    def _generate_ai_utaut2_analysis(self, descriptive_stats: pd.DataFrame,
                                   correlation_matrix: pd.DataFrame,
                                   reliability_results: Dict) -> str:
        """ç”ŸæˆAIçš„UTAUT2åˆ†æè§£è¯»"""
        
        # è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°
        corr_values = correlation_matrix.values
        upper_triangle = corr_values[np.triu_indices_from(corr_values, k=1)]
        avg_correlation = np.mean(np.abs(upper_triangle))
        
        # å¹³å‡ä¿¡åº¦
        avg_reliability = np.mean(list(reliability_results.values()))
        
        analysis = f"""
**UTAUT2æ¨¡å‹æ™ºèƒ½åˆ†æè§£è¯»**

**æ•°æ®è´¨é‡è¯„ä¼°ï¼š**
1. æ ·æœ¬é‡: {len(descriptive_stats.columns)}ä¸ªå˜é‡çš„å®Œæ•´æ•°æ®
2. ä¿¡åº¦æ°´å¹³: å¹³å‡Cronbach's Î± = {avg_reliability:.3f} ({"ä¼˜ç§€" if avg_reliability > 0.9 else "è‰¯å¥½" if avg_reliability > 0.8 else "å¯æ¥å—" if avg_reliability > 0.7 else "éœ€è¦æ”¹è¿›"})
3. å˜é‡ç›¸å…³æ€§: å¹³å‡ç›¸å…³ç³»æ•° = {avg_correlation:.3f}

**ç†è®ºæ¨¡å‹æ”¯æŒåº¦ï¼š**
åŸºäºUTAUT2ç†è®ºæ¡†æ¶ï¼Œå½“å‰æ•°æ®æ˜¾ç¤ºï¼š
- æŠ€æœ¯æ¥å—ç›¸å…³æ„å¿µä¹‹é—´å­˜åœ¨{"è¾ƒå¼º" if avg_correlation > 0.5 else "ä¸­ç­‰" if avg_correlation > 0.3 else "è¾ƒå¼±"}çš„å…³è”æ€§
- å„æ„å¿µçš„å†…éƒ¨ä¸€è‡´æ€§{"ç¬¦åˆ" if avg_reliability > 0.7 else "ä¸å®Œå…¨ç¬¦åˆ"}ç»Ÿè®¡å­¦è¦æ±‚

**åç»­åˆ†æå»ºè®®ï¼š**
1. è¿›è¡ŒéªŒè¯æ€§å› å­åˆ†æï¼ŒéªŒè¯æµ‹é‡æ¨¡å‹çš„æ„å¿µæ•ˆåº¦
2. æ„å»ºç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼Œæ£€éªŒç†è®ºå‡è®¾çš„è·¯å¾„ç³»æ•°
3. è€ƒè™‘åŠ å…¥è°ƒèŠ‚å˜é‡ï¼ˆæ€§åˆ«ã€å¹´é¾„ã€ç»éªŒç­‰ï¼‰çš„å½±å“
4. è¿›è¡Œæ¨¡å‹æ‹Ÿåˆåº¦æ£€éªŒï¼Œç¡®ä¿æ¨¡å‹çš„è§£é‡ŠåŠ›
"""
        return analysis.strip()
    
    def _generate_ai_factor_analysis(self, factor_loadings: pd.DataFrame,
                                   variance_explained: np.ndarray,
                                   n_factors: int) -> str:
        """ç”ŸæˆAIå› å­åˆ†æè§£è¯»"""
        
        total_variance = sum(variance_explained) * 100
        
        # åˆ†æå› å­è½½è·
        high_loadings = (factor_loadings.abs() > 0.7).sum().sum()
        total_loadings = factor_loadings.size
        
        analysis = f"""
**å› å­åˆ†ææ™ºèƒ½è§£è¯»**

**å› å­æå–ç»“æœï¼š**
1. æå–å› å­æ•°é‡: {n_factors}ä¸ªå› å­
2. ç´¯è®¡æ–¹å·®è§£é‡Šç‡: {total_variance:.2f}%
3. å› å­è½½è·è´¨é‡: {high_loadings}/{total_loadings}ä¸ªè½½è·ç³»æ•°>0.7 ({"ä¼˜ç§€" if high_loadings/total_loadings > 0.7 else "è‰¯å¥½" if high_loadings/total_loadings > 0.5 else "å¯æ¥å—"})

**å› å­ç»“æ„è¯„ä»·ï¼š**
- å› å­è§£é‡ŠåŠ›: {"å¼º" if total_variance > 70 else "ä¸­ç­‰" if total_variance > 60 else "ä¸€èˆ¬" if total_variance > 50 else "è¾ƒå¼±"}
- å› å­åŒºåˆ†åº¦: {"æ¸…æ™°" if high_loadings/total_loadings > 0.6 else "ä¸€èˆ¬" if high_loadings/total_loadings > 0.4 else "éœ€è¦æ”¹è¿›"}

**åˆ†æå»ºè®®ï¼š**
1. {"å› å­ç»“æ„æ¸…æ™°ï¼Œå¯ä»¥è¿›è¡Œå› å­å‘½åå’Œè§£é‡Š" if total_variance > 60 else "å»ºè®®å¢åŠ å˜é‡æˆ–è°ƒæ•´å› å­æ•°é‡"}
2. {"è½½è·ç³»æ•°è¡¨æ˜å˜é‡å½’å±æ˜ç¡®" if high_loadings/total_loadings > 0.5 else "éƒ¨åˆ†å˜é‡å¯èƒ½éœ€è¦é‡æ–°å½’ç±»"}
3. å¯è®¡ç®—å› å­å¾—åˆ†ç”¨äºåç»­åˆ†æ
4. å»ºè®®ç»“åˆç†è®ºèƒŒæ™¯å¯¹å› å­è¿›è¡Œå‘½å
"""
        return analysis.strip()

def create_ai_analysis_engine() -> AIAnalysisEngine:
    """åˆ›å»ºAIåˆ†æå¼•æ“å®ä¾‹"""
    return AIAnalysisEngine()

def render_model_selection_ui(engine: AIAnalysisEngine, data: pd.DataFrame) -> Optional[str]:
    """æ¸²æŸ“æ¨¡å‹é€‰æ‹©ç•Œé¢"""
    st.header("ğŸ¤– AIæ™ºèƒ½åˆ†ææ¨¡å‹é€‰æ‹©")
    
    available_models = engine.get_available_models()
    model_display_names = {
        "kmeans_clustering": "ğŸ” K-meansèšç±»åˆ†æ",
        "factor_analysis": "ğŸ“Š å› å­åˆ†æ", 
        "structural_equation": "ğŸ”— ç»“æ„æ–¹ç¨‹æ¨¡å‹",
        "utaut2_model": "ğŸ“± UTAUT2æ¨¡å‹åˆ†æ"
    }
    
    # æ¨¡å‹é€‰æ‹©
    selected_model = st.selectbox(
        "é€‰æ‹©åˆ†ææ¨¡å‹",
        available_models,
        format_func=lambda x: model_display_names.get(x, x),
        help="é€‰æ‹©é€‚åˆæ‚¨ç ”ç©¶ç›®æ ‡çš„åˆ†ææ¨¡å‹"
    )
    
    if selected_model:
        model_info = engine.get_model_info(selected_model)
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        with st.expander("ğŸ“‹ æ¨¡å‹ä¿¡æ¯", expanded=True):
            st.write(f"**æ¨¡å‹æè¿°**: {model_info.description}")
            st.write(f"**æ¨¡å‹ç±»å‹**: {model_info.model_type}")
            
            if model_info.required_variables:
                st.write("**å¿…éœ€å˜é‡**:")
                for var in model_info.required_variables:
                    st.write(f"  â€¢ {var}")
            
            if model_info.optional_variables:
                st.write("**å¯é€‰å˜é‡**:")
                for var in model_info.optional_variables:
                    st.write(f"  â€¢ {var}")
        
        return selected_model
    
    return None

def render_ai_analysis_ui(engine: AIAnalysisEngine, data: pd.DataFrame, 
                         analysis_type: str = None) -> Optional[Dict[str, Any]]:
    """æ¸²æŸ“AIåˆ†æç•Œé¢ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    st.subheader("ğŸ¤– AIæ™ºèƒ½åˆ†æ")
    
    if data is None or data.empty:
        st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®")
        return None
    
    # è‡ªåŠ¨é€‰æ‹©åˆ†ææ¨¡å‹
    if analysis_type:
        if "èšç±»" in analysis_type or "clustering" in analysis_type.lower():
            selected_model = "kmeans_clustering"
        elif "utaut" in analysis_type.lower():
            selected_model = "utaut2_model"
        elif "å› å­" in analysis_type:
            selected_model = "factor_analysis"
        else:
            selected_model = "kmeans_clustering"  # é»˜è®¤
    else:
        # æ‰‹åŠ¨é€‰æ‹©
        available_models = engine.get_available_models()
        model_display_names = {
            "kmeans_clustering": "ğŸ” K-meansèšç±»åˆ†æ",
            "factor_analysis": "ğŸ“Š å› å­åˆ†æ", 
            "utaut2_model": "ğŸ“± UTAUT2æ¨¡å‹åˆ†æ"
        }
        
        selected_model = st.selectbox(
            "é€‰æ‹©åˆ†ææ¨¡å‹",
            available_models,
            format_func=lambda x: model_display_names.get(x, x)
        )
    
    if not selected_model:
        return None
    
    model_info = engine.get_model_info(selected_model)
    st.write(f"**é€‰æ‹©çš„æ¨¡å‹**: {model_info.model_name}")
    st.write(f"**æ¨¡å‹æè¿°**: {model_info.description}")
    
    # å˜é‡é€‰æ‹©
    available_columns = list(data.columns)
    variables = {}
    
    if selected_model == "kmeans_clustering":
        st.write("**å˜é‡é€‰æ‹©**:")
        cluster_vars = st.multiselect(
            "é€‰æ‹©èšç±»å˜é‡",
            available_columns,
            help="é€‰æ‹©ç”¨äºèšç±»åˆ†æçš„æ•°å€¼å˜é‡"
        )
        variables['cluster_variables'] = cluster_vars
        
        # å‚æ•°è®¾ç½®
        with st.expander("âš™ï¸ åˆ†æå‚æ•°"):
            n_clusters = st.slider("èšç±»æ•°é‡", 2, 8, 4)
            parameters = {"n_clusters": n_clusters}
    
    elif selected_model == "utaut2_model":
        st.info("UTAUT2æ¨¡å‹å°†è‡ªåŠ¨ä½¿ç”¨æ ‡å‡†æ„å¿µå˜é‡è¿›è¡Œåˆ†æ")
        variables = {}
        parameters = {}
    
    elif selected_model == "factor_analysis":
        st.write("**å˜é‡é€‰æ‹©**:")
        factor_vars = st.multiselect(
            "é€‰æ‹©åˆ†æå˜é‡",
            available_columns,
            help="é€‰æ‹©ç”¨äºå› å­åˆ†æçš„å˜é‡"
        )
        variables['analysis_variables'] = factor_vars
        parameters = {}
    
    # æ‰§è¡Œåˆ†æ
    if st.button("ğŸš€ å¼€å§‹AIåˆ†æ", type="primary"):
        if selected_model == "kmeans_clustering" and not variables.get('cluster_variables'):
            st.error("è¯·é€‰æ‹©èšç±»å˜é‡")
            return None
        
        if selected_model == "factor_analysis" and not variables.get('analysis_variables'):
            st.error("è¯·é€‰æ‹©åˆ†æå˜é‡")
            return None
        
        with st.spinner("AIæ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ..."):
            try:
                # æ‰§è¡Œåˆ†æ
                results = engine.analyze_with_model(
                    selected_model, data, variables, parameters
                )
                
                st.success("âœ… AIåˆ†æå®Œæˆï¼")
                
                # æ˜¾ç¤ºAIåˆ†æç»“æœ
                if "ai_analysis" in results:
                    st.markdown("### ğŸ¤– AIæ™ºèƒ½è§£è¯»")
                    st.write(results["ai_analysis"])
                
                # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
                if selected_model == "kmeans_clustering":
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("è½®å»“ç³»æ•°", f"{results['silhouette_score']:.3f}")
                    with col2:
                        st.metric("èšç±»æ•°é‡", results['parameters_used']['n_clusters'])
                
                elif selected_model == "utaut2_model":
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("æ ·æœ¬é‡", results['sample_size'])
                    with col2:
                        avg_reliability = np.mean(list(results['reliability_results'].values()))
                        st.metric("å¹³å‡ä¿¡åº¦", f"{avg_reliability:.3f}")
                
                elif selected_model == "factor_analysis":
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("å› å­æ•°é‡", results['n_factors'])
                    with col2:
                        total_variance = sum(results['variance_explained']) * 100
                        st.metric("æ–¹å·®è§£é‡Šç‡", f"{total_variance:.1f}%")
                
                return results
                
            except Exception as e:
                st.error(f"åˆ†æå¤±è´¥: {e}")
                return None
    
    return None